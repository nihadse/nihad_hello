import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from tensorflow.keras.models import Model

# Define Siamese network architecture
input_keywords = Input(shape=(max_keywords_length,))
input_sectors = Input(shape=(max_sectors_length,))

embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)

embedded_keywords = embedding_layer(input_keywords)
embedded_sectors = embedding_layer(input_sectors)

flatten_layer = Flatten()

flattened_keywords = flatten_layer(embedded_keywords)
flattened_sectors = flatten_layer(embedded_sectors)

concatenated = Concatenate()([flattened_keywords, flattened_sectors])
dense_layer = Dense(128, activation='relu')(concatenated)
output_layer = Dense(1, activation='sigmoid')(dense_layer)

siamese_model = Model(inputs=[input_keywords, input_sectors], outputs=output_layer)

# Compile and train the model
siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
siamese_model.fit([keywords_train, sectors_train], labels_train, epochs=num_epochs, batch_size=batch_size)

# Inference
similarity_scores = siamese_model.predict([new_keywords, new_sectors])

# Classify based on threshold
predictions = (similarity_scores > threshold).astype(int)








import pandas as pd
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Load your CSV files
df = pd.read_csv('/mnt/Output_Entreprise_Personne_Morale.csv')

# Remove rows where 'Libellé code sectoriels' is not equal to the specified value
df = df[df['Libellé code sectoriels'] != 'CODE TECHNIQUE ATTRIBUE AUX PARTICULIERS']

# Drop rows with missing values in 'Libellé code sectoriels'
df = df.dropna(subset=["Libellé code sectoriels"])

# Convert 'Libellé code sectoriels' to lowercase
df['Libellé code sectoriels'] = df['Libellé code sectoriels'].str.lower()

# Read keyword CSV file
keywords = pd.read_csv('/mnt/keyword_class.csv', delimiter=';')
keywords.rename(columns={'class': 'secteur_sonar'}, inplace=True)

# Convert 'key' column to lowercase and to list
keywords['key'] = keywords['key'].str.lower()
keywords_list = keywords['key'].astype(str).tolist()

# Load your pre-trained Word2Vec model
model = Word2Vec.load('/path/to/your/word2vec/model')

# Get embeddings for sectors and keywords
sector_embeddings = np.array([model.wv[sector] for sector in sectors])
keyword_embeddings = np.array([model.wv[keyword] for keyword in keywords_list])

# Calculate cosine similarity between sector and keyword embeddings
similarity_matrix = cosine_similarity(sector_embeddings, keyword_embeddings)

# Create a DataFrame to store the results
results_df = pd.DataFrame(index=sectors, columns=["Most Matched Keyword", "Similarity Score"])

# Display the similarity between sectors and keywords
for i, sector in enumerate(sectors):
    max_similarity_index = np.argmax(similarity_matrix[i])
    most_matched_keyword = keywords_list[max_similarity_index]
    similarity_score = similarity_matrix[i, max_similarity_index]
    results_df.loc[sector] = [most_matched_keyword, similarity_score]

# Reset the index of results_df
results_df.reset_index(inplace=True)

# Merge results_df with the 'keywords' DataFrame based on the 'Most Matched Keyword' column
results_df = pd.merge(results_df, keywords, left_on="Most Matched Keyword", right_on="key", how='left')
results_df.rename(columns={"index": "Libellé code












word_list = ['word1', 'word2', 'word3']  # Replace with your list of words

embeddings = [model[word] for word in word_list if word in model]







from gensim.models import KeyedVectors
import gzip

# Load the pre-trained Word2Vec model
model_path = 'path/to/GoogleNews-vectors-negative300-SLIM.bin.gz'
model = KeyedVectors.load_word2vec_format(gzip.open(model_path, 'rt', encoding='utf8'), binary=False)

# Example: Get the word embedding for a specific word
word_embedding = model['example_word']
print("Word embedding for 'example_word':", word_embedding)

# Example: Find similar words
similar_words = model.most_similar('example_word', topn=5)
print("Similar words to 'example_word':", similar_words)










import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from factoextra import fviz_cluster

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
sector_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings
keywords = ["keyword1", "keyword2", ...]  # Replace with your actual keywords
sectors = ["sector1", "sector2", ...]  # Replace with your actual sectors

# Perform k-means clustering on keyword embeddings
n_clusters = len(keywords)
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels_keywords = kmeans.fit_predict(keyword_embeddings)

# Find the most similar keyword for each sector
cluster_assignments_sectors = []
for sector_embedding in sector_embeddings:
    similarities = cosine_similarity([sector_embedding], keyword_embeddings)[0]
    most_similar_keyword_index = np.argmax(similarities)
    cluster_assignments_sectors.append(cluster_labels_keywords[most_similar_keyword_index])

# Visualize using fviz_cluster
data = np.concatenate([keyword_embeddings, sector_embeddings], axis=0)
kmeans_result = KMeans(n_clusters=n_clusters, random_state=42).fit(data)

# Visualize clustered data
fviz_cluster(kmeans_result, data = data, stand = False,
             geom = "point", ellipse = False, pointsize = 2, 
             main = "Cluster Visualization for Keywords and Assigned Clusters for Sectors")

plt.show()










import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from factor_analyzer import FactorAnalyzer
import matplotlib.pyplot as plt

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
sector_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings
keywords = ["keyword1", "keyword2", ...]  # Replace with your actual keywords
sectors = ["sector1", "sector2", ...]  # Replace with your actual sectors

# Perform k-means clustering on keyword embeddings
n_clusters = len(keywords)
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels_keywords = kmeans.fit_predict(keyword_embeddings)

# Find the most similar keyword for each sector
cluster_assignments_sectors = []
for sector_embedding in sector_embeddings:
    similarities = cosine_similarity([sector_embedding], keyword_embeddings)[0]
    most_similar_keyword_index = np.argmax(similarities)
    cluster_assignments_sectors.append(cluster_labels_keywords[most_similar_keyword_index])

# Create a DataFrame for factor analysis
data = pd.DataFrame(keyword_embeddings, columns=[f"dim{i+1}" for i in range(keyword_embeddings.shape[1])])

# Factor Analysis
fa = FactorAnalyzer(n_factors=n_clusters, rotation=None)
fa.fit(data)

# Visualize factor loadings
loadings = fa.loadings_
plt.figure(figsize=(8, 6))
plt.scatter(loadings[:, 0], loadings[:, 1], color='blue')
for i, txt in enumerate(keywords):
    plt.annotate(txt, (loadings[i, 0], loadings[i, 1]), fontsize=8)
plt.title('Factor Loadings for Keywords')
plt.xlabel('Factor 1 Loading')
plt.ylabel('Factor 2 Loading')
plt.show()

# Visualize assigned clusters for sectors
plt.figure(figsize=(8, 6))
plt.scatter(loadings[:, 0], loadings[:, 1], color='blue')
for i, txt in enumerate(keywords):
    plt.annotate(txt, (loadings[i, 0], loadings[i, 1]), fontsize=8)
for i, sector in enumerate(sectors):
    plt.scatter(loadings[i, 0], loadings[i, 1], color='red')
    plt.annotate(sector, (loadings[i, 0], loadings[i, 1]), fontsize=8)
plt.title('Factor Loadings for Keywords with Assigned Clusters for Sectors')
plt.xlabel('Factor 1 Loading')
plt.ylabel('Factor 2 Loading')
plt.show()










import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
sector_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings
keywords = ["keyword1", "keyword2", ...]  # Replace with your actual keywords
sectors = ["sector1", "sector2", ...]  # Replace with your actual sectors

# Perform k-means clustering on keyword embeddings
n_clusters = len(keywords)
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels_keywords = kmeans.fit_predict(keyword_embeddings)

# Find the most similar keyword for each sector
cluster_assignments_sectors = []
for sector_embedding in sector_embeddings:
    similarities = cosine_similarity([sector_embedding], keyword_embeddings).flatten()
    most_similar_keyword_index = np.argmax(similarities)
    cluster_assignments_sectors.append(cluster_labels_keywords[most_similar_keyword_index])

# Visualize clustered data using t-SNE
all_embeddings = np.concatenate([keyword_embeddings, sector_embeddings], axis=0)
tsne = TSNE(n_components=2, random_state=42)
embedded = tsne.fit_transform(all_embeddings)

# Separate the embeddings back into keyword and sector
keyword_embedded = embedded[:len(keyword_embeddings)]
sector_embedded = embedded[len(keyword_embeddings):]

# Plotting t-SNE results with cluster labels for keywords and assigned clusters for sectors
plt.figure(figsize=(8, 8))

# Plotting keywords
for i, keyword in enumerate(keywords):
    plt.scatter(keyword_embedded[i, 0], keyword_embedded[i, 1], label=f'Cluster {cluster_labels_keywords[i]}', marker='o')
    plt.text(keyword_embedded[i, 0], keyword_embedded[i, 1], keywords[i], fontsize=8)

# Plotting sectors
for i, sector in enumerate(sectors):
    assigned_cluster = cluster_assignments_sectors[i]
    plt.scatter(sector_embedded[i, 0], sector_embedded[i, 1], label=f'Assigned Cluster {assigned_cluster}', marker='x')
    plt.text(sector_embedded[i, 0], sector_embedded[i, 1], sectors[i], fontsize=8)

# Add labels and legend
plt.title('t-SNE Visualization of Word Embeddings with K-Means Cluster Labels for Keywords and Assigned Clusters for Sectors')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()















import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
sector_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings
keywords = ["keyword1", "keyword2", ...]  # Replace with your actual keywords
sectors = ["sector1", "sector2", ...]  # Replace with your actual sectors

# Perform k-means clustering on keyword embeddings
n_clusters = len(keywords)
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels_keywords = kmeans.fit_predict(keyword_embeddings)

# Find the most similar keyword for each sector
cluster_assignments_sectors = []
for sector_embedding in sector_embeddings:
    similarities = cosine_similarity([sector_embedding], keyword_embeddings)[0]
    most_similar_keyword_indices = np.argsort(similarities)[::-1]
    most_similar_keyword_index = most_similar_keyword_indices[0]
    cluster_assignments_sectors.append(cluster_labels_keywords[most_similar_keyword_index])

# Visualize clustered data using t-SNE
all_embeddings = np.concatenate([keyword_embeddings, sector_embeddings], axis=0)
tsne = TSNE(n_components=2, random_state=42)
embedded = tsne.fit_transform(all_embeddings)

# Separate the embeddings back into keyword and sector
keyword_embedded = embedded[:len(keyword_embeddings)]
sector_embedded = embedded[len(keyword_embeddings):]

# Plotting t-SNE results with cluster labels for keywords and assigned clusters for sectors
plt.figure(figsize=(8, 8))

# Plotting keywords
for i, keyword in enumerate(keywords):
    plt.scatter(keyword_embedded[i, 0], keyword_embedded[i, 1], label=f'Cluster {cluster_labels_keywords[i]}', marker='o')
    plt.text(keyword_embedded[i, 0], keyword_embedded[i, 1], keywords[i], fontsize=8)

# Plotting sectors
for i, sector in enumerate(sectors):
    assigned_cluster = cluster_assignments_sectors[i]
    plt.scatter(sector_embedded[i, 0], sector_embedded[i, 1], label=f'Assigned Cluster {assigned_cluster}', marker='x')
    plt.text(sector_embedded[i, 0], sector_embedded[i, 1], sectors[i], fontsize=8)

# Add labels and legend
plt.title('t-SNE Visualization of Word Embeddings with K-Means Cluster Labels for Keywords and Assigned Clusters for Sectors')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()












import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
sector_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings
keywords = ["keyword1", "keyword2", ...]  # Replace with your actual keywords
sectors = ["sector1", "sector2", ...]  # Replace with your actual sectors

# Perform k-means clustering on keyword embeddings
n_clusters = len(keywords)
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels_keywords = kmeans.fit_predict(keyword_embeddings)

# Find the most similar keyword for each sector
cluster_assignments_sectors = []
for sector_embedding in sector_embeddings:
    similarities = cosine_similarity([sector_embedding], keyword_embeddings)[0]
    most_similar_keyword_index = np.argmax(similarities)
    cluster_assignments_sectors.append(cluster_labels_keywords[most_similar_keyword_index])

# Visualize clustered data using t-SNE
all_embeddings = np.concatenate([keyword_embeddings, sector_embeddings], axis=0)
tsne = TSNE(n_components=2, random_state=42)
embedded = tsne.fit_transform(all_embeddings)

# Separate the embeddings back into keyword and sector
keyword_embedded = embedded[:len(keyword_embeddings)]
sector_embedded = embedded[len(keyword_embeddings):]

# Plotting t-SNE results with cluster labels for keywords and assigned clusters for sectors
plt.figure(figsize=(8, 8))

# Plotting keywords
for i, keyword in enumerate(keywords):
    plt.scatter(keyword_embedded[i, 0], keyword_embedded[i, 1], label=f'Cluster {cluster_labels_keywords[i]}', marker='o')
    plt.text(keyword_embedded[i, 0], keyword_embedded[i, 1], keywords[i], fontsize=8)

# Plotting sectors
for i, sector in enumerate(sectors):
    assigned_cluster = cluster_assignments_sectors[i]
    plt.scatter(sector_embedded[i, 0], sector_embedded[i, 1], label=f'Assigned Cluster {assigned_cluster}', marker='x')
    plt.text(sector_embedded[i, 0], sector_embedded[i, 1], sectors[i], fontsize=8)

# Add labels and legend
plt.title('t-SNE Visualization of Word Embeddings with K-Means Cluster Labels for Keywords and Assigned Clusters for Sectors')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()














import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
client_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings
similarity_scores = np.array([0.8, 0.7, 0.9, ...])  # Replace with your actual similarity scores
keywords = ["keyword1", "keyword2", ...]  # Replace with your actual keywords
clients = ["client1", "client2", ...]  # Replace with your actual client sectors

# Perform k-means clustering on keyword embeddings
n_clusters = len(keywords)
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels_keywords = kmeans.fit_predict(keyword_embeddings)

# Find the most similar keyword for each client
cluster_assignments_clients = []
for client_embedding in client_embeddings:
    similarities = cosine_similarity([client_embedding], keyword_embeddings)[0]
    most_similar_keyword_index = np.argmax(similarities)
    cluster_assignments_clients.append(cluster_labels_keywords[most_similar_keyword_index])

# Visualize clustered data using t-SNE
all_embeddings = np.concatenate([keyword_embeddings, client_embeddings], axis=0)
tsne = TSNE(n_components=2, random_state=42)
embedded = tsne.fit_transform(all_embeddings)

# Separate the embeddings back into keyword and client sectors
keyword_embedded = embedded[:len(keyword_embeddings)]
client_embedded = embedded[len(keyword_embeddings):]

# Plotting t-SNE results with cluster labels for keywords and assigned clusters for clients
plt.figure(figsize=(8, 8))

# Plotting keywords
for i, keyword in enumerate(keywords):
    plt.scatter(keyword_embedded[i, 0], keyword_embedded[i, 1], label=f'Cluster {cluster_labels_keywords[i]}', marker='o')
    plt.text(keyword_embedded[i, 0], keyword_embedded[i, 1], keywords[i], fontsize=8)

# Plotting client sectors
for i, client in enumerate(clients):
    assigned_cluster = cluster_assignments_clients[i]
    plt.scatter(client_embedded[i, 0], client_embedded[i, 1], label=f'Assigned Cluster {assigned_cluster}', marker='x')
    plt.text(client_embedded[i, 0], client_embedded[i, 1], clients[i], fontsize=8)

# Add labels and legend
plt.title('t-SNE Visualization of Word Embeddings with K-Means Cluster Labels for Keywords and Assigned Clusters for Clients')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()









import numpy as np
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
client_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings
similarity_scores = np.array([0.8, 0.7, 0.9, ...])  # Replace with your actual similarity scores
keywords = ["keyword1", "keyword2", ...]  # Replace with your actual keywords
clients = ["client1", "client2", ...]  # Replace with your actual client sectors

# Concatenate keyword and client embeddings
all_embeddings = np.concatenate([keyword_embeddings, client_embeddings], axis=0)

# Perform k-means clustering
n_clusters = len(keywords)  # Set the number of clusters equal to the number of keywords
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(all_embeddings)

# Visualize clustered data using t-SNE
tsne = TSNE(n_components=2, random_state=42)
embedded = tsne.fit_transform(all_embeddings)

# Separate the embeddings back into keyword and client sectors
keyword_embedded = embedded[:len(keyword_embeddings)]
client_embedded = embedded[len(keyword_embeddings):]

# Plotting t-SNE results with cluster labels
plt.figure(figsize=(8, 8))

# Plotting keywords
for i, keyword in enumerate(keywords):
    plt.scatter(keyword_embedded[i, 0], keyword_embedded[i, 1], label=f'Cluster {cluster_labels[i]}', marker='o')
    plt.text(keyword_embedded[i, 0], keyword_embedded[i, 1], keywords[i], fontsize=8)

# Plotting client sectors
for i, client in enumerate(clients):
    plt.scatter(client_embedded[i, 0], client_embedded[i, 1], label=f'Cluster {cluster_labels[i + len(keyword_embeddings)]}', marker='x')
    plt.text(client_embedded[i, 0], client_embedded[i, 1], clients[i], fontsize=8)

# Add labels and legend
plt.title('t-SNE Visualization of Word Embeddings with K-Means Cluster Labels')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()









import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
client_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings
keywords = ["keyword1", "keyword2", ...]  # Replace with your actual keywords
clients = ["client1", "client2", ...]  # Replace with your actual client sectors

# Concatenate keyword and client embeddings
all_embeddings = np.concatenate([keyword_embeddings, client_embeddings], axis=0)

# Perform t-SNE
tsne = TSNE(n_components=2, random_state=42)
embedded = tsne.fit_transform(all_embeddings)

# Separate the embeddings back into keyword and client sectors
keyword_embedded = embedded[:len(keyword_embeddings)]
client_embedded = embedded[len(keyword_embeddings):]

# Plotting t-SNE results with text annotations
plt.figure(figsize=(8, 8))

# Plotting keywords
for i, keyword in enumerate(keywords):
    plt.scatter(keyword_embedded[i, 0], keyword_embedded[i, 1], label='Keyword', marker='o', color='blue')
    plt.text(keyword_embedded[i, 0], keyword_embedded[i, 1], keywords[i], fontsize=8, color='blue')

# Plotting client sectors
for i, client in enumerate(clients):
    plt.scatter(client_embedded[i, 0], client_embedded[i, 1], label='Client', marker='x', color='orange')
    plt.text(client_embedded[i, 0], client_embedded[i, 1], clients[i], fontsize=8, color='orange')

# Add labels and legend
plt.title('t-SNE Visualization of Word Embeddings with Text Annotations')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()







import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Assuming you have word embeddings for keywords and client sectors of activity
keyword_embeddings = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], ...])  # Replace with your actual embeddings
client_embeddings = np.array([[0.4, 0.5, 0.6], [0.5, 0.6, 0.7], ...])  # Replace with your actual embeddings

# Concatenate keyword and client embeddings
all_embeddings = np.concatenate([keyword_embeddings, client_embeddings], axis=0)

# Perform t-SNE
tsne = TSNE(n_components=2, random_state=42)
embedded = tsne.fit_transform(all_embeddings)

# Separate the embeddings back into keyword and client sectors
keyword_embedded = embedded[:len(keyword_embeddings)]
client_embedded = embedded[len(keyword_embeddings):]

# Plotting t-SNE results
plt.figure(figsize=(8, 8))
plt.scatter(keyword_embedded[:, 0], keyword_embedded[:, 1], label='Keyword', marker='o')
plt.scatter(client_embedded[:, 0], client_embedded[:, 1], label='Client', marker='x')

# Add labels and legend
plt.title('t-SNE Visualization of Word Embeddings')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()





import networkx as nx
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'df' with columns 'sector', 'keyword', and 'similarity'
# Replace this with your actual DataFrame and column names
# df = ...

# Create a graph
G = nx.Graph()

# Add nodes and edges based on your data
for index, row in df.iterrows():
    G.add_node(row['sector'])
    G.add_node(row['keyword'])
    G.add_edge(row['sector'], row['keyword'], weight=row['similarity'])

# Position nodes based on sectors
pos = nx.spring_layout(G, seed=42)

# Draw the graph with node colors and sizes based on similarity scores
edge_labels = {(i, j): f"{G[i][j]['weight']:.2f}" for i, j in G.edges()}
nx.draw_networkx_nodes(G, pos, node_size=700)
nx.draw_networkx_labels(G, pos)
nx.draw_networkx_edges(G, pos)
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

plt.show()






import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have your data in a list of tuples like (sector, keyword, similarity_score)
data = [("Finance", "investment", 0.8),
        ("Technology", "innovation", 0.6),
        ("Healthcare", "medical", 0.7),
        # Add more data as needed
       ]

# Unpack data into separate lists
sectors, keywords, similarity_scores = zip(*data)

# Create a DataFrame for easier plotting with Seaborn
import pandas as pd
df = pd.DataFrame({'Sector': sectors, 'Keyword': keywords, 'Similarity Score': similarity_scores})

# Plotting the graph using Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Sector', y='Keyword', hue='Similarity Score', palette='viridis', s=100, alpha=0.8)

# Adding labels and legend
plt.xlabel('Sector of Activity')
plt.ylabel('Related Keywords')
plt.title('Sector of Activity vs. Related Keywords')
plt.legend(title='Similarity Score')

# Display the plot
plt.show()










from sklearn.cluster import KMeans
import numpy as np

# Replace these with your actual class and keyword embeddings
class_embeddings = [...]  # Your class embeddings
keyword_embeddings = [...]  # Your keyword embeddings

# Combine class and keyword embeddings into a single matrix
data_matrix = np.concatenate((class_embeddings, keyword_embeddings), axis=0)

# Determine the number of clusters based on the number of classes
num_clusters = len(class_embeddings)

# Apply K-means clustering
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters = kmeans.fit_predict(data_matrix)

# Assign each keyword to a cluster
clustered_data = {}
for i, cluster in enumerate(clusters):
    if cluster not in clustered_data:
        clustered_data[cluster] = []
    clustered_data[cluster].append((keywords[i], class_embeddings[i]))

# Print or further analyze the clustered data
for cluster, data_points in clustered_data.items():
    print(f"Cluster {cluster + 1}:")
    for data_point in data_points:
        print(f"Keyword: {data_point[0]}, Class Embedding: {data_point[1]}")
    print("\n")








**Rapport sur l'Analyse de Similarité Sémantique entre les Mots-Clés et les Secteurs d'Activité Utilisant les Embeddings de Mots**

*Introduction :*
Cette étude se concentre sur l'évaluation de la similarité sémantique entre un fichier de mots-clés et les secteurs d'activité en utilisant les embeddings de mots. L'objectif est de classifier les secteurs en fonction des relations sémantiques identifiées à travers les embeddings de mots.

*Méthodologie :*
1. **Entrée de Données :**
   - Utilisation de deux fichiers d'entrée :
      - Fichier de Mots-Clés : Contient des mots-clés pertinents pour l'analyse.
      - Fichier de Secteurs : Englobe les secteurs d'activité à classer.

2. **Embeddings de Mots :**
   - Application de techniques d'embedding de mots pour représenter les mots sous forme de vecteurs dans un espace multidimensionnel.
   - Utilisation d'embeddings pré-entraînés ou d'embeddings entraînés sur les données fournies.

3. **Calcul de la Similarité Sémantique :**
   - Mesure de la similarité sémantique entre les mots-clés et les secteurs en utilisant la similarité cosinus ou d'autres métriques appropriées.
   - Établissement d'un seuil de classification basé sur les scores de similarité.

*Résultats :*
   - Identification des relations sémantiques entre les mots-clés et les secteurs.
   - Classification des secteurs en fonction du seuil établi pour la similarité.

*Conclusion :*
   - Résumé des résultats et de leur pertinence par rapport à l'objectif.
   - Suggestions pour de futures recherches ou améliorations de la méthodologie.

*Recommandations :*
   - Recommandations pour affiner le processus de classification ou explorer des approches alternatives.

*Remerciements :*
   - Remerciements pour tout outil, ensemble de données ou ressource utilisé dans l'analyse.

*Références :*
   - Citation de la littérature pertinente, des outils ou des méthodologies utilisés.

*Annexe :*
   - Inclusion de toute information supplémentaire, extraits de code ou détails additionnels soutenant l'analyse.

*Note :*
Ce rapport offre une vue d'ensemble concise de l'analyse de similarité sémantique réalisée entre les mots-clés et les secteurs à l'aide des embeddings de mots, en mettant l'accent sur la méthodologie employée. Pour une compréhension plus détaillée, veuillez vous référer à l'annexe jointe et aux matériaux supplémentaires.




https://designs.ai/imagemaker/start/en


https://www.craiyon.com/


https://starryai.com/app/create/art?project=


https://app.jasper.ai/?signInWithGoogle=popup

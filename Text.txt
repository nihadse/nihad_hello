from fuzzywuzzy import fuzz

def find_matching_clients(clients_list, interdicted_cheques_list, threshold=70):
    matches = []
    for client in clients_list:
        for interdicted_name in interdicted_cheques_list:
            similarity_ratio = fuzz.ratio(client, interdicted_name)
            if similarity_ratio >= threshold:
                matches.append((client, interdicted_name, similarity_ratio))
    return matches

clients_list = ["nihad", "bnp tahraoui makhlouf", "pharma", "omar", "naciri", "ahlam", 
                "salma", "hafida", "fouzia", "samia", "sifo", "atika", "senhadji ninaf"]

interdicted_cheques_list = ["nihrde", "spa pbn ", "eurl tahraoui makhlou", "pharma sarl", "translates"]

matches = find_matching_clients(clients_list, interdicted_cheques_list)

if matches:
    print("Matches found:")
    for match in matches:
        print(f"Client: '{match[0]}' matches with Interdicted Cheque Name: '{match[1]}' (Similarity: {match[2]}%)")
else:
    print("No matches found.")




from collections import Counter

# Example list of names
names_list = ["nihad senhadji", "bessai Fatiha", "Fatiha"]

# Tokenize the names
tokens = []
for name in names_list:
    tokens.extend(name.lower().split())

# Count the occurrences of each token
token_counts = Counter(tokens)

# Print the token counts
for token, count in token_counts.items():
    print(f"{token}: {count}")



"Clients Interdits ChÃ©quier"



import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Sample data (replace with your actual data loading process)
clients_list = [
    "John Doe",
    "Alice Smith",
    "Michael Johnson",
    "Emily Brown",
    "Robert Lee",
    "Jennifer Davis",
    "David Wilson",
    "Sarah Martin",
    "Daniel Thompson",
    "Lisa Garcia"
]

# Sample monthly file data (replace with your actual data loading process)
monthly_file_df = pd.DataFrame({
    'Client_Name': [
        "John D.",
        "Alice S.",
        "Michael J.",
        "Emely B.",
        "Robert L.",
        "Jane Davis",
        "David Williams",
        "Daniel Thomson"
    ]
})

# Load the pre-trained Sentence Transformer model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Encode client names
client_embeddings = model.encode(clients_list, convert_to_tensor=True)
monthly_embeddings = model.encode(monthly_file_df['Client_Name'].tolist(), convert_to_tensor=True)

# Calculate cosine similarity matrix
similarity_matrix = util.pytorch_cos_sim(monthly_embeddings, client_embeddings)

# Prepare to collect the most similar pairs
max_similarity_data = {'Monthly_Client_Name': [], 'Our_Client_Name': [], 'Similarity_Score': []}

# Set a threshold for similarity score
threshold = 0.85  # Adjust as per your requirement

# Find the most similar client name for each monthly file client
for i in range(len(monthly_file_df)):
    # Find index of the our client name with max similarity score
    max_sim_index = similarity_matrix[i].argmax().item()
    max_sim_score = similarity_matrix[i][max_sim_index].item()
    
    # Append the most similar pair if similarity score meets threshold
    if max_sim_score >= threshold:
        max_similarity_data['Monthly_Client_Name'].append(monthly_file_df['Client_Name'].iloc[i])
        max_similarity_data['Our_Client_Name'].append(clients_list[max_sim_index])
        max_similarity_data['Similarity_Score'].append(max_sim_score)

# Create DataFrame from collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Print the resulting DataFrame
print(max_similarity_df)





import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Sample data sizes
num_clients = 218000
num_interdicted_cheques = 1014

# Example lists (replace with your actual data loading process)
clients_list = [...]  # Load your 218000 client names
interdicted_cheques_list = [...]  # Load your 1014 interdicted cheques

# Load the pre-trained Sentence Transformer model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Encode client names and interdicted cheques
client_embeddings = model.encode(clients_list, convert_to_tensor=True)
interdicted_embeddings = model.encode(interdicted_cheques_list, convert_to_tensor=True)

# Calculate cosine similarity matrix
similarity_matrix = util.pytorch_cos_sim(client_embeddings, interdicted_embeddings)

# Prepare to collect the most similar pairs
max_similarity_data = {'Client_Name': [], 'Interdicted_Name': [], 'Similarity_Score': []}

# Find the most similar interdicted cheque for each client
for i in range(num_clients):
    # Find index of the interdicted cheque with max similarity score
    max_sim_index = similarity_matrix[i].argmax().item()
    max_sim_score = similarity_matrix[i][max_sim_index].item()
    
    # Append the most similar pair to the data dictionary
    max_similarity_data['Client_Name'].append(clients_list[i])
    max_similarity_data['Interdicted_Name'].append(interdicted_cheques_list[max_sim_index])
    max_similarity_data['Similarity_Score'].append(max_sim_score)

    # Print progress
    if (i + 1) % 1000 == 0:
        print(f"Processed {i + 1} clients...")

# Create DataFrame from collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Print the resulting DataFrame
print(max_similarity_df)




import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Prepare the data
clients_list = ["nihad", "pharma", "naciri", "ahlam", "salma", "hafida", "fouzia", "samia", "sifo", "atika", "senhadji nihaf"]
interdicted_cheques_list = ["nihrde", "spa", "pharma sarl", "translates"]

# Load the model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Encode the names
client_embeddings = model.encode(clients_list, convert_to_tensor=True)
interdicted_embeddings = model.encode(interdicted_cheques_list, convert_to_tensor=True)

# Calculate similarity matrix
similarity_matrix = util.pytorch_cos_sim(client_embeddings, interdicted_embeddings)

# Find the most similar pair
max_similarity_data = {'Client_Name': [], 'Interdicted_Name': [], 'Similarity_Score': []}

for i in range(len(clients_list)):
    # Find the index of the interdicted name with max similarity
    max_sim_index = similarity_matrix[i].argmax().item()
    max_sim_score = similarity_matrix[i][max_sim_index].item()
    
    # Add the pair to max_similarity_data if similarity score is above threshold
    if max_sim_score > 0.50:  # Adjust threshold as needed
        max_similarity_data['Client_Name'].append(clients_list[i])
        max_similarity_data['Interdicted_Name'].append(interdicted_cheques_list[max_sim_index])
        max_similarity_data['Similarity_Score'].append(max_sim_score)

# Create DataFrame from max similarity data
max_similarity_df = pd.DataFrame(max_similarity_data)
print(max_similarity_df)






import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Prepare the data
clients_list = ["nihad", "pharma", "naciri", "ahlam", "salma", "hafida", "fouzia", "samia", "sifo", "atika", "senhadji nihaf"]
interdicted_cheques_list = ["nihrde", "spa", "pharma sarl", "translates"]

# Load the model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Encode the names
client_embeddings = model.encode(clients_list, convert_to_tensor=True)
interdicted_embeddings = model.encode(interdicted_cheques_list, convert_to_tensor=True)

# Calculate similarity matrix
similarity_matrix = util.pytorch_cos_sim(client_embeddings, interdicted_embeddings)

# Define threshold and find matches
threshold = 0.50
matched_data = {'Client_Name': [], 'Interdicted_Name': [], 'Similarity_Score': []}

for i in range(len(clients_list)):
    for j in range(len(interdicted_cheques_list)):
        similarity_score = similarity_matrix[i][j].item()
        if similarity_score > threshold:
            matched_data['Client_Name'].append(clients_list[i])
            matched_data['Interdicted_Name'].append(interdicted_cheques_list[j])
            matched_data['Similarity_Score'].append(similarity_score)

# Create DataFrame from matches
matches_df = pd.DataFrame(matched_data)
print(matches_df)




from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Example lists (replace these with your actual data)
interdicted_clients = [
    "John Doe",
    "Jane Smith",
    "Alice Johnson",
    "Robert Brown",
    "Michael Clark"
]

client_database = [
    "Jon Doe",
    "Jane Smith",
    "Alice Johnson",
    "Robert Brown",
    "Michelle Clark"
]

# Initialize SentenceTransformer model
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')

# Encode names into embeddings
interdicted_embeddings = model.encode(interdicted_clients)
client_embeddings = model.encode(client_database)

# Function to find the best match using cosine similarity
def find_best_match(embedding, embeddings_list, names_list):
    similarities = cosine_similarity([embedding], embeddings_list)[0]
    best_match_index = np.argmax(similarities)
    best_match_name = names_list[best_match_index]
    best_similarity_score = similarities[best_match_index]
    return best_match_name, best_similarity_score

# Find matches and print results
print("Matches found:")
for i, interdicted_name in enumerate(interdicted_clients):
    best_match, similarity_score = find_best_match(interdicted_embeddings[i], client_embeddings, client_database)
    print(f"{interdicted_name} -> {best_match} (Similarity Score: {similarity_score:.4f})")




from sentence_transformers import SentenceTransformer, util
import pandas as pd

# Function to calculate embeddings
def calculate_embeddings(names_list, model):
    return model.encode(names_list, convert_to_tensor=True)

# Load the model (adjust modelPath as needed)
model = SentenceTransformer('modelPath')

# Sample Data (replace with actual data)
client_database = ['client_name1', 'client_name2', ...]  # 218,000 entries
interdicted_checks_clients = ['cheque_name1', 'cheque_name2', ...]  # 1,104 entries

# Calculate embeddings for both lists
client_database_embeddings = calculate_embeddings(client_database, model)
interdicted_checks_clients_embeddings = calculate_embeddings(interdicted_checks_clients, model)

# Calculate cosine similarity between embeddings (returns a tensor)
similarity_matrix_tensor = util.pytorch_cos_sim(client_database_embeddings, interdicted_checks_clients_embeddings)

# Initialize lists to store max similarity results
max_similarity_data = {'Client_Name': [], 'Cheque_Name': [], 'Max_Similarity_Score': []}

# Iterate over interdicted cheque list to find max similarity for each
for j in range(len(interdicted_checks_clients)):
    max_similarity_index = similarity_matrix_tensor[:, j].argmax().item()
    max_similarity_score = similarity_matrix_tensor[max_similarity_index, j].item()
    max_client_name = client_database[max_similarity_index]

    # Store the results
    max_similarity_data['Client_Name'].append(max_client_name)
    max_similarity_data['Cheque_Name'].append(interdicted_checks_clients[j])
    max_similarity_data['Max_Similarity_Score'].append(max_similarity_score)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

print("Max Similarity DataFrame:\n", max_similarity_df)






from sentence_transformers import SentenceTransformer, util
import pandas as pd

# Function to calculate embeddings
def calculate_embeddings(list1, model):
    return model.encode(list1, convert_to_tensor=True)

# Load the model (adjust modelPath as needed)
model = SentenceTransformer('modelPath')

# Sample Data (replace with actual data)
clients = ['client_name1', 'client_name2', 'client_name3', 'client_name4']
interdicted_cheques = ['cheque_name1', 'cheque_name2', 'cheque_name3', 'cheque_name4']

# Calculate embeddings for both lists
clients_embeddings = calculate_embeddings(clients, model)
interdicted_cheques_embeddings = calculate_embeddings(interdicted_cheques, model)

# Calculate cosine similarity between embeddings
similarity_matrix = util.pytorch_cos_sim(clients_embeddings, interdicted_cheques_embeddings).numpy()

# Display the similarity matrix
print(similarity_matrix)

# Create DataFrame to store the max similarity results
max_similarity_data = {'Client_Name': [], 'Cheque_Name': [], 'Similarity_Score': []}

# Extract max similarity for each interdicted cheque name
threshold = 0.80

for i in range(len(interdicted_cheques)):
    for j in range(len(clients)):
        similarity_score = similarity_matrix[j][i]

        if similarity_score >= threshold:
            max_similarity_data['Client_Name'].append(clients[j])
            max_similarity_data['Cheque_Name'].append(interdicted_cheques[i])
            max_similarity_data['Similarity_Score'].append(similarity_score)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

print(max_similarity_df)



from sentence_transformers import SentenceTransformer, util
import pandas as pd

# Function to calculate pairwise similarity
def calculate_similarity(list1, list2, model):
    embeddings1 = model.encode(list1, convert_to_tensor=True)
    embeddings2 = model.encode(list2, convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2)
    return similarity_matrix

# Load the model (adjust modelPath as needed)
model = SentenceTransformer('modelPath')

# Sample Data
clients = ['client_name1', 'client_name2', 'client_name3', 'client_name4']
interdicted_cheques = ['cheque_name1', 'cheque_name2', 'cheque_name3', 'cheque_name4']

# Calculate similarity matrix
similarity_matrix = calculate_similarity(clients, interdicted_cheques, model)

# Create DataFrame to store the max similarity results
max_similarity_data = {'Client_Name': [], 'Cheque_Name': [], 'Similarity_Score': []}

# Extract max similarity for each interdicted cheque name
threshold = 0.80

for i in range(len(interdicted_cheques)):
    for j in range(len(clients)):
        similarity_score = similarity_matrix[j][i].item()

        if similarity_score >= threshold:
            max_similarity_data['Client_Name'].append(clients[j])
            max_similarity_data['Cheque_Name'].append(interdicted_cheques[i])
            max_similarity_data['Similarity_Score'].append(similarity_score)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

print(max_similarity_df)




import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util

# Load the model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Example data (replace these with your actual data)
client_database = ["Client Name 1", "Client Name 2", "Client Name 3"]  # Replace with your actual client data
interdicted_checks = ["Client Name A", "Client Name B", "Client Name 1"]  # Replace with your actual interdicted check data

# Get embeddings for client database and interdicted checks
client_database_embeddings = model.encode(client_database, convert_to_tensor=True)
interdicted_checks_embeddings = model.encode(interdicted_checks, convert_to_tensor=True)

# Calculate cosine similarity between client database and interdicted checks embeddings
similarity_matrix = util.pytorch_cos_sim(interdicted_checks_embeddings, client_database_embeddings).numpy()

# Create DataFrame to store the max similarity results
max_similarity_data = {
    "Interdicted_Name": [],
    "Matched_Client_Name": [],
    "Max_Similarity_Score": []
}

# Set the similarity threshold
threshold = 0.8
considered_pairs = set()  # Keep track of considered pairs to avoid duplicates

# Extract max similarity for each interdicted check name
for i in range(len(interdicted_checks)):
    similarity_row = similarity_matrix[i]
    max_similarity_index = np.argmax(similarity_row)
    max_similarity_score = similarity_row[max_similarity_index]
    
    if max_similarity_score >= threshold:
        current_pair = (interdicted_checks[i], client_database[max_similarity_index])
        reversed_pair = (client_database[max_similarity_index], interdicted_checks[i])
        
        if current_pair not in considered_pairs and reversed_pair not in considered_pairs:
            considered_pairs.add(current_pair)
            
            max_similarity_data["Interdicted_Name"].append(interdicted_checks[i])
            max_similarity_data["Matched_Client_Name"].append(client_database[max_similarity_index])
            max_similarity_data["Max_Similarity_Score"].append(max_similarity_score)

# Create DataFrame from the collected data
similarity_df = pd.DataFrame(max_similarity_data)

# Drop duplicate rows
similarity_df.drop_duplicates(inplace=True)

# Remove rows where 'Interdicted_Name' appears in 'Matched_Client_Name' to avoid redundancy
final_df = similarity_df[~similarity_df['Matched_Client_Name'].isin(similarity_df['Interdicted_Name'])]

print(final_df)




import pandas as pd

# Sample DataFrame with similar and different values
data = {
    'Column1': ['A', 'B', 'C', 'D'],
    'Column2': ['A', 'Y', 'C', 'X']
}

df = pd.DataFrame(data)

# Concatenating with condition to keep only one value if they are similar, with a space when concatenating
df['Concatenated'] = df.apply(
    lambda row: row['Column1'] if row['Column1'] == row['Column2'] else row['Column1'] + ' ' + row['Column2'], axis=1
)

print(df)




def normalize_names(row):
    name = row['name'] if pd.notna(row['name']) else ''
    family_name = row['family_name'] if pd.notna(row['family_name']) else ''

    # Strip whitespace
    name = name.strip()
    family_name = family_name.strip()

    # If both columns are empty, return a placeholder
    if name == '' and family_name == '':
        return 'unknown'
    
    # If both columns are non-empty and identical, keep one
    if name.lower() == family_name.lower():
        return name
    
    # If one of the columns is empty, use the non-empty column
    if name == '':
        return family_name
    if family_name == '':
        return name
    
    # Combine both fields if they are distinct
    return f"{name} {family_name}"



import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

# Load the datasets
interdicted_checks = pd.read_csv('interdicted_checks.csv')
client_database = pd.read_csv('client_database.csv')

# Data preprocessing function with handling for 'srl' and misaligned columns
def preprocess_name(name):
    name = name.lower()  # Convert to lowercase
    name = re.sub(r'[^a-z\s]', '', name)  # Remove special characters
    name = re.sub(r'\s+', ' ', name).strip()  # Remove extra whitespace
    if name.endswith(' srl'):
        name = 'srl ' + name[:-4].strip()  # Move 'srl' to the beginning
    return name

def normalize_names(row):
    # If both columns are non-empty and identical, keep one
    if pd.notna(row['name']) and pd.notna(row['family_name']):
        if row['name'].strip().lower() == row['family_name'].strip().lower():
            return row['name'].strip()
    # If family_name is empty, combine columns
    if pd.isna(row['family_name']) or row['family_name'].strip() == '':
        return row['name'].strip()
    # Combine both fields if they are distinct
    return f"{row['name'].strip()} {row['family_name'].strip()}"

# Apply normalization
interdicted_checks['combined_name'] = interdicted_checks.apply(normalize_names, axis=1)
client_database['combined_name'] = client_database.apply(normalize_names, axis=1)

# Apply preprocessing
interdicted_checks['clean_name'] = interdicted_checks['combined_name'].apply(preprocess_name)
client_database['clean_name'] = client_database['combined_name'].apply(preprocess_name)

# Vectorize names using TF-IDF
vectorizer = TfidfVectorizer().fit(pd.concat([interdicted_checks['clean_name'], client_database['clean_name']]))
interdicted_vectors = vectorizer.transform(interdicted_checks['clean_name'])
client_vectors = vectorizer.transform(client_database['clean_name'])

# Compute cosine similarity
similarity_matrix = cosine_similarity(interdicted_vectors, client_vectors)

# Identify the most similar names
threshold = 0.8  # Similarity threshold for considering a match
matches = []
for i, row in enumerate(similarity_matrix):
    max_sim = np.max(row)
    if max_sim >= threshold:
        client_index = np.argmax(row)
        matches.append((interdicted_checks['name'][i], client_database['name'][client_index], max_sim))

# Create a DataFrame of matches
matches_df = pd.DataFrame(matches, columns=['Interdicted Name', 'Client Name', 'Similarity'])
print(matches_df)

# Save the matches to a CSV file
matches_df.to_csv('matches.csv', index=False)






import pandas as pd
import numpy as np
import re
from sentence_transformers import SentenceTransformer, util

# Load the transformer model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Load the datasets
interdicted_checks = pd.read_csv('interdicted_checks.csv')
client_database = pd.read_csv('client_database.csv')

# Data preprocessing function with handling for 'srl', 'eurl', and correction for 'sar'
def preprocess_name(name):
    name = name.lower()  # Convert to lowercase
    name = re.sub(r'[^a-z\s]', '', name)  # Remove special characters
    name = re.sub(r'\s+', ' ', name).strip()  # Remove extra whitespace
    # Correct 'sar' to 'sarl'
    if name.endswith(' sar'):
        name = name[:-3] + ' sarl'
    # Move known suffixes to the beginning
    for suffix in [' srl', ' eurl', ' sarl']:
        if name.endswith(suffix):
            name = suffix.strip() + ' ' + name[:-len(suffix)].strip()
    return name

def normalize_names(row):
    # If both columns are non-empty and identical, keep one
    if pd.notna(row['name']) and pd.notna(row['family_name']):
        if row['name'].strip().lower() == row['family_name'].strip().lower():
            return row['name'].strip()
    # If family_name is empty, combine columns
    if pd.isna(row['family_name']) or row['family_name'].strip() == '':
        return row['name'].strip()
    # Combine both fields if they are distinct
    return f"{row['name'].strip()} {row['family_name'].strip()}"

# Apply normalization
interdicted_checks['combined_name'] = interdicted_checks.apply(normalize_names, axis=1)
client_database['combined_name'] = client_database.apply(normalize_names, axis=1)

# Apply preprocessing
interdicted_checks['clean_name'] = interdicted_checks['combined_name'].apply(preprocess_name)
client_database['clean_name'] = client_database['combined_name'].apply(preprocess_name)

# Generate embeddings for names
interdicted_embeddings = model.encode(interdicted_checks['clean_name'].tolist(), convert_to_tensor=True)
client_embeddings = model.encode(client_database['clean_name'].tolist(), convert_to_tensor=True)

# Compute cosine similarity
cosine_scores = util.pytorch_cos_sim(interdicted_embeddings, client_embeddings)

# Identify the most similar names
threshold = 0.8  # Similarity threshold for considering a match
matches = []
for i in range(len(interdicted_checks)):
    for j in range(len(client_database)):
        if cosine_scores[i][j] >= threshold:
            matches.append((interdicted_checks['combined_name'][i], client_database['combined_name'][j], cosine_scores[i][j].item()))

# Create a DataFrame of matches
matches_df = pd.DataFrame(matches, columns=['Interdicted Name', 'Client Name', 'Similarity'])
print(matches_df)

# Save the matches to a CSV file
matches_df.to_csv('matches.csv', index=False)





import pandas as pd
import numpy as np
import re
from sentence_transformers import SentenceTransformer, util

# Load the transformer model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Load the datasets
interdicted_checks = pd.read_csv('interdicted_checks.csv')
client_database = pd.read_csv('client_database.csv')

# Data preprocessing function with handling for 'srl', 'eurl', and other suffixes
def preprocess_name(name):
    name = name.lower()  # Convert to lowercase
    name = re.sub(r'[^a-z\s]', '', name)  # Remove special characters
    name = re.sub(r'\s+', ' ', name).strip()  # Remove extra whitespace
    # Move known suffixes to the beginning
    for suffix in [' srl', ' eurl']:
        if name.endswith(suffix):
            name = suffix.strip() + ' ' + name[:-len(suffix)].strip()
    return name

def normalize_names(row):
    # If both columns are non-empty and identical, keep one
    if pd.notna(row['name']) and pd.notna(row['family_name']):
        if row['name'].strip().lower() == row['family_name'].strip().lower():
            return row['name'].strip()
    # If family_name is empty, combine columns
    if pd.isna(row['family_name']) or row['family_name'].strip() == '':
        return row['name'].strip()
    # Combine both fields if they are distinct
    return f"{row['name'].strip()} {row['family_name'].strip()}"

# Apply normalization
interdicted_checks['combined_name'] = interdicted_checks.apply(normalize_names, axis=1)
client_database['combined_name'] = client_database.apply(normalize_names, axis=1)

# Apply preprocessing
interdicted_checks['clean_name'] = interdicted_checks['combined_name'].apply(preprocess_name)
client_database['clean_name'] = client_database['combined_name'].apply(preprocess_name)

# Generate embeddings for names
interdicted_embeddings = model.encode(interdicted_checks['clean_name'].tolist(), convert_to_tensor=True)
client_embeddings = model.encode(client_database['clean_name'].tolist(), convert_to_tensor=True)

# Compute cosine similarity
cosine_scores = util.pytorch_cos_sim(interdicted_embeddings, client_embeddings)

# Identify the most similar names
threshold = 0.8  # Similarity threshold for considering a match
matches = []
for i in range(len(interdicted_checks)):
    for j in range(len(client_database)):
        if cosine_scores[i][j] >= threshold:
            matches.append((interdicted_checks['combined_name'][i], client_database['combined_name'][j], cosine_scores[i][j].item()))

# Create a DataFrame of matches
matches_df = pd.DataFrame(matches, columns=['Interdicted Name', 'Client Name', 'Similarity'])
print(matches_df)

# Save the matches to a CSV file
matches_df.to_csv('matches.csv', index=False)

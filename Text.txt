import pandas as pd
import json
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import load_dataset

# Step 1: Read the dataset from the Excel file
# Make sure to provide the correct path to your Excel file
df = pd.read_excel('path_to_your_file.xlsx')

# Step 2: Prepare the dataset for fine-tuning
name_pairs = []
for _, row in df.iterrows():
    # Create prompt and completion pairs from Arabic and French names
    prompt = f"Arabic: {row['name_ar']} {row['family_name_ar']}\nFrench transliterations:"
    completion = f" {row['name_fr']} {row['family_name_fr']}"
    
    name_pairs.append({
        "prompt": prompt,
        "completion": completion
    })

# Save the prepared data into a JSONL file (JSON Lines format)
with open('name_transliterations.jsonl', 'w', encoding='utf-8') as f:
    for pair in name_pairs:
        json.dump(pair, f, ensure_ascii=False)
        f.write('\n')

# Step 3: Fine-tune the GPT-2 model on the dataset
# Load the dataset from the JSONL file
dataset = load_dataset('json', data_files={'train': 'name_transliterations.jsonl'})

# Load the pre-trained GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['prompt'], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Define the training arguments for fine-tuning
training_args = TrainingArguments(
    output_dir="./results",  # Directory where the fine-tuned model will be saved
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_steps=10_000,
    save_total_limit=2,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
trainer.save_model("fine_tuned_model")

# Step 4: Generate French transliterations for Arabic names using the fine-tuned model

def generate_french_variations(arabic_name):
    # Prepare the prompt for generation
    prompt = f"Arabic: {arabic_name}\nFrench transliterations:"
    inputs = tokenizer(prompt, return_tensors="pt")

    # Generate French transliterations
    outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=5, temperature=0.7)

    # Decode and display the generated variations
    for i, output in enumerate(outputs):
        generated_text = tokenizer.decode(output, skip_special_tokens=True)
        print(f"Variation {i + 1}: {generated_text}")

# Example: Generate French variations for an Arabic name
generate_french_variations("محمد")

import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

data = {'Names_': ['nihad senhadji',
                   'nihad seohadji',
                   'nihad se_adji',
                   'nihEd senhadji',
                   'senhadjinihad',
                   'niha_ senhadji',
                   'nihad sanhadj',
                   'nihad benhadji',
                   'MME nihad senhadji',
                   'nihad sen hadji',
                   'senhadji   nihad ',
                   'nihad senhadji 7',
                   'senhadji_nihad',
                   'nihad#senhadji',
                   'MME nihad senhadji']}
df = pd.DataFrame(data)
df['Names'] = df['Names_']
# Convert all names to lowercase
df['Names'] = df['Names'].str.lower()
df['Names'] = df['Names'].str.replace('[0-9_#]', ' ', regex=True)
df['Names'] = df['Names'].str.replace('  ', ' ', regex=True)
df['Names'] = df['Names'].str.replace('mme ', '', regex=True)
df['Names'] = df['Names'].str.replace('   ', ' ', regex=True)
df['Names'] = df['Names'].str.replace('  ', ' ', regex=True)
# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the max similarity results
max_similarity_data = {'Names_': [],'Name_cleaned': [], 'max_similar_Name': [], 'Max_Similarity_Score': []}

# Extract max similarity for each name
threshold = 0
considered_pairs = set()  # Keep track of considered pairs to avoid duplicates

for i in range(len(df)):
    # Exclude self-similarity and find the max similarity
    similarity_row = similarity_matrix[i].clone()
    similarity_row[i] = 0  # Exclude self-similarity
    max_similar_index = similarity_row.argmax().item()
    max_similarity_score = similarity_matrix[i][max_similar_index].item()

    # Ensure the pair is considered only once
    current_pair = (df['Names'][i], df['Names'][max_similar_index])
    reversed_pair = (df['Names'][max_similar_index], df['Names'][i])

    if current_pair not in considered_pairs and reversed_pair not in considered_pairs and max_similarity_score >= threshold:
        considered_pairs.add(current_pair)
        max_similarity_data['Names_'].append(df['Names_'][i])
        max_similarity_data['Name_cleaned'].append(df['Names'][i])
        max_similarity_data['max_similar_Name'].append(df['Names'][max_similar_index])
        max_similarity_data['Max_Similarity_Score'].append(max_similarity_score)


# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Drop duplicate rows
max_similarity_df = max_similarity_df.drop_duplicates()

# Display the DataFrame without index numbers
print(max_similarity_df.to_string(index=False))











import pandas as pd
from sentence_transformers import SentenceTransformer, util
import torch

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise Euclidean distance
def calculate_euclidean_distance(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    euclidean_distances = torch.cdist(embeddings, embeddings, p=2)  # Using p=2 for Euclidean distance
    return euclidean_distances

# Calculate Euclidean distance matrix
euclidean_distances = calculate_euclidean_distance(df, 'Names', model)

# Create DataFrame to store the max Euclidean distance results
max_distance_data = {'Name1': [], 'Name2': [], 'Max_Euclidean_Distance': []}

# Extract max Euclidean distance for each name
threshold_distance = 1.5  # Adjust as needed
considered_pairs = set()  # Keep track of considered pairs to avoid duplicates

for i in range(len(df)):
    # Exclude self-distance and find the max distance
    distance_row = euclidean_distances[i].clone()
    distance_row[i] = float('inf')  # Exclude self-distance
    max_dissimilar_index = distance_row.argmin().item()
    max_distance = euclidean_distances[i][max_dissimilar_index].item()

    # Ensure the pair is considered only once and meets the threshold
    current_pair = (df['Names'][i], df['Names'][max_dissimilar_index])
    reversed_pair = (df['Names'][max_dissimilar_index], df['Names'][i])

    if (
        current_pair not in considered_pairs
        and reversed_pair not in considered_pairs
        and max_distance >= threshold_distance
    ):
        considered_pairs.add(current_pair)
        max_distance_data['Name1'].append(df['Names'][i])
        max_distance_data['Name2'].append(df['Names'][max_dissimilar_index])
        max_distance_data['Max_Euclidean_Distance'].append(max_distance)

# Create DataFrame from the collected data
max_distance_df = pd.DataFrame(max_distance_data)

# Drop duplicate rows
max_distance_df = max_distance_df.drop_duplicates()

# Create a new column 'Names2' in df and assign values from 'Name2' column
df['Names2'] = df['Names'].replace(dict(zip(max_distance_df['Name1'], max_distance_df['Name2'])))

# Add a new column 'Euclidean_Distance' in df
df['Euclidean_Distance'] = df.apply(lambda row: max_distance_df[
    (max_distance_df['Name1'] == row['Names']) & (max_distance_df['Name2'] == row['Names2'])
]['Max_Euclidean_Distance'].values[0] if not max_distance_df[
    (max_distance_df['Name1'] == row['Names']) & (max_distance_df['Name2'] == row['Names2'])
].empty else None, axis=1)

# Display the modified DataFrame
print(df)











import pandas as pd
from sentence_transformers import SentenceTransformer, util
from nltk.metrics import jaccard_distance

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Function to calculate pairwise Jaccard distance
def calculate_jaccard_distance(set1, set2):
    return jaccard_distance(set(set1), set(set2))

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the max similarity and Jaccard results
max_similarity_data = {'Name1': [], 'Name2': [], 'Max_Similarity_Score': [], 'Jaccard_Distance': []}

# Extract max similarity and Jaccard distance for each name
threshold_similarity = 0.9
threshold_jaccard = 0.5  # Adjust as needed
considered_pairs = set()  # Keep track of considered pairs to avoid duplicates

for i in range(len(df)):
    # Exclude self-similarity and find the max similarity
    similarity_row = similarity_matrix[i].clone()
    similarity_row[i] = 0  # Exclude self-similarity
    max_similar_index = similarity_row.argmax().item()
    max_similarity_score = similarity_matrix[i][max_similar_index].item()

    # Calculate Jaccard distance
    jaccard_dist = calculate_jaccard_distance(df['Names'][i], df['Names'][max_similar_index])

    # Ensure the pair is considered only once and meets the thresholds
    current_pair = (df['Names'][i], df['Names'][max_similar_index])
    reversed_pair = (df['Names'][max_similar_index], df['Names'][i])

    if (
        current_pair not in considered_pairs
        and reversed_pair not in considered_pairs
        and max_similarity_score >= threshold_similarity
        and jaccard_dist >= threshold_jaccard
    ):
        considered_pairs.add(current_pair)
        max_similarity_data['Name1'].append(df['Names'][i])
        max_similarity_data['Name2'].append(df['Names'][max_similar_index])
        max_similarity_data['Max_Similarity_Score'].append(max_similarity_score)
        max_similarity_data['Jaccard_Distance'].append(jaccard_dist)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Drop duplicate rows
max_similarity_df = max_similarity_df.drop_duplicates()

# Create a new column 'Names2' in df and assign values from 'Name2' column
df['Names2'] = df['Names'].replace(dict(zip(max_similarity_df['Name1'], max_similarity_df['Name2'])))

# Add a new column 'Similarity_Score' in df
df['Similarity_Score'] = df.apply(lambda row: max_similarity_df[
    (max_similarity_df['Name1'] == row['Names']) & (max_similarity_df['Name2'] == row['Names2'])
]['Max_Similarity_Score'].values[0] if not max_similarity_df[
    (max_similarity_df['Name1'] == row['Names']) & (max_similarity_df['Name2'] == row['Names2'])
].empty else None, axis=1)

# Add a new column 'Jaccard_Distance' in df
df['Jaccard_Distance'] = df.apply(lambda row: max_similarity_df[
    (max_similarity_df['Name1'] == row['Names']) & (max_similarity_df['Name2'] == row['Names2'])
]['Jaccard_Distance'].values[0] if not max_similarity_df[
    (max_similarity_df['Name1'] == row['Names']) & (max_similarity_df['Name2'] == row['Names2'])
].empty else None, axis=1)

# Display the modified DataFrame
print(df)










import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, auc, f1_score

# Assuming max_similarity_df is available
# Calculate precision, recall, and F1 score for different thresholds
precision, recall, thresholds = precision_recall_curve(max_similarity_df['Max_Similarity_Score'], pos_label=None)
f1_scores = 2 * (precision * recall) / (precision + recall)

# Plot precision-recall curve
plt.plot(recall, precision, label='Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# Plot F1 scores at different thresholds
plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
plt.xlabel('Threshold')
plt.ylabel('F1 Score')
plt.title('F1 Score at Different Thresholds')
plt.legend()
plt.show()

# Find the threshold that maximizes F1 score
best_threshold = thresholds[np.argmax(f1_scores)]
print(f'Best Threshold (Max F1 Score): {best_threshold}')












import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the max similarity results
max_similarity_data = {'Name1': [], 'Name2': [], 'Max_Similarity_Score': []}

# Extract max similarity for each name
threshold = 0.9
considered_pairs = set()  # Keep track of considered pairs to avoid duplicates

for i in range(len(df)):
    # Exclude self-similarity and find the max similarity
    similarity_row = similarity_matrix[i].clone()
    similarity_row[i] = 0  # Exclude self-similarity
    max_similar_index = similarity_row.argmax().item()
    max_similarity_score = similarity_matrix[i][max_similar_index].item()

    # Ensure the pair is considered only once
    current_pair = (df['Names'][i], df['Names'][max_similar_index])
    reversed_pair = (df['Names'][max_similar_index], df['Names'][i])

    if current_pair not in considered_pairs and reversed_pair not in considered_pairs and max_similarity_score >= threshold:
        considered_pairs.add(current_pair)
        max_similarity_data['Name1'].append(df['Names'][i])
        max_similarity_data['Name2'].append(df['Names'][max_similar_index])
        max_similarity_data['Max_Similarity_Score'].append(max_similarity_score)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Drop duplicate rows
max_similarity_df = max_similarity_df.drop_duplicates()

# Create a new column 'Names2' in df and assign values from 'Name2' column
df['Names2'] = df['Names'].replace(dict(zip(max_similarity_df['Name1'], max_similarity_df['Name2'])))

# Remove rows where 'Names' appear in 'Names2'
df = df[~df['Names'].isin(df['Names2'])]

# Add a new column 'Similarity_Score' in df
df['Similarity_Score'] = df.apply(lambda row: max_similarity_df[
    (max_similarity_df['Name1'] == row['Names']) & (max_similarity_df['Name2'] == row['Names2'])
]['Max_Similarity_Score'].values[0] if not max_similarity_df[
    (max_similarity_df['Name1'] == row['Names']) & (max_similarity_df['Name2'] == row['Names2'])
].empty else None, axis=1)

# Display the modified DataFrame
print(df)








import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the max similarity results
max_similarity_data = {'Name1': [], 'Name2': [], 'Max_Similarity_Score': []}

# Extract max similarity for each name
threshold = 0.9
considered_pairs = set()  # Keep track of considered pairs to avoid duplicates

for i in range(len(df)):
    # Exclude self-similarity and find the max similarity
    similarity_row = similarity_matrix[i].clone()
    similarity_row[i] = 0  # Exclude self-similarity
    max_similar_index = similarity_row.argmax().item()
    max_similarity_score = similarity_matrix[i][max_similar_index].item()

    # Ensure the pair is considered only once
    current_pair = (df['Names'][i], df['Names'][max_similar_index])
    reversed_pair = (df['Names'][max_similar_index], df['Names'][i])

    if current_pair not in considered_pairs and reversed_pair not in considered_pairs and max_similarity_score >= threshold:
        considered_pairs.add(current_pair)
        max_similarity_data['Name1'].append(df['Names'][i])
        max_similarity_data['Name2'].append(df['Names'][max_similar_index])
        max_similarity_data['Max_Similarity_Score'].append(max_similarity_score)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Drop duplicate rows
max_similarity_df = max_similarity_df.drop_duplicates()

# Create a new column 'Names2' in df and assign values from 'Name2' column
df['Names2'] = df['Names'].replace(dict(zip(max_similarity_df['Name1'], max_similarity_df['Name2'])))

# Remove rows where 'Names' appear in 'Names2'
df = df[~df['Names'].isin(df['Names2'])]

# Add a new column 'Similarity_Score' in df
df['Similarity_Score'] = df.apply(lambda row: max_similarity_df[
    (max_similarity_df['Name1'] == row['Names']) & (max_similarity_df['Name2'] == row['Names2'])
]['Max_Similarity_Score'].values[0], axis=1)

# Display the modified DataFrame
print(df)








import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the max similarity results
max_similarity_data = {'Name1': [], 'Name2': [], 'Max_Similarity_Score': []}

# Extract max similarity for each name
threshold = 0.9
considered_pairs = set()  # Keep track of considered pairs to avoid duplicates

for i in range(len(df)):
    # Exclude self-similarity and find the max similarity
    similarity_row = similarity_matrix[i].clone()
    similarity_row[i] = 0  # Exclude self-similarity
    max_similar_index = similarity_row.argmax().item()
    max_similarity_score = similarity_matrix[i][max_similar_index].item()

    # Ensure the pair is considered only once
    current_pair = (df['Names'][i], df['Names'][max_similar_index])
    reversed_pair = (df['Names'][max_similar_index], df['Names'][i])

    if current_pair not in considered_pairs and reversed_pair not in considered_pairs and max_similarity_score >= threshold:
        considered_pairs.add(current_pair)
        max_similarity_data['Name1'].append(df['Names'][i])
        max_similarity_data['Name2'].append(df['Names'][max_similar_index])
        max_similarity_data['Max_Similarity_Score'].append(max_similarity_score)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Drop duplicate rows
max_similarity_df = max_similarity_df.drop_duplicates()

# Display the DataFrame without index numbers
print(max_similarity_df.to_string(index=False))











import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the max similarity results
max_similarity_data = {'Name1': [], 'Name2': [], 'Max_Similarity_Score': []}

# Extract max similarity for each name
threshold = 0.9
considered_pairs = set()  # Keep track of considered pairs to avoid duplicates

for i in range(len(df)):
    # Exclude self-similarity and find the max similarity
    similarity_row = similarity_matrix[i].clone()
    similarity_row[i] = 0  # Exclude self-similarity
    max_similar_index = similarity_row.argmax().item()
    max_similarity_score = similarity_matrix[i][max_similar_index].item()

    # Ensure the pair is considered only once
    current_pair = (df['Names'][i], df['Names'][max_similar_index])
    reversed_pair = (df['Names'][max_similar_index], df['Names'][i])

    if current_pair not in considered_pairs and reversed_pair not in considered_pairs:
        considered_pairs.add(current_pair)
        max_similarity_data['Name1'].append(df['Names'][i])
        max_similarity_data['Name2'].append(df['Names'][max_similar_index])
        max_similarity_data['Max_Similarity_Score'].append(max_similarity_score)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Display the DataFrame
print(max_similarity_df)









import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the max similarity results
max_similarity_data = {'Name1': [], 'Name2': [], 'Max_Similarity_Score': []}

# Extract max similarity for each name
threshold = 0.9

for i in range(len(df)):
    max_similar_index = similarity_matrix[i].argmax().item()
    max_similarity_score = similarity_matrix[i][max_similar_index].item()

    if max_similarity_score > threshold:
        max_similarity_data['Name1'].append(df['Names'][i])
        max_similarity_data['Name2'].append(df['Names'][max_similar_index])
        max_similarity_data['Max_Similarity_Score'].append(max_similarity_score)

# Create DataFrame from the collected data
max_similarity_df = pd.DataFrame(max_similarity_data)

# Display the DataFrame
print(max_similarity_df)







import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create lists to store similar names and scores
similar_names_list = []
similarity_scores_list = []

# Extract most similar names and their scores
threshold = 0.8  # Set your similarity threshold

for i in range(len(df)):
    for j in range(i + 1, len(df)):
        similarity_score = similarity_matrix[i][j].item()
        if similarity_score > threshold:
            similar_names_list.append((df['Names'][i], df['Names'][j]))
            similarity_scores_list.append(similarity_score)

# Add similar names and scores to the DataFrame
df['Similar_Names'] = similar_names_list
df['Similarity_Scores'] = similarity_scores_list

# Display the DataFrame
print(df)




import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the most similar names
most_similar_data = {'Name1': [], 'Name2': [], 'Similarity_Score': []}

# Extract most similar names for each name
threshold = 0.8  # Set your similarity threshold

for i in range(len(df)):
    most_similar_index = similarity_matrix[i].argsort()[-2].item()  # Index of the second highest similarity (excluding self)
    most_similar_score = similarity_matrix[i][most_similar_index].item()

    most_similar_data['Name1'].append(df['Names'].tolist())  # Append all names to 'Name1'
    most_similar_data['Name2'].append(df['Names'][most_similar_index])
    most_similar_data['Similarity_Score'].append(most_similar_score)

# Create DataFrame from the collected data
most_similar_df = pd.DataFrame(most_similar_data)

# Display the DataFrame
print(most_similar_df)




import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Create DataFrame to store the most similar names
most_similar_data = {'Name1': [], 'Name2': [], 'Similarity_Score': []}

# Extract most similar names for each name
threshold = 0.8  # Set your similarity threshold

for i in range(len(df)):
    most_similar_index = similarity_matrix[i].argsort()[-2]  # Index of the second highest similarity (excluding self)
    most_similar_score = similarity_matrix[i][most_similar_index].item()

    if most_similar_score > threshold:
        most_similar_data['Name1'].append(df['Names'][i])
        most_similar_data['Name2'].append(df['Names'][most_similar_index])
        most_similar_data['Similarity_Score'].append(most_similar_score)

# Create DataFrame from the collected data
most_similar_df = pd.DataFrame(most_similar_data)

# Display the DataFrame
print(most_similar_df)







import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Extract most similar pairs and their similarity scores
threshold = 0.8  # Set your similarity threshold
similar_pairs_data = {'Name1': [], 'Name2': [], 'Similarity_Score': []}

for i in range(len(df)):
    for j in range(i + 1, len(df)):
        similarity_score = similarity_matrix[i][j].item()
        if similarity_score > threshold:
            similar_pairs_data['Name1'].append(df['Names'][i])
            similar_pairs_data['Name2'].append(df['Names'][j])
            similar_pairs_data['Similarity_Score'].append(similarity_score)

# Create DataFrame from the collected data
similar_pairs_df = pd.DataFrame(similar_pairs_data)

# Display the DataFrame
print(similar_pairs_df)









import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria', 'saliyah', 'jon']}
df = pd.DataFrame(data)

# Function to calculate pairwise similarity
def calculate_similarity(df, column_name, model):
    embeddings = model.encode(df[column_name].tolist(), convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings)
    return similarity_matrix

# Calculate similarity matrix
similarity_matrix = calculate_similarity(df, 'Names', model)

# Extract most similar pairs and their similarity scores
threshold = 0.8  # Set your similarity threshold
most_similar_pairs = []

for i in range(len(df)):
    for j in range(i + 1, len(df)):
        similarity_score = similarity_matrix[i][j].item()
        if similarity_score > threshold:
            most_similar_pairs.append((df['Names'][i], df['Names'][j], similarity_score))

# Display most similar pairs with similarity scores
print("Most Similar Pairs and Their Similarity Scores:")
for pair in most_similar_pairs:
    print(f"{pair[0]} - {pair[1]} : Similarity Score = {pair[2]:.4f}")







import pandas as pd
from sentence_transformers import SentenceTransformer, util
from fuzzywuzzy import fuzz

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Function to correct names in a DataFrame column
def correct_names_bert(df, column_name, model, similarity_threshold=0.8, fuzzy_threshold=90):
    corrected_names = []
    similarity_scores = []

    for name in df[column_name]:
        # Get BERT embeddings for names
        name_embedding = model.encode(name, convert_to_tensor=True)

        # Find similar names
        similar_names = [
            other_name
            for other_name in df[column_name]
            if util.pytorch_cos_sim(name_embedding, model.encode(other_name, convert_to_tensor=True)) > similarity_threshold
            or fuzz.ratio(name, other_name) > fuzzy_threshold
        ]

        # If there are similar names, choose the most similar one
        if similar_names:
            most_similar_name = max(similar_names, key=lambda x: util.pytorch_cos_sim(name_embedding, model.encode(x, convert_to_tensor=True)))
            corrected_names.append(most_similar_name)
            similarity_scores.append(util.pytorch_cos_sim(name_embedding, model.encode(most_similar_name, convert_to_tensor=True)).item())
        else:
            corrected_names.append(name)
            similarity_scores.append(1.0)  # Default similarity for the original name

    return corrected_names, similarity_scores

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria']}
df = pd.DataFrame(data)

# Correct names in the 'Names' column using BERT embeddings and fuzzy matching
corrected_names, similarity_scores = correct_names_bert(df, 'Names', model)

# Add corrected names and similarity scores to the DataFrame
df['Corrected_Names'] = corrected_names
df['Similarity_Scores'] = similarity_scores

# Display the DataFrame
print(df)







import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Function to correct names in a DataFrame column
def correct_names_bert(df, column_name, model, similarity_threshold=0.8):
    corrected_names = []

    for name in df[column_name]:
        # Get BERT embeddings for names
        name_embedding = model.encode(name, convert_to_tensor=True)

        # Find similar names
        similar_names = [
            other_name
            for other_name in df[column_name]
            if util.pytorch_cos_sim(name_embedding, model.encode(other_name, convert_to_tensor=True)) > similarity_threshold
        ]

        # If there are similar names, choose the most similar one
        if similar_names:
            most_similar_name = max(similar_names, key=lambda x: util.pytorch_cos_sim(name_embedding, model.encode(x, convert_to_tensor=True)))
            corrected_names.append(most_similar_name)
        else:
            corrected_names.append(name)

    return corrected_names

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria']}
df = pd.DataFrame(data)

# Correct names in the 'Names' column using BERT embeddings
df['Corrected_Names'] = correct_names_bert(df, 'Names', model)

# Display the DataFrame
print(df)







import zipfile

zip_file_path = 'your_zip_file.zip'
extract_folder = 'destination_folder'

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)



Great! If you have a DataFrame with a column containing both correct and potentially misspelled names, you can apply the similarity detection and correction process to the entire column. Here's an example using pandas, assuming you already have a pre-trained Word2Vec model: python




import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Load pre-trained Word2Vec model (you need to have the model file)
word2vec_model = Word2Vec.load("path/to/word2vec/model")

# Function to vectorize a name
def vectorize_name(name, model):
    tokens = word_tokenize(name.lower())  # Assuming names are in lowercase
    vectors = [model.wv[token] for token in tokens if token in model.wv]
    return np.mean(vectors, axis=0) if vectors else None

# Function to correct names in a DataFrame column
def correct_names(df, column_name, model, similarity_threshold=0.8):
    corrected_names = []

    for name in df[column_name]:
        vec_name = vectorize_name(name, model)

        # Find similar names
        similar_names = [
            other_name
            for other_name in df[column_name]
            if cosine_similarity([vec_name], [vectorize_name(other_name, model)])[0][0] > similarity_threshold
        ]

        # If there are similar names, choose the most similar one
        if similar_names:
            most_similar_name = max(similar_names, key=lambda x: cosine_similarity([vec_name], [vectorize_name(x, model)])[0][0])
            corrected_names.append(most_similar_name)
        else:
            corrected_names.append(name)

    return corrected_names

# Example DataFrame
data = {'Names': ['saliha', 'john', 'salih', 'maria']}
df = pd.DataFrame(data)

# Correct names in the 'Names' column
df['Corrected_Names'] = correct_names(df, 'Names', word2vec_model)

# Display the DataFrame
print(df)








import pandas as pd
import numpy as np

# Example DataFrame
data = {'client_id': [1, 2, 1, 3, 2, 3],
        'transaction_date': ['2023-06-01', '2023-06-01', '2023-06-02', '2023-06-03', '2023-06-01', '2023-06-03']}

transactions = pd.DataFrame(data)

# Convert transaction_date column to datetime format
transactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'])

# Group by client_id and transaction_date, then count occurrences
co_occurrence = transactions.groupby(['client_id', 'transaction_date']).size().reset_index(name='count')

# Create a pivot table to get the co-occurrence matrix
co_occurrence_matrix = co_occurrence.pivot_table(index='client_id', columns='transaction_date', values='count', fill_value=0)

print(co_occurrence_matrix.values)








import pandas as pd
import numpy as np

# Example DataFrame
data = {'clientbid': [1, 2, 1, 3, 2, 3],
        'date': ['06/05/2012', '06/05/2012', '07/05/2012', '08/05/2012', '06/05/2012', '06/05/2012']}

transactions = pd.DataFrame(data)

# Convert date column to datetime format
transactions['date'] = pd.to_datetime(transactions['date'], format='%d/%m/%Y')

# Create a co-occurrence matrix
co_occurrence_matrix = pd.crosstab(transactions['clientbid'], transactions['date']).values

print(co_occurrence_matrix)







import numpy as np

def calculate_mb_affinity_matrix(client_ids, transaction_dates):
  """Calculates the MB Affinity matrix for a given dataset.

  Args:
    client_ids: A list of client IDs.
    transaction_dates: A list of transaction dates.

  Returns:
    A NumPy array representing the MB Affinity matrix.
  """

  # Group the transactions by client ID.
  grouped_transactions = {}
  for client_id, transaction_date in zip(client_ids, transaction_dates):
    if client_id not in grouped_transactions:
      grouped_transactions[client_id] = []
    grouped_transactions[client_id].append(transaction_date)

  # Calculate the co-occurrence matrix for each client ID.
  mb_affinity_matrix = np.zeros((len(client_ids), len(client_ids)))
  for client_id in grouped_transactions.keys():
    for i in range(len(grouped_transactions[client_id])):
      for j in range(len(grouped_transactions[client_id])):
        if i != j:
          # Check if the two clients meet in the same date.
          if grouped_transactions[client_id][i] == grouped_transactions[client_id][j]:
            # Update the MB Affinity matrix.
            mb_affinity_matrix[client_id][i] += 1

  # Round the matrix elements to the nearest integer.
  mb_affinity_matrix = np.round(mb_affinity_matrix).astype(int)

  # Set the diagonal elements of the MB Affinity matrix to 1.
  for i in range(len(mb_affinity_matrix)):
    mb_affinity_matrix[i][i] = 1

  # Return the MB Affinity matrix.
  return mb_affinity_matrix

# Example usage:

client_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]
transaction_dates = ["2023-08-04", "2023-08-05", "2023-08-06", "2023-08-07", "2023-08-08", "2023-08-09", "2023-08-10", "2023-08-11", "2023-08-12"]

# Calculate the MB Affinity matrix.
mb_affinity_matrix = calculate_mb_affinity_matrix(client_ids, transaction_dates)

# Print the MB Affinity matrix.
print(mb_affinity_matrix)






import numpy as np

def calculate_occurrence_matrix(client_ids, transaction_dates):
  """Calculates the occurrence matrix of client IDs and transaction dates.

  Args:
    client_ids: A list of client IDs.
    transaction_dates: A list of transaction dates.

  Returns:
    A NumPy array representing the occurrence matrix.
  """

  # Create a dictionary to track the occurrences.
  occurrences = {}

  # Iterate over the client IDs and transaction dates.
  for client_id, transaction_date in zip(client_ids, transaction_dates):
    # If the client ID and transaction date are not in the dictionary,
    # initialize them to 0.
    if (client_id, transaction_date) not in occurrences:
      occurrences[client_id, transaction_date] = 0

    # Increment the occurrence count for the client ID and transaction date.
    occurrences[client_id, transaction_date] += 1

  # Convert the dictionary to a NumPy array.
  occurrence_matrix = np.array([[occurrences.get((client_id, transaction_date), 0) for transaction_date in set(transaction_dates)] for client_id in set(client_ids)])

  # Return the occurrence matrix.
  return occurrence_matrix

# Example usage:

# Create a list of client IDs and transaction dates.
client_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]
transaction_dates = ["2023-08-04", "2023-08-05", "2023-08-06", "2023-08-07", "2023-08-08", "2023-08-09", "2023-08-10", "2023-08-11", "2023-08-12"]

# Calculate the occurrence matrix.
occurrence_matrix = calculate_occurrence_matrix(client_ids, transaction_dates)

# Print the occurrence matrix.
print(occurrence_matrix)








def calculate_co_occurrence_matrix(data, window_size=2):
    """Calculates the co-occurrence matrix for a given dataset.

    Args:
        data: A list of lists, where each inner list represents a transaction.
        window_size: The size of the context window.

    Returns:
        A NumPy array representing the co-occurrence matrix.
    """

    # Create a counter to track the co-occurrences.
    co_occurrences = Counter()

    # Iterate over the transactions.
    for transaction in data:
        # Create a context window for each word in the transaction.
        for i in range(len(transaction)):
            window = transaction[i:i + window_size]

            # Iterate over the words in the context window.
            for j in range(len(window)):
                # If the current word is not the same as the center word,
                # increment the co-occurrence count.
                if window[j] != transaction[i]:
                    co_occurrences[(window[j], transaction[i])] += 1

    # Convert the counter to a NumPy array.
    co_occurrence_matrix = np.array([list(co_occurrences.values())])

    # Return the co-occurrence matrix.
    return co_occurrence_matrix





Pour détecter l'emplacement des distributeurs automatiques de billets (ATM), vous pouvez utiliser un classificateur d'arbre de décision, ce qui peut vous aider dans votre détection géographique. Voici comment vous pouvez aborder cette tâche en utilisant un classificateur d'arbre de décision




Retraits multiples en un temps court : Si un titulaire de carte effectue de multiples retraits en espèces en un laps de temps très court, cela peut être suspect. Par exemple, plusieurs retraits en quelques minutes ou heures. Retraits fréquents : Des retraits fréquents et réguliers, en particulier à des ATM situés dans des zones inhabituelles par rapport aux habitudes du titulaire de la carte, peuvent être suspects.


import pandas as pd

# Charger vos données de transactions dans un DataFrame Pandas
data = pd.read_csv('votre_fichier_de_transactions.csv')

# Trier les données par client et par date
data.sort_values(by=['ID_client', 'date_transaction'], inplace=True)

# Définir un seuil pour le temps entre les retraits multiples (en minutes)
seuil_temps_entre_retraits = 60  # Par exemple, 60 minutes (1 heure)

# Définir un seuil pour le nombre maximum de retraits en un temps court
seuil_retraits_en_un_temps_court = 3  # Par exemple, plus de 3 retraits en 1 heure

# Créer une colonne avec le temps écoulé depuis la transaction précédente pour chaque client
data['temps_ecoule'] = data.groupby('ID_client')['date_transaction'].diff().dt.total_seconds() / 60

# Identifier les retraits multiples en un temps court
data['retraits_multiples'] = data['temps_ecoule'] < seuil_temps_entre_retraits

# Identifier les retraits fréquents
data['retraits_frequents'] = data.groupby('ID_client')['retraits_multiples'].rolling(window=seuil_retraits_en_un_temps_court).sum().reset_index(level=0, drop=True)

# Filtrer les transactions suspectes
transactions_suspectes = data[(data['retraits_multiples']) | (data['retraits_frequents'])]

# Afficher les transactions suspectes
print(transactions_suspectes)











import pandas as pd import folium # Exemple de données de transactions avec des informations de localisation (latitude et longitude) data = pd.read_csv('votre_fichier.csv') # Création d'une carte interactive avec Folium m = folium.Map(location=[latitude_moyenne, longitude_moyenne], zoom_start=6)  # Définissez la position initiale de la carte et le niveau de zoom # Marquage des emplacements des ATM sur la carte for index, row in data.iterrows(): folium.Marker([row['latitude'], row['longitude']]).add_to(m) # Affichage de la carte m.save('carte_atm.html')



Bien sûr, voici une explication détaillée de chacune de ces étapes : Analyse temporelle : L'analyse temporelle consiste à examiner les tendances liées au temps dans vos données de transactions. Vous pouvez effectuer cette analyse en suivant ces étapes : Regroupement par heure, jour de la semaine ou mois : Tout d'abord, vous allez regrouper vos données de transactions en fonction de l'unité de temps qui vous intéresse. Cela peut être l'heure du jour, le jour de la semaine ou le mois de l'année. Statistiques descriptives : Une fois que vous avez regroupé les données, vous pouvez calculer des statistiques descriptives pour chaque groupe. Par exemple, pour l'heure du jour, vous pourriez calculer la moyenne, la médiane, l'écart-type, etc., des montants de transactions effectuées à cette heure. Création de graphiques : Pour visualiser les tendances temporelles, vous pouvez créer des graphiques tels que des courbes temporelles, des histogrammes ou des boîtes à moustaches. Cela vous permettra de voir s'il y a des schémas récurrents au fil du temps, comme des heures de la journée où les retraits importants sont plus fréquents. L'analyse temporelle peut révéler des informations importantes sur les moments où les retraits d'espèces importants se produisent le plus fréquemment, ce qui peut aider à cibler les mesures de prévention. Analyse géographique : L'analyse géographique consiste à examiner la répartition géographique des transactions, en particulier si vos données incluent des informations sur la localisation des distributeurs automatiques (ATM). Voici comment cela peut être réalisé : Utilisation de données de localisation : Si vos données contiennent des informations sur la latitude et la longitude des ATM, vous pouvez utiliser ces données pour cartographier l'emplacement de chaque ATM. Utilisation de bibliothèques de cartographie : Des bibliothèques telles que Folium en Python vous permettent de créer des cartes interactives. Vous pouvez marquer chaque emplacement d'ATM sur la carte en utilisant ces bibliothèques. Analyse de la distribution spatiale : Une fois que vous avez généré la carte, vous pouvez analyser la distribution spatiale des ATM. Par exemple, recherchez des clusters d'ATM où les retraits d'espèces importants sont plus fréquents. Examen des tendances géographiques : Vous pouvez également suivre les tendances au fil du temps en superposant des données temporelles sur la carte, ce qui peut révéler des variations saisonnières ou des schémas géographiques spécifiques. L'analyse géographique peut aider à identifier les zones géographiques où des mesures de sécurité supplémentaires peuvent être nécessaires. Évaluation des liens entre les clients : L'évaluation des liens entre les clients consiste à utiliser des techniques d'analyse de réseau ou d'analyse de graphes pour détecter des connexions entre les clients qui effectuent des retraits similaires. Voici comment cela fonctionne : Création d'un graphe : Vous pouvez représenter vos clients comme des nœuds (ou sommets) dans un graphe, et les liens entre les clients comme des arêtes. Les arêtes peuvent représenter des transactions, des connexions sociales ou d'autres types de relations. Analyse de réseau : Utilisez des techniques d'analyse de réseau pour identifier des motifs, des clusters ou des sous-graphes intéressants. Vous pouvez utiliser des mesures telles que la centralité, la cohésion ou la détection de communautés pour identifier des groupes de clients liés. Interprétation des résultats : Une fois que vous avez analysé le graphe, interprétez les résultats pour déterminer s'il existe des groupes de clients liés par des transactions ou des connexions. Cela peut aider à identifier des réseaux de fraude potentiels. L'évaluation des liens entre les clients peut révéler des schémas de coopération ou de coordination entre les fraudeurs, ce qui est essentiel pour la prévention et la détection des fraudes.














C'est un excellent point de départ pour votre étude sur les retraits d'espèces importants et coordonnés aux distributeurs automatiques. Voici quelques étapes que vous pourriez envisager pour analyser ces données :

Nettoyage des données : Assurez-vous que vos données sont propres et cohérentes. Cela peut impliquer la gestion des doublons, la correction des erreurs de saisie, la suppression des données manquantes, etc.

Agrégation des données : Regroupez les données de transactions par client, en incluant les informations sur les comptes en euros et en dzd.

Définition des retraits d'espèces importants : Établissez des critères pour ce que vous considérez comme des retraits d'espèces importants en euros et en dzd.

Identification des transactions suspectes : Utilisez ces critères pour identifier les transactions qui répondent à la définition des retraits d'espèces importants et coordonnés.

Analyse des schémas : Examinez les données pour déterminer s'il y a des schémas récurrents, tels que des retraits importants en euros suivis de retraits importants en dzd, ou des retraits coordonnés entre plusieurs clients.

Analyse temporelle : Étudiez les tendances temporelles pour voir si ces retraits se produisent à des moments particuliers ou à des intervalles réguliers.

Analyse géographique : Si les données comprennent des informations sur l'emplacement des ATM, examinez s'il y a des concentrations géographiques de ces retraits.

Évaluation des liens entre les clients : Recherchez des liens ou des connexions entre les clients qui effectuent ces retraits, s'ils existent.

Étude des montants et des comptes : Analysez les montants des retraits, les comptes impliqués (euros et dzd) et les éventuels transferts de fonds.

Prévention et rapports : En fonction de vos conclusions, envisagez des mesures de prévention, telles que des alertes de sécurité ou une surveillance accrue des comptes concernés. Générez également des rapports pour documenter vos découvertes.

N'oubliez pas de respecter les réglementations en matière de protection de la vie privée et de sécurité des données tout au long de votre analyse. En outre, vous pourriez envisager de collaborer avec des experts en sécurité financière ou en analyse de données pour optimiser votre étude.







La détection de communautés dans les fraudes et les fuites de capitaux implique la recherche de schémas de comportement inhabituels ou coordonnés entre plusieurs titulaires de cartes Visa. Les actions suspectes de porteurs de cartes Visa dans ce contexte peuvent inclure :

Transactions coordonnées : Plusieurs titulaires de carte effectuent des transactions similaires ou coordonnées en peu de temps.

Transactions croisées : Plusieurs titulaires de carte effectuent des transactions entre eux de manière répétée.

Utilisation de cartes multiples : Des titulaires de carte utilisent fréquemment différentes cartes Visa pour effectuer des transactions frauduleuses.

Partage d'informations de carte : Les titulaires de carte partagent des informations de carte Visa, telles que les numéros de carte, pour effectuer des transactions frauduleuses.

Transactions internationales soudaines : Des achats ou des retraits d'argent sont effectués à l'étranger de manière inattendue.

Utilisation de comptes multiples : Des titulaires de carte créent et utilisent plusieurs comptes Visa sous des noms différents pour dissimuler leurs activités.

Utilisation d'adresses de livraison suspectes : Les fraudes peuvent impliquer des adresses de livraison inhabituelles ou fictives.

Transactions de montants inhabituellement élevés : Des paiements ou des achats de grande valeur qui dépassent la norme du titulaire de la carte.

Activité en dehors des heures habituelles : Des transactions effectuées en dehors des heures d'utilisation habituelles de la carte.

Retraits d'espèces importants et coordonnés : Des groupes de titulaires de carte effectuent des retraits d'espèces importants et coordonnés à partir de distributeurs automatiques.

La détection de communautés dans les fraudes et les fuites de capitaux repose sur l'analyse de ces schémas pour identifier des groupes de fraudeurs qui travaillent ensemble. Les émetteurs de cartes Visa utilisent des outils avancés pour repérer de telles activités suspectes et prévenir les fraudes. Si vous êtes un émetteur de cartes Visa et que vous soupçonnez de telles activités, il est essentiel de contacter les autorités compétentes pour enquêter sur la situation.





Send a message














import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

# Assuming your results_communautes is a DataFrame with columns "Client ID" and "Frozenset ID"

# Create an empty graph for each unique "Frozenset ID"
frozenset_graphs = {}
unique_frozenset_ids = results_communautes["Frozenset ID"].unique()
color_map = dict(zip(unique_frozenset_ids, mcolors.TABLEAU_COLORS))  # Assign unique colors

for frozenset_id in unique_frozenset_ids:
    G = nx.Graph()
    frozenset_graphs[frozenset_id] = G

# Add nodes and edges to the respective frozenset graph
for index, row in results_communautes.iterrows():
    client_id = row["Client ID"]
    frozenset_id = row["Frozenset ID"]
    
    G = frozenset_graphs[frozenset_id]
    G.add_node(client_id)
    
    # Add edges based on your criteria (if needed)

# Visualize each frozenset graph with unique colors
for frozenset_id, G in frozenset_graphs.items():
    pos = nx.spring_layout(G)  # You can choose a different layout as needed
    
    # Get the unique color for this frozenset_id
    node_colors = [color_map[frozenset_id] for _ in G.nodes()]
    
    plt.figure()
    nx.draw(G, pos, with_labels=True, node_color=node_colors)
    plt.title(f"Graph for Frozenset ID {frozenset_id}")
    plt.show()









import networkx as nx
import matplotlib.pyplot as plt

# Assuming your results_communautes is a DataFrame with columns "Client ID" and "Frozenset ID"

# Create an empty graph for each unique "Frozenset ID"
frozenset_graphs = {}

for frozenset_id in results_communautes["Frozenset ID"].unique():
    G = nx.Graph()
    frozenset_graphs[frozenset_id] = G

# Add nodes and edges to the respective frozenset graph
for index, row in results_communautes.iterrows():
    client_id = row["Client ID"]
    frozenset_id = row["Frozenset ID"]
    
    G = frozenset_graphs[frozenset_id]
    G.add_node(client_id)
    
    # Add edges based on your criteria (if needed)

# Visualize each frozenset graph
for frozenset_id, G in frozenset_graphs.items():
    pos = nx.spring_layout(G)  # You can choose a different layout as needed
    plt.figure()
    nx.draw(G, pos, with_labels=True)
    plt.title(f"Graph for Frozenset ID {frozenset_id}")
    plt.show()








import networkx as nx
import matplotlib.pyplot as plt

# Create an empty graph for each agency
agency_graphs = {}

for agency in results_communautes["Agency"].unique():
    G = nx.Graph()
    agency_graphs[agency] = G

# Add nodes and edges to the respective agency graph
for index, row in results_communautes.iterrows():
    client_id = row["Client ID"]
    agency = row["Agency"]
    frozenset_id = row["Frozenset ID"]
    
    G = agency_graphs[agency]
    G.add_node(client_id)
    
    # Add edges based on Frozenset ID
    # Assuming you want to connect nodes within the same Frozenset ID
    for other_index, other_row in results_communautes.iterrows():
        if other_row["Frozenset ID"] == frozenset_id and other_row["Client ID"] != client_id:
            G.add_edge(client_id, other_row["Client ID"])

# Visualize each agency graph
for agency, G in agency_graphs.items():
    pos = nx.spring_layout(G)  # You can choose a different layout as needed
    plt.figure()
    nx.draw(G, pos, with_labels=True)
    plt.title(f"Graph for Agency {agency}")
    plt.show()










import networkx as nx
import matplotlib.pyplot as plt

# Assuming your results_communautes is a DataFrame with columns "Client ID," "Agency," and "Frozenset ID"

# Create an empty graph for each agency
agency_graphs = {}

for agency in results_communautes["Agency"].unique():
    G = nx.Graph()
    agency_graphs[agency] = G

# Add nodes and edges to the respective agency graph
for index, row in results_communautes.iterrows():
    client_id = row["Client ID"]
    agency = row["Agency"]
    frozenset_id = row["Frozenset ID"]
    
    G = agency_graphs[agency]
    G.add_node(client_id)
    
    # Add edges based on your criteria (e.g., frozenset ID)

# Visualize each agency graph
for agency, G in agency_graphs.items():
    pos = nx.spring_layout(G)  # You can choose a different layout as needed
    plt.figure()
    nx.draw(G, pos, with_labels=True)
    plt.title(f"Graph for Agency {agency}")
    plt.show()








import networkx as nx
import matplotlib.pyplot as plt

# Create an empty graph
G = nx.Graph()

# Create a dictionary to store the nodes' agency information
agency_dict = {}

# Add nodes and edges to the graph based on your DataFrame
for index, row in results_communautes.iterrows():
    client_id = row["Client ID"]
    agency = row["Agency"]
    amount = row["Transaction Amount"]
    frozenset_id = row["Frozenset ID"]
    
    G.add_node(client_id)
    agency_dict[client_id] = agency

# Add edges between nodes based on your criteria (e.g., transaction amount, frozenset ID)

# Group nodes by agency
agency_communities = {}
for node, agency in agency_dict.items():
    if agency in agency_communities:
        agency_communities[agency].append(node)
    else:
        agency_communities[agency] = [node]

# Visualize the graph with different colors for each agency's community
pos = nx.spring_layout(G)  # You can choose a different layout as needed

for agency, nodes in agency_communities.items():
    nx.draw_networkx_nodes(G, pos, nodelist=nodes, label=agency)
    nx.draw_networkx_labels(G, pos, labels={node: node for node in nodes})

# Add edges, customize colors, and labels as needed

plt.axis('off')
plt.show()








import networkx as nx
import matplotlib.pyplot as plt

# Create an empty graph
G = nx.Graph()

# Create a dictionary to store the nodes' agency information
agency_dict = {}

# Add nodes and edges to the graph based on your results_communities
for client_id, agency, amount, frozenset_id in results_communities:
    G.add_node(client_id)
    agency_dict[client_id] = agency

# Add edges between nodes based on your criteria (e.g., transaction amount, frozenset ID)

# Group nodes by agency
agency_communities = {}
for node, agency in agency_dict.items():
    if agency in agency_communities:
        agency_communities[agency].append(node)
    else:
        agency_communities[agency] = [node]

# Visualize the graph with different colors for each agency's community
pos = nx.spring_layout(G)  # You can choose a different layout as needed

for agency, nodes in agency_communities.items():
    nx.draw_networkx_nodes(G, pos, nodelist=nodes, label=agency)
    nx.draw_networkx_labels(G, pos, labels={node: node for node in nodes})

# Add edges, customize colors, and labels as needed

plt.axis('off')
plt.show()






import pandas as pd

# Sample DataFrame with 'ID', 'amount', and 'label' columns
data = {'ID': [1, 2, 3, 4, 5],
        'amount': [100, 200, 150, 50, 300],
        'label': ['Verment', 'Purchase', 'Verment', 'Transfer', 'Verment']}
df = pd.DataFrame(data)

# Define the label you want to extract
label_to_extract = 'Verment'

# Create the new column 'amount_Verment' based on the label
df['amount_Verment'] = df.apply(lambda row: row['amount'] if row['label'] == label_to_extract else 0, axis=1)

# Print the resulting DataFrame
print(df)









import pandas as pd

# Sample DataFrame with 'ID', 'amount', and 'label' columns
data = {'ID': [1, 2, 3, 4, 5],
        'amount': [100, 200, 150, 50, 300],
        'label': ['Verment', 'Purchase', 'Verment', 'Transfer', 'Verment']}
df = pd.DataFrame(data)

# Function to extract the amount based on the label
def extract_amount(label, label_to_extract):
    if label == label_to_extract:
        return df['amount']
    else:
        return 0

# Define the label you want to extract
label_to_extract = 'Verment'

# Create the new column 'amount_Verment' using the apply function
df['amount_Verment'] = df['label'].apply(extract_amount, args=(label_to_extract,))

# Print the resulting DataFrame
print(df)






# Sample DataFrames with 'name' and 'complimentary' columns
client_names_data = {'name': ['John Doe', 'Alice Smith', 'Bob Johnson']}
complimentary_data = {'complimentary': ['John Doe', 'Eve Johnson', 'Alice Smith', 'David Lee']}
client_names_df = pd.DataFrame(client_names_data)
complimentary_df = pd.DataFrame(complimentary_data)

# Check if names in 'complimentary' column match client names
complimentary_df['is_client'] = complimentary_df['complimentary'].isin(client_names_df['name'])

# Print the resulting DataFrame with 'is_client' column
print(complimentary_df)







# Sample list of client names (you would replace this with your actual client data)
client_names = ['John Doe', 'Alice Smith', 'Bob Johnson']

# Example DataFrame with a 'complimentary' column
data = {'complimentary': ['John Doe', 'Eve Johnson', 'Alice Smith', 'David Lee']}
df = pd.DataFrame(data)

# Check if names in 'complimentary' column match client names
df['is_client'] = df['complimentary'].isin(client_names)

# Print the resulting DataFrame
print(df)






import pandas as pd import numpy as np # Load your client data into a Pandas DataFrame # Replace 'your_client_data.csv' with the actual file path or URL of your client data df = pd.read_csv('your_client_data.csv') # 1. Transaction Frequency # Calculate the average number of transactions per day for each client. df['transaction_date'] = pd.to_datetime(df['transaction_date']) df['transaction_frequency'] = df.groupby('client_id')['transaction_date'].diff().dt.days # 2. Preferred Transaction Type # Identify the most common transaction type for each client. df['preferred_transaction_type'] = df.groupby('client_id')['transaction_type'].transform(lambda x: x.mode().iloc[0]) # 3. Average Transaction Amount # Calculate the average transaction amount for each client. df['average_transaction_amount'] = df.groupby('client_id')['amount'].transform('mean') # 4. Recency of Activity # Calculate the number of days since the last transaction for each client. df['recency_of_activity'] = (df['transaction_date'].max() - df.groupby('client_id')['transaction_date'].transform('max')).dt.days # 5. Client Tenure # Calculate the number of days since the first transaction for each client. df['client_tenure'] = (df['transaction_date'].max() - df.groupby('client_id')['transaction_date'].transform('min')).dt.days # 6. Spending Habits # Analyze spending patterns by calculating the proportion of transactions that are withdrawals. df['withdrawal_proportion'] = df.groupby('client_id')['transaction_type'].transform(lambda x: (x == 'withdrawal').mean()) # 7. Transaction Day and Time # Extract the day of the week and hour of the day for each transaction. df['transaction_day'] = df['transaction_date'].dt.day_name() df['transaction_hour'] = df['transaction_date'].dt.hour # 8. Cumulative Transaction Count # Calculate the cumulative number of transactions for each client. df['cumulative_transaction_count'] = df.groupby('client_id').cumcount() + 1 # 9. Client Segmentation # Use the behavior features to segment clients into groups, e.g., active, passive, high spenders, low spenders. # You can use clustering or predefined criteria for segmentation. # Save the updated DataFrame with behavior analysis features to a new CSV file df.to_csv('updated_client_data.csv', index=False) # These features can help you gain insights into your clients' behaviors and tailor your analysis accordingly.





import pandas as pd

# Load the updated client data with behavior analysis features
df = pd.read_csv('updated_client_data.csv')

# 1. Transaction Frequency Analysis
# Analyze the transaction frequency distribution for clients.
transaction_frequency_stats = df['transaction_frequency'].describe()
# You can create histograms or box plots to visualize the distribution.

# 2. Preferred Transaction Type Analysis
# Determine the most common transaction types among clients.
preferred_transaction_type_counts = df['preferred_transaction_type'].value_counts()

# 3. Average Transaction Amount Analysis
# Analyze the distribution of average transaction amounts for clients.
average_transaction_amount_stats = df['average_transaction_amount'].describe()
# Create histograms or box plots for visualization.

# 4. Recency of Activity Analysis
# Understand how recently clients have been active.
recency_stats = df['recency_of_activity'].describe()

# 5. Client Tenure Analysis
# Analyze how long clients have been with your business.
tenure_stats = df['client_tenure'].describe()

# 6. Spending Habits Analysis
# Analyze spending patterns by examining withdrawal proportions.
withdrawal_proportions_stats = df['withdrawal_proportion'].describe()

# 7. Transaction Day and Time Analysis
# Identify common transaction days and hours.
transaction_day_counts = df['transaction_day'].value_counts()
transaction_hour_counts = df['transaction_hour'].value_counts()

# 8. Cumulative Transaction Count Analysis
# Analyze the cumulative transaction count over time for each client.
# This can help identify trends in client activity.
# You can create time series plots for visualization.

# 9. Client Segmentation Analysis
# Apply clustering algorithms or predefined criteria to segment clients.
# Analyze the characteristics and behaviors of each segment separately.

# You can use various data visualization libraries like Matplotlib or Seaborn to create plots and charts to visualize the results of your analysis.

# Example: Creating a bar chart for transaction day analysis
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.bar(transaction_day_counts.index, transaction_day_counts.values)
plt.title('Transaction Day Analysis')
plt.xlabel('Day of the Week')
plt.ylabel('Transaction Count')
plt.show()

# You can repeat similar visualization and analysis for other behavior features.





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load your dataset into a Pandas DataFrame
# Replace 'your_dataset.csv' with the actual file path or URL of your dataset
df = pd.read_csv('your_dataset.csv')

# Dataset Overview
total_transactions = len(df)
start_date = df['transaction_date'].min()
end_date = df['transaction_date'].max()
unique_clients = df['client_id'].nunique()

# Summary Statistics
total_inflow = df[df['transaction_type'].isin(['deposit', 'transfer_in'])]['amount'].sum()
total_outflow = df[df['transaction_type'].isin(['withdrawal', 'transfer_out'])]['amount'].sum()
average_transaction_amount = df['amount'].mean()
largest_deposit = df[df['transaction_type'] == 'deposit']['amount'].max()
largest_withdrawal = df[df['transaction_type'] == 'withdrawal']['amount'].max()

# Distribution of Transactions
transaction_types = df['transaction_type'].value_counts()
transaction_amounts = df['amount']

# Generate the Pie Chart
plt.figure(figsize=(8, 8))
plt.pie(transaction_types, labels=transaction_types.index, autopct='%1.1f%%')
plt.title('Distribution of Transaction Types')
plt.show()

# Generate the Histogram
plt.figure(figsize=(10, 6))
plt.hist(transaction_amounts, bins=20, color='blue', alpha=0.7)
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')
plt.title('Distribution of Transaction Amounts')
plt.show()

# New Feature Suggestions
# 1. Transaction Frequency
df['transaction_date'] = pd.to_datetime(df['transaction_date'])
df['transaction_frequency'] = df.groupby('client_id')['transaction_date'].diff().dt.days

# 2. Transaction Categories (Replace with your own logic)
# df['transaction_category'] = ...

# 3. Transaction Patterns
# df['transaction_patterns'] = ...

# 4. Daily/Weekly/Monthly Aggregates
# df['daily_balance'] = df.groupby(['client_id', pd.Grouper(key='transaction_date', freq='D')])['amount'].cumsum()
# df['weekly_balance'] = df.groupby(['client_id', pd.Grouper(key='transaction_date', freq='W')])['amount'].cumsum()
# df['monthly_balance'] = df.groupby(['client_id', pd.Grouper(key='transaction_date', freq='M')])['amount'].cumsum()

# 5. Balance Change
# df['balance_change'] = df.groupby('client_id')['amount'].cumsum()

# 6. Client Segmentation
# df['client_segment'] = ...

# 7. Transaction Time Analysis
# df['transaction_hour'] = df['transaction_date'].dt.hour
# df['transaction_day'] = df['transaction_date'].dt.day

# Save the updated DataFrame with new features to a new CSV file
df.to_csv('updated_dataset.csv', index=False)

# You can further customize and expand these features based on your specific dataset and analysis needs.











import pandas as pd

# Assuming you have a DataFrame 'df' with columns 'client_id' and 'transaction_date'
# Replace 'df' with the actual variable containing your dataset

# Convert 'transaction_date' to a datetime object if it's not already
df['transaction_date'] = pd.to_datetime(df['transaction_date')

# Create a new column 'transaction_month' to store the month of each transaction
df['transaction_month'] = df['transaction_date'].dt.to_period('M')

# Group the data by 'client_id' and 'transaction_month' and count the number of transactions
client_monthly_transaction_counts = df.groupby(['client_id', 'transaction_month']).size().reset_index(name='transaction_count')

# Calculate the mean number of transactions for each client ID and month
mean_monthly_transaction_count = client_monthly_transaction_counts.groupby('client_id')['transaction_count'].mean()

# Create a new DataFrame to store the SMA
sma_df = pd.DataFrame()

# Calculate SMA for each client
for client_id, mean_transactions in mean_monthly_transaction_count.items():
    client_data = df[df['client_id'] == client_id]
    sma = client_data['transaction_count'].rolling(window=int(mean_transactions)).mean()
    
    # Append the SMA data to the new DataFrame
    sma_df = sma_df.append({'client_id': client_id, 'SMA': sma}, ignore_index=True)

print(sma_df)






import pandas as pd

# Assuming you have a DataFrame 'df' with columns 'client_id' and 'transaction_date'
# Replace 'df' with the actual variable containing your dataset

# Convert 'transaction_date' to a datetime object if it's not already
df['transaction_date'] = pd.to_datetime(df['transaction_date'])

# Create a new column 'transaction_month' to store the month of each transaction
df['transaction_month'] = df['transaction_date'].dt.to_period('M')

# Group the data by 'client_id' and 'transaction_month' and count the number of transactions
client_monthly_transaction_counts = df.groupby(['client_id', 'transaction_month']).size().reset_index(name='transaction_count')

# Calculate the mean number of transactions for each client ID and month
mean_monthly_transaction_count = client_monthly_transaction_counts.groupby('client_id')['transaction_count'].mean()

print(mean_monthly_transaction_count)





import pandas as pd
import matplotlib.pyplot as plt

# Load your transaction data from a CSV file
df = pd.read_csv("transactions.csv")

# Convert the "Date" column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Set the "Date" column as the DataFrame index
df.set_index('Date', inplace=True)

# Calculate the monthly SMA for each client ID
window_size = 1  # Monthly window size
sma_df = df.groupby("Client ID")['Amount'].rolling(window=window_size, freq='M').mean().reset_index()
sma_df.rename(columns={'Amount': f'SMA_{window_size}_Monthly'}, inplace=True)

# Visualize the monthly SMAs for each client ID
for client_id in sma_df['Client ID'].unique():
    client_data = sma_df[sma_df['Client ID'] == client_id]
    plt.figure(figsize=(12, 6))
    plt.plot(client_data['Date'], client_data[f'SMA_{window_size}_Monthly'], label=f'SMA_{window_size}_Monthly')
    plt.plot(df[df['Client ID'] == client_id].index, df[df['Client ID'] == client_id]['Amount'], label='Amount')
    plt.title(f'Monthly SMA for Client ID {client_id}')
    plt.xlabel('Date')
    plt.ylabel('Amount')
    plt.legend()
    plt.grid()
    plt.show()





import pandas as pd
import matplotlib.pyplot as plt

# Load your transaction data from a CSV file
df = pd.read_csv("transactions.csv")

# Convert the "Date" column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Set the "Date" column as the DataFrame index
df.set_index('Date', inplace=True)

# Calculate the monthly SMA for each client ID
window_size = 1  # Monthly window size
sma_df = df.groupby("Client ID")['Amount'].rolling(window=window_size, freq='M').mean().reset_index()
sma_df.rename(columns={'Amount': f'SMA_{window_size}_Monthly'}, inplace=True)

# Visualize the monthly SMAs for each client ID
for client_id in sma_df['Client ID'].unique():
    client_data = sma_df[sma_df['Client ID'] == client_id]
    plt.figure(figsize=(12, 6))
    plt.plot(client_data['Date'], client_data[f'SMA_{window_size}_Monthly'], label=f'SMA_{window_size}_Monthly')
    plt.plot(df[df['Client ID'] == client_id].index, df[df['Client ID'] == client_id]['Amount'], label='Amount')
    plt.title(f'Monthly SMA for Client ID {client_id}')
    plt.xlabel('Date')
    plt.ylabel('Amount')
    plt.legend()
    plt.grid()
    plt.show()







import pandas as pd
import matplotlib.pyplot as plt

# Load your transaction data from a CSV file
df = pd.read_csv("transactions.csv")

# Convert the "Date" column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Set the "Date" column as the DataFrame index
df.set_index('Date', inplace=True)

# Group transactions by month and calculate the total amount for each month
monthly_totals = df.resample('M').sum()

# Plot the monthly tendencies
plt.figure(figsize=(12, 6))
plt.plot(monthly_totals.index, monthly_totals['Amount'], marker='o', linestyle='-')
plt.title('Monthly Tendency of Account Transactions')
plt.xlabel('Month')
plt.ylabel('Total Amount')
plt.grid()
plt.show()






Interpreting a Simple Moving Average (SMA) involves understanding the insights it provides regarding trends and patterns in your data. Here's how to interpret an SMA: Trend Identification: A rising SMA suggests an upward trend in the data, while a falling SMA indicates a downward trend. When the SMA is relatively flat, it suggests a lack of a strong trend. Smoothing Effect: The SMA smooths out short-term fluctuations and noise in your data. This makes it easier to see long-term trends and reduces the impact of temporary spikes or dips. Support and Resistance: Traders and analysts often use SMAs to identify potential support and resistance levels in financial data. For example, if the price of a stock is above its 50-day SMA, the SMA may act as a support level. Crossovers: SMA crossovers, where a shorter-term SMA (e.g., 50-day) crosses above or below a longer-term SMA (e.g., 200-day), can signal changes in trends. A "golden cross" (shorter-term above longer-term) is seen as a bullish signal, while a "death cross" (shorter-term below longer-term) is viewed as bearish. Reversal Points: SMAs can help identify potential reversal points in trends. For example, a sharp price increase followed by a crossover of the SMA could signal an upcoming reversal. Divergences: Analyzing the relationship between the SMA and the actual data can reveal divergences. When the data is making higher highs, but the SMA is not, it could indicate a weakening trend, and vice versa. Use in Forecasting: SMAs can be used to forecast future values, but it's essential to understand that they are lagging indicators. This means they reflect past data, so they may not predict abrupt changes or provide precise future values. Adjustment of Window Size: You can fine-tune the level of smoothing and responsiveness by adjusting the window size "n." A smaller "n" makes the SMA more sensitive to recent changes, while a larger "n" provides smoother, longer-term trends. In summary, SMAs are a valuable tool for trend analysis and noise reduction in time series data. Interpretation involves looking at the relationship between the SMA and the actual data, identifying trends, potential support and resistance levels, and using crossovers and divergences to make informed decisions in various fields, including finance, economics, and business.

Certainly! Here are examples to illustrate each of the interpretations of a Simple Moving Average (SMA):

Trend Identification:
Example: If the 10-day SMA of a stock price is consistently rising, it indicates an upward trend, as shown by the increasing SMA values.

Smoothing Effect:
Example: Daily stock prices can be very volatile, but a 50-day SMA smooths out these fluctuations, making it easier to see the underlying trend.

Support and Resistance:
Example: If the 200-day SMA of a stock price acts as a support level, it means that the stock tends to bounce back when its price approaches or touches this moving average.

Crossovers:
Example: When the 50-day SMA crosses above the 200-day SMA for a stock, it's often seen as a "golden cross," signaling a potential bullish trend reversal.

Reversal Points:
Example: A stock has been in a strong uptrend, but when its 10-day SMA crosses below its 30-day SMA, it could indicate a potential reversal or a downtrend.

Divergences:
Example: The stock's price is making higher highs, but the 14-day SMA is not keeping pace, suggesting a potential weakening of the trend.

Use in Forecasting:
Example: You can use a 5-day SMA to forecast the average sales for the next week based on historical sales data, but it may not capture sudden changes in demand.

Adjustment of Window Size:
Example: If you use a 20-day SMA for analyzing monthly sales data, it provides a more stable and long-term view of trends, whereas a 5-day SMA would be more responsive but noisier for the same data.

These examples illustrate how SMAs can be applied to various scenarios and highlight their utility in trend analysis and decision-making across different domains.









# Description du Jeu de Données

Nous avons importé un jeu de données qui comprend les informations suivantes. Voici les types de données associés à chaque colonne :

1. **Numero_compte** : Texte (object)
2. **RACINE** : Entier (int64)
3. **ORDINAL** : Entier (int64)
4. **Date_operation_Atlas** : Texte (object)
5. **NO_MVT_DANS_OPERATION** : Entier (int64)
6. **Libelle_operation** : Texte (object)
7. **Libelle_complementaire_operation** : Texte (object)
8. **Code_operation** : Entier (int64)
9. **Montant_evt_comptable** : Flottant (float64)
10. **Reference_denotage** : Texte (object)
11. **Reference_client** : Entier (int64)
12. **Date Valeur comptable** : Texte (object)

Ce jeu de données semble contenir une variété de types de données, notamment des données textuelles, des entiers et des valeurs décimales. Avant de poursuivre notre analyse, nous devrons peut-être effectuer des transformations ou des nettoyages de données en fonction de ces types. Il est essentiel de comprendre ces types pour travailler efficacement avec les données.



# Chargement des Packages

Dans cette section, nous allons charger les packages et les bibliothèques Python nécessaires pour faciliter notre projet de détection de fraude au sein des communautés en utilisant la théorie des graphes. Ces packages sont essentiels pour la manipulation des données, la visualisation et l'apprentissage automatique. Voici la liste des packages que nous avons importés :

1. **pandas** : Utilisé pour la manipulation et l'analyse des données.
2. **seaborn** : Une bibliothèque de visualisation de données basée sur matplotlib.
3. **openpyxl** : Permet de travailler avec des fichiers Excel.
4. **numpy** : Fournit un support pour les fonctions mathématiques et les tableaux.
5. **networkx** : Une bibliothèque puissante pour la création, la manipulation et l'étude des réseaux complexes.
6. **re** : Fournit des opérations d'expressions régulières.
7. **pickle** : Utilisé pour la sérialisation et la désérialisation des objets Python.
8. **time** : Pour les fonctions et mesures liées au temps.
9. **matplotlib** : Nous permet de créer diverses visualisations de données.
10. **collections** : Fournit des structures de données et des algorithmes supplémentaires.
11. **datetime** : Pour travailler avec les dates et les heures.
12. **sklearn.model_selection** : Fait partie de scikit-learn, utilisé pour la division des données et l'évaluation des modèles.
13. **sklearn.tree** : Contient DecisionTreeClassifier pour les tâches d'apprentissage automatique.
14. **sklearn.linear_model** : Pour la modélisation de régression linéaire.
15. **sklearn.feature_extraction.text** : Prend en charge le traitement des données textuelles à l'aide de CountVectorizer.
16. **sklearn.metrics** : Contient la métrique de score d'exactitude pour l'évaluation des modèles.
17. **warnings** : Aide à gérer les messages d'avertissement dans le code.
18. **pyvis.network** : Utilisé pour créer des visualisations interactives de réseaux.
19. **community** : Fait partie de networkx, pour la détection de communautés dans les graphes.

Ces packages joueront un rôle crucial dans notre projet, de la prétraitement et l'analyse des données à la visualisation des graphes et à l'apprentissage automatique pour la détection de la fraude. Plongeons dans la mise en œuvre de notre algorithme de détection de fraude au sein des communautés en utilisant ces bibliothèques.



import pytesseract
from pdf2image import convert_from_path

# Path to your scanned PDF
pdf_path = 'your_swift_messages.pdf'  # Replace with the path to your scanned PDF

# Convert each page of the scanned PDF to an image and extract text
extracted_text = ""
images = convert_from_path(pdf_path)

for image in images:
    text = pytesseract.image_to_string(image)
    extracted_text += text

# Print or process the extracted text as needed
print(extracted_text)







import PyPDF2
import pytesseract
from PIL import Image

# Open the scanned PDF
pdf_path = 'your_swift_messages.pdf'  # Replace with the path to your scanned PDF
pdf_file = open(pdf_path, 'rb')
pdf_reader = PyPDF2.PdfFileReader(pdf_file)

# Initialize a variable to store the extracted text
extracted_text = ""

# Loop through each page of the PDF
for page_num in range(pdf_reader.numPages):
    page = pdf_reader.getPage(page_num)
    
    # Convert the scanned page to an image
    page_image = page.extract_text() 
    page_image = Image.frombytes('L', page_image.size, page_image)
    
    # Perform OCR to extract text from the image
    text = pytesseract.image_to_string(page_image)
    
    # Append the extracted text from this page to the result
    extracted_text += text

# Close the PDF file
pdf_file.close()

# Print or process the extracted text as needed
print(extracted_text)






import pytesseract.pytesseract as pytesseract
from PIL import Image

# Open the scanned image
image = Image.open('your_swift_message.png')  # Replace with the path to your image file

# Perform OCR to extract text
extracted_text = pytesseract.image_to_string(image)

# Print or process the extracted text as needed
print(extracted_text)

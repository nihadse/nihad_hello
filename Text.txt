import pandas as pd

# Sample DataFrame with 'ID', 'amount', and 'label' columns
data = {'ID': [1, 2, 3, 4, 5],
        'amount': [100, 200, 150, 50, 300],
        'label': ['Verment', 'Purchase', 'Verment', 'Transfer', 'Verment']}
df = pd.DataFrame(data)

# Define the label you want to extract
label_to_extract = 'Verment'

# Create the new column 'amount_Verment' based on the label
df['amount_Verment'] = df.apply(lambda row: row['amount'] if row['label'] == label_to_extract else 0, axis=1)

# Print the resulting DataFrame
print(df)









import pandas as pd

# Sample DataFrame with 'ID', 'amount', and 'label' columns
data = {'ID': [1, 2, 3, 4, 5],
        'amount': [100, 200, 150, 50, 300],
        'label': ['Verment', 'Purchase', 'Verment', 'Transfer', 'Verment']}
df = pd.DataFrame(data)

# Function to extract the amount based on the label
def extract_amount(label, label_to_extract):
    if label == label_to_extract:
        return df['amount']
    else:
        return 0

# Define the label you want to extract
label_to_extract = 'Verment'

# Create the new column 'amount_Verment' using the apply function
df['amount_Verment'] = df['label'].apply(extract_amount, args=(label_to_extract,))

# Print the resulting DataFrame
print(df)






# Sample DataFrames with 'name' and 'complimentary' columns
client_names_data = {'name': ['John Doe', 'Alice Smith', 'Bob Johnson']}
complimentary_data = {'complimentary': ['John Doe', 'Eve Johnson', 'Alice Smith', 'David Lee']}
client_names_df = pd.DataFrame(client_names_data)
complimentary_df = pd.DataFrame(complimentary_data)

# Check if names in 'complimentary' column match client names
complimentary_df['is_client'] = complimentary_df['complimentary'].isin(client_names_df['name'])

# Print the resulting DataFrame with 'is_client' column
print(complimentary_df)







# Sample list of client names (you would replace this with your actual client data)
client_names = ['John Doe', 'Alice Smith', 'Bob Johnson']

# Example DataFrame with a 'complimentary' column
data = {'complimentary': ['John Doe', 'Eve Johnson', 'Alice Smith', 'David Lee']}
df = pd.DataFrame(data)

# Check if names in 'complimentary' column match client names
df['is_client'] = df['complimentary'].isin(client_names)

# Print the resulting DataFrame
print(df)






import pandas as pd import numpy as np # Load your client data into a Pandas DataFrame # Replace 'your_client_data.csv' with the actual file path or URL of your client data df = pd.read_csv('your_client_data.csv') # 1. Transaction Frequency # Calculate the average number of transactions per day for each client. df['transaction_date'] = pd.to_datetime(df['transaction_date']) df['transaction_frequency'] = df.groupby('client_id')['transaction_date'].diff().dt.days # 2. Preferred Transaction Type # Identify the most common transaction type for each client. df['preferred_transaction_type'] = df.groupby('client_id')['transaction_type'].transform(lambda x: x.mode().iloc[0]) # 3. Average Transaction Amount # Calculate the average transaction amount for each client. df['average_transaction_amount'] = df.groupby('client_id')['amount'].transform('mean') # 4. Recency of Activity # Calculate the number of days since the last transaction for each client. df['recency_of_activity'] = (df['transaction_date'].max() - df.groupby('client_id')['transaction_date'].transform('max')).dt.days # 5. Client Tenure # Calculate the number of days since the first transaction for each client. df['client_tenure'] = (df['transaction_date'].max() - df.groupby('client_id')['transaction_date'].transform('min')).dt.days # 6. Spending Habits # Analyze spending patterns by calculating the proportion of transactions that are withdrawals. df['withdrawal_proportion'] = df.groupby('client_id')['transaction_type'].transform(lambda x: (x == 'withdrawal').mean()) # 7. Transaction Day and Time # Extract the day of the week and hour of the day for each transaction. df['transaction_day'] = df['transaction_date'].dt.day_name() df['transaction_hour'] = df['transaction_date'].dt.hour # 8. Cumulative Transaction Count # Calculate the cumulative number of transactions for each client. df['cumulative_transaction_count'] = df.groupby('client_id').cumcount() + 1 # 9. Client Segmentation # Use the behavior features to segment clients into groups, e.g., active, passive, high spenders, low spenders. # You can use clustering or predefined criteria for segmentation. # Save the updated DataFrame with behavior analysis features to a new CSV file df.to_csv('updated_client_data.csv', index=False) # These features can help you gain insights into your clients' behaviors and tailor your analysis accordingly.





import pandas as pd

# Load the updated client data with behavior analysis features
df = pd.read_csv('updated_client_data.csv')

# 1. Transaction Frequency Analysis
# Analyze the transaction frequency distribution for clients.
transaction_frequency_stats = df['transaction_frequency'].describe()
# You can create histograms or box plots to visualize the distribution.

# 2. Preferred Transaction Type Analysis
# Determine the most common transaction types among clients.
preferred_transaction_type_counts = df['preferred_transaction_type'].value_counts()

# 3. Average Transaction Amount Analysis
# Analyze the distribution of average transaction amounts for clients.
average_transaction_amount_stats = df['average_transaction_amount'].describe()
# Create histograms or box plots for visualization.

# 4. Recency of Activity Analysis
# Understand how recently clients have been active.
recency_stats = df['recency_of_activity'].describe()

# 5. Client Tenure Analysis
# Analyze how long clients have been with your business.
tenure_stats = df['client_tenure'].describe()

# 6. Spending Habits Analysis
# Analyze spending patterns by examining withdrawal proportions.
withdrawal_proportions_stats = df['withdrawal_proportion'].describe()

# 7. Transaction Day and Time Analysis
# Identify common transaction days and hours.
transaction_day_counts = df['transaction_day'].value_counts()
transaction_hour_counts = df['transaction_hour'].value_counts()

# 8. Cumulative Transaction Count Analysis
# Analyze the cumulative transaction count over time for each client.
# This can help identify trends in client activity.
# You can create time series plots for visualization.

# 9. Client Segmentation Analysis
# Apply clustering algorithms or predefined criteria to segment clients.
# Analyze the characteristics and behaviors of each segment separately.

# You can use various data visualization libraries like Matplotlib or Seaborn to create plots and charts to visualize the results of your analysis.

# Example: Creating a bar chart for transaction day analysis
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.bar(transaction_day_counts.index, transaction_day_counts.values)
plt.title('Transaction Day Analysis')
plt.xlabel('Day of the Week')
plt.ylabel('Transaction Count')
plt.show()

# You can repeat similar visualization and analysis for other behavior features.





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load your dataset into a Pandas DataFrame
# Replace 'your_dataset.csv' with the actual file path or URL of your dataset
df = pd.read_csv('your_dataset.csv')

# Dataset Overview
total_transactions = len(df)
start_date = df['transaction_date'].min()
end_date = df['transaction_date'].max()
unique_clients = df['client_id'].nunique()

# Summary Statistics
total_inflow = df[df['transaction_type'].isin(['deposit', 'transfer_in'])]['amount'].sum()
total_outflow = df[df['transaction_type'].isin(['withdrawal', 'transfer_out'])]['amount'].sum()
average_transaction_amount = df['amount'].mean()
largest_deposit = df[df['transaction_type'] == 'deposit']['amount'].max()
largest_withdrawal = df[df['transaction_type'] == 'withdrawal']['amount'].max()

# Distribution of Transactions
transaction_types = df['transaction_type'].value_counts()
transaction_amounts = df['amount']

# Generate the Pie Chart
plt.figure(figsize=(8, 8))
plt.pie(transaction_types, labels=transaction_types.index, autopct='%1.1f%%')
plt.title('Distribution of Transaction Types')
plt.show()

# Generate the Histogram
plt.figure(figsize=(10, 6))
plt.hist(transaction_amounts, bins=20, color='blue', alpha=0.7)
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')
plt.title('Distribution of Transaction Amounts')
plt.show()

# New Feature Suggestions
# 1. Transaction Frequency
df['transaction_date'] = pd.to_datetime(df['transaction_date'])
df['transaction_frequency'] = df.groupby('client_id')['transaction_date'].diff().dt.days

# 2. Transaction Categories (Replace with your own logic)
# df['transaction_category'] = ...

# 3. Transaction Patterns
# df['transaction_patterns'] = ...

# 4. Daily/Weekly/Monthly Aggregates
# df['daily_balance'] = df.groupby(['client_id', pd.Grouper(key='transaction_date', freq='D')])['amount'].cumsum()
# df['weekly_balance'] = df.groupby(['client_id', pd.Grouper(key='transaction_date', freq='W')])['amount'].cumsum()
# df['monthly_balance'] = df.groupby(['client_id', pd.Grouper(key='transaction_date', freq='M')])['amount'].cumsum()

# 5. Balance Change
# df['balance_change'] = df.groupby('client_id')['amount'].cumsum()

# 6. Client Segmentation
# df['client_segment'] = ...

# 7. Transaction Time Analysis
# df['transaction_hour'] = df['transaction_date'].dt.hour
# df['transaction_day'] = df['transaction_date'].dt.day

# Save the updated DataFrame with new features to a new CSV file
df.to_csv('updated_dataset.csv', index=False)

# You can further customize and expand these features based on your specific dataset and analysis needs.











import pandas as pd

# Assuming you have a DataFrame 'df' with columns 'client_id' and 'transaction_date'
# Replace 'df' with the actual variable containing your dataset

# Convert 'transaction_date' to a datetime object if it's not already
df['transaction_date'] = pd.to_datetime(df['transaction_date')

# Create a new column 'transaction_month' to store the month of each transaction
df['transaction_month'] = df['transaction_date'].dt.to_period('M')

# Group the data by 'client_id' and 'transaction_month' and count the number of transactions
client_monthly_transaction_counts = df.groupby(['client_id', 'transaction_month']).size().reset_index(name='transaction_count')

# Calculate the mean number of transactions for each client ID and month
mean_monthly_transaction_count = client_monthly_transaction_counts.groupby('client_id')['transaction_count'].mean()

# Create a new DataFrame to store the SMA
sma_df = pd.DataFrame()

# Calculate SMA for each client
for client_id, mean_transactions in mean_monthly_transaction_count.items():
    client_data = df[df['client_id'] == client_id]
    sma = client_data['transaction_count'].rolling(window=int(mean_transactions)).mean()
    
    # Append the SMA data to the new DataFrame
    sma_df = sma_df.append({'client_id': client_id, 'SMA': sma}, ignore_index=True)

print(sma_df)






import pandas as pd

# Assuming you have a DataFrame 'df' with columns 'client_id' and 'transaction_date'
# Replace 'df' with the actual variable containing your dataset

# Convert 'transaction_date' to a datetime object if it's not already
df['transaction_date'] = pd.to_datetime(df['transaction_date'])

# Create a new column 'transaction_month' to store the month of each transaction
df['transaction_month'] = df['transaction_date'].dt.to_period('M')

# Group the data by 'client_id' and 'transaction_month' and count the number of transactions
client_monthly_transaction_counts = df.groupby(['client_id', 'transaction_month']).size().reset_index(name='transaction_count')

# Calculate the mean number of transactions for each client ID and month
mean_monthly_transaction_count = client_monthly_transaction_counts.groupby('client_id')['transaction_count'].mean()

print(mean_monthly_transaction_count)





import pandas as pd
import matplotlib.pyplot as plt

# Load your transaction data from a CSV file
df = pd.read_csv("transactions.csv")

# Convert the "Date" column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Set the "Date" column as the DataFrame index
df.set_index('Date', inplace=True)

# Calculate the monthly SMA for each client ID
window_size = 1  # Monthly window size
sma_df = df.groupby("Client ID")['Amount'].rolling(window=window_size, freq='M').mean().reset_index()
sma_df.rename(columns={'Amount': f'SMA_{window_size}_Monthly'}, inplace=True)

# Visualize the monthly SMAs for each client ID
for client_id in sma_df['Client ID'].unique():
    client_data = sma_df[sma_df['Client ID'] == client_id]
    plt.figure(figsize=(12, 6))
    plt.plot(client_data['Date'], client_data[f'SMA_{window_size}_Monthly'], label=f'SMA_{window_size}_Monthly')
    plt.plot(df[df['Client ID'] == client_id].index, df[df['Client ID'] == client_id]['Amount'], label='Amount')
    plt.title(f'Monthly SMA for Client ID {client_id}')
    plt.xlabel('Date')
    plt.ylabel('Amount')
    plt.legend()
    plt.grid()
    plt.show()





import pandas as pd
import matplotlib.pyplot as plt

# Load your transaction data from a CSV file
df = pd.read_csv("transactions.csv")

# Convert the "Date" column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Set the "Date" column as the DataFrame index
df.set_index('Date', inplace=True)

# Calculate the monthly SMA for each client ID
window_size = 1  # Monthly window size
sma_df = df.groupby("Client ID")['Amount'].rolling(window=window_size, freq='M').mean().reset_index()
sma_df.rename(columns={'Amount': f'SMA_{window_size}_Monthly'}, inplace=True)

# Visualize the monthly SMAs for each client ID
for client_id in sma_df['Client ID'].unique():
    client_data = sma_df[sma_df['Client ID'] == client_id]
    plt.figure(figsize=(12, 6))
    plt.plot(client_data['Date'], client_data[f'SMA_{window_size}_Monthly'], label=f'SMA_{window_size}_Monthly')
    plt.plot(df[df['Client ID'] == client_id].index, df[df['Client ID'] == client_id]['Amount'], label='Amount')
    plt.title(f'Monthly SMA for Client ID {client_id}')
    plt.xlabel('Date')
    plt.ylabel('Amount')
    plt.legend()
    plt.grid()
    plt.show()







import pandas as pd
import matplotlib.pyplot as plt

# Load your transaction data from a CSV file
df = pd.read_csv("transactions.csv")

# Convert the "Date" column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Set the "Date" column as the DataFrame index
df.set_index('Date', inplace=True)

# Group transactions by month and calculate the total amount for each month
monthly_totals = df.resample('M').sum()

# Plot the monthly tendencies
plt.figure(figsize=(12, 6))
plt.plot(monthly_totals.index, monthly_totals['Amount'], marker='o', linestyle='-')
plt.title('Monthly Tendency of Account Transactions')
plt.xlabel('Month')
plt.ylabel('Total Amount')
plt.grid()
plt.show()






Interpreting a Simple Moving Average (SMA) involves understanding the insights it provides regarding trends and patterns in your data. Here's how to interpret an SMA: Trend Identification: A rising SMA suggests an upward trend in the data, while a falling SMA indicates a downward trend. When the SMA is relatively flat, it suggests a lack of a strong trend. Smoothing Effect: The SMA smooths out short-term fluctuations and noise in your data. This makes it easier to see long-term trends and reduces the impact of temporary spikes or dips. Support and Resistance: Traders and analysts often use SMAs to identify potential support and resistance levels in financial data. For example, if the price of a stock is above its 50-day SMA, the SMA may act as a support level. Crossovers: SMA crossovers, where a shorter-term SMA (e.g., 50-day) crosses above or below a longer-term SMA (e.g., 200-day), can signal changes in trends. A "golden cross" (shorter-term above longer-term) is seen as a bullish signal, while a "death cross" (shorter-term below longer-term) is viewed as bearish. Reversal Points: SMAs can help identify potential reversal points in trends. For example, a sharp price increase followed by a crossover of the SMA could signal an upcoming reversal. Divergences: Analyzing the relationship between the SMA and the actual data can reveal divergences. When the data is making higher highs, but the SMA is not, it could indicate a weakening trend, and vice versa. Use in Forecasting: SMAs can be used to forecast future values, but it's essential to understand that they are lagging indicators. This means they reflect past data, so they may not predict abrupt changes or provide precise future values. Adjustment of Window Size: You can fine-tune the level of smoothing and responsiveness by adjusting the window size "n." A smaller "n" makes the SMA more sensitive to recent changes, while a larger "n" provides smoother, longer-term trends. In summary, SMAs are a valuable tool for trend analysis and noise reduction in time series data. Interpretation involves looking at the relationship between the SMA and the actual data, identifying trends, potential support and resistance levels, and using crossovers and divergences to make informed decisions in various fields, including finance, economics, and business.

Certainly! Here are examples to illustrate each of the interpretations of a Simple Moving Average (SMA):

Trend Identification:
Example: If the 10-day SMA of a stock price is consistently rising, it indicates an upward trend, as shown by the increasing SMA values.

Smoothing Effect:
Example: Daily stock prices can be very volatile, but a 50-day SMA smooths out these fluctuations, making it easier to see the underlying trend.

Support and Resistance:
Example: If the 200-day SMA of a stock price acts as a support level, it means that the stock tends to bounce back when its price approaches or touches this moving average.

Crossovers:
Example: When the 50-day SMA crosses above the 200-day SMA for a stock, it's often seen as a "golden cross," signaling a potential bullish trend reversal.

Reversal Points:
Example: A stock has been in a strong uptrend, but when its 10-day SMA crosses below its 30-day SMA, it could indicate a potential reversal or a downtrend.

Divergences:
Example: The stock's price is making higher highs, but the 14-day SMA is not keeping pace, suggesting a potential weakening of the trend.

Use in Forecasting:
Example: You can use a 5-day SMA to forecast the average sales for the next week based on historical sales data, but it may not capture sudden changes in demand.

Adjustment of Window Size:
Example: If you use a 20-day SMA for analyzing monthly sales data, it provides a more stable and long-term view of trends, whereas a 5-day SMA would be more responsive but noisier for the same data.

These examples illustrate how SMAs can be applied to various scenarios and highlight their utility in trend analysis and decision-making across different domains.









# Description du Jeu de Données

Nous avons importé un jeu de données qui comprend les informations suivantes. Voici les types de données associés à chaque colonne :

1. **Numero_compte** : Texte (object)
2. **RACINE** : Entier (int64)
3. **ORDINAL** : Entier (int64)
4. **Date_operation_Atlas** : Texte (object)
5. **NO_MVT_DANS_OPERATION** : Entier (int64)
6. **Libelle_operation** : Texte (object)
7. **Libelle_complementaire_operation** : Texte (object)
8. **Code_operation** : Entier (int64)
9. **Montant_evt_comptable** : Flottant (float64)
10. **Reference_denotage** : Texte (object)
11. **Reference_client** : Entier (int64)
12. **Date Valeur comptable** : Texte (object)

Ce jeu de données semble contenir une variété de types de données, notamment des données textuelles, des entiers et des valeurs décimales. Avant de poursuivre notre analyse, nous devrons peut-être effectuer des transformations ou des nettoyages de données en fonction de ces types. Il est essentiel de comprendre ces types pour travailler efficacement avec les données.



# Chargement des Packages

Dans cette section, nous allons charger les packages et les bibliothèques Python nécessaires pour faciliter notre projet de détection de fraude au sein des communautés en utilisant la théorie des graphes. Ces packages sont essentiels pour la manipulation des données, la visualisation et l'apprentissage automatique. Voici la liste des packages que nous avons importés :

1. **pandas** : Utilisé pour la manipulation et l'analyse des données.
2. **seaborn** : Une bibliothèque de visualisation de données basée sur matplotlib.
3. **openpyxl** : Permet de travailler avec des fichiers Excel.
4. **numpy** : Fournit un support pour les fonctions mathématiques et les tableaux.
5. **networkx** : Une bibliothèque puissante pour la création, la manipulation et l'étude des réseaux complexes.
6. **re** : Fournit des opérations d'expressions régulières.
7. **pickle** : Utilisé pour la sérialisation et la désérialisation des objets Python.
8. **time** : Pour les fonctions et mesures liées au temps.
9. **matplotlib** : Nous permet de créer diverses visualisations de données.
10. **collections** : Fournit des structures de données et des algorithmes supplémentaires.
11. **datetime** : Pour travailler avec les dates et les heures.
12. **sklearn.model_selection** : Fait partie de scikit-learn, utilisé pour la division des données et l'évaluation des modèles.
13. **sklearn.tree** : Contient DecisionTreeClassifier pour les tâches d'apprentissage automatique.
14. **sklearn.linear_model** : Pour la modélisation de régression linéaire.
15. **sklearn.feature_extraction.text** : Prend en charge le traitement des données textuelles à l'aide de CountVectorizer.
16. **sklearn.metrics** : Contient la métrique de score d'exactitude pour l'évaluation des modèles.
17. **warnings** : Aide à gérer les messages d'avertissement dans le code.
18. **pyvis.network** : Utilisé pour créer des visualisations interactives de réseaux.
19. **community** : Fait partie de networkx, pour la détection de communautés dans les graphes.

Ces packages joueront un rôle crucial dans notre projet, de la prétraitement et l'analyse des données à la visualisation des graphes et à l'apprentissage automatique pour la détection de la fraude. Plongeons dans la mise en œuvre de notre algorithme de détection de fraude au sein des communautés en utilisant ces bibliothèques.



import pytesseract
from pdf2image import convert_from_path

# Path to your scanned PDF
pdf_path = 'your_swift_messages.pdf'  # Replace with the path to your scanned PDF

# Convert each page of the scanned PDF to an image and extract text
extracted_text = ""
images = convert_from_path(pdf_path)

for image in images:
    text = pytesseract.image_to_string(image)
    extracted_text += text

# Print or process the extracted text as needed
print(extracted_text)







import PyPDF2
import pytesseract
from PIL import Image

# Open the scanned PDF
pdf_path = 'your_swift_messages.pdf'  # Replace with the path to your scanned PDF
pdf_file = open(pdf_path, 'rb')
pdf_reader = PyPDF2.PdfFileReader(pdf_file)

# Initialize a variable to store the extracted text
extracted_text = ""

# Loop through each page of the PDF
for page_num in range(pdf_reader.numPages):
    page = pdf_reader.getPage(page_num)
    
    # Convert the scanned page to an image
    page_image = page.extract_text() 
    page_image = Image.frombytes('L', page_image.size, page_image)
    
    # Perform OCR to extract text from the image
    text = pytesseract.image_to_string(page_image)
    
    # Append the extracted text from this page to the result
    extracted_text += text

# Close the PDF file
pdf_file.close()

# Print or process the extracted text as needed
print(extracted_text)






import pytesseract.pytesseract as pytesseract
from PIL import Image

# Open the scanned image
image = Image.open('your_swift_message.png')  # Replace with the path to your image file

# Perform OCR to extract text
extracted_text = pytesseract.image_to_string(image)

# Print or process the extracted text as needed
print(extracted_text)

def agg_func(x):
    if x.name == 'id_tiers':
        return ', '.join(x.astype(str))  # Combine id_tiers values into a string
    return x.iloc[0]  # Return the first value for other columns

# Apply the custom aggregation function, grouping by both 'raison sociale' and 'birthday'
client_database = client_database_.groupby(['raison sociale', 'birthday']).agg(agg_func).reset_index()

print(client_database)





def agg_func(x):
    if x.name == 'id_tiers':
        return ', '.join(x.astype(str))  # Combine id_tiers values into a string
    return x.iloc[0]  # Return the first value for other columns

# Apply the custom aggregation function, grouping by both 'raison sociale' and 'birthday'
client_database = client_database_.groupby(['raison sociale', 'birthday']).agg(agg_func).reset_index()

print(client_database)



def agg_func(x):
    if x.name == 'id_tiers':
        # Combine id_tiers into a comma-separated string
        return ', '.join(x.astype(str))
    elif x.name == 'birthday':
        # If all birthdays are the same, return only one; otherwise, list all unique birthdays
        unique_birthdays = x.unique()
        if len(unique_birthdays) == 1:
            return unique_birthdays[0]
        else:
            return ', '.join(unique_birthdays.astype(str))
    else:
        # For other columns, return the first entry
        return x.iloc[0]

# Apply the custom aggregation function
client_database = client_database_.groupby(['raison sociale', 'name']).agg(agg_func).reset_index()

print(client_database)


def agg_func(x):
    if x.name == 'id_tiers':
        # Combine id_tiers into a comma-separated string
        return ', '.join(x.astype(str))
    elif x.name == 'birthday':
        # Return the first birthday (assumes all are the same in the group)
        return x.iloc[0]
    else:
        # For other columns, concatenate as a single string or return first value
        return x.iloc[0]

# Apply the custom aggregation function
client_database = client_database_.groupby(['raison sociale', 'birthday']).agg(agg_func).reset_index()

print(client_database)









def agg_func(x):
    if x.name == 'id_tiers':
        # Combine id_tiers into a comma-separated string
        return ', '.join(x.astype(str))
    elif x.name == 'birthday':
        # Return the first birthday (assumes all are the same in the group)
        return x.iloc[0]
    else:
        # For other columns, concatenate as a single string or return first value
        return x.iloc[0]

# Apply the custom aggregation function
client_database = client_database_.groupby(['raison sociale', 'birthday']).agg(agg_func).reset_index()

print(client_database)






# Define the custom aggregation function
def agg_func(x):
    if x.name == 'Id tiers':  # For 'Id tiers', combine into a single string
        return ', '.join(map(str, x))
    else:
        return x.iloc[0]  # For other columns, take the first value

# Define a function to group by both 'raison sociale' and 'date of birth'
def group_by_person(client_database_):
    # Group by both 'raison sociale' and 'date of birth'
    grouped_data = client_database_.groupby(['raison sociale', 'date of birth']).agg(agg_func).reset_index()
    return grouped_data

# Apply the grouping function
client_database_ = group_by_person(client_database_)







# Define a custom aggregation function
def agg_func(x):
    if x.name == 'Id tiers':  # Handle 'Id tiers'
        return x.iloc[0]
    elif x.name == 'date of birth' or x.name == 'date of creation':
        # Return date of birth for 'personne physique' and date of creation for 'personne morale'
        if 'code nature juridique' in x.index and x['code nature juridique'].iloc[0] == 1:
            return x['date of creation'].iloc[0] if 'date of creation' in x else None
        elif 'code nature juridique' in x.index and x['code nature juridique'].iloc[0] == 2:
            return x['date of birth'].iloc[0] if 'date of birth' in x else None
    elif x.name == 'raison sociale':  # Handle 'raison sociale'
        return x.iloc[0]
    else:
        return ', '.join(map(str, x))  # Default join for other columns

# Group by 'raison sociale' and 'date of birth'/'date of creation' based on client type
aggregated_data = client_database.groupby(
    ['raison sociale', 'code nature juridique', 'date of birth', 'date of creation']
).agg(agg_func).reset_index()

# Display the resulting DataFrame
print(aggregated_data)






import streamlit as st
from langchain.document_loaders import PyPDFDirectoryLoader
import httpx

# Azure API credentials
OIDC_CLIENT_ID = "Your_Client_ID"
OIDC_CLIENT_SECRET = "Your_Client_Secret"
OIDC_ENDPOINT = "Your_OIDC_Endpoint"
OIDC_SCOPE = "Your_OIDC_Scope"
APIGEE_ENDPOINT = "Your_APIGEE_Endpoint"
AZURE_AOAI_MODEL_DEPLOYMENT_NAME = "gpt-4"
AZURE_AOAI_API_VERSION = "2024-06-01"
AZURE_AOAI_KEY = "Your_API_Key"

# Load PDFs from a predefined directory
def load_pdfs_from_directory(directory_path):
    loader = PyPDFDirectoryLoader(directory_path)
    docs = loader.load()
    combined_text = "\n".join([doc.page_content for doc in docs])
    return combined_text

# Function to process the question and fetch response
def ask_question(docs, question):
    prompt = f"{docs}\n\nQuestion: {question}\n\nProvide the pages and sources for the information."
    with httpx.Client(verify=False) as client:
        response = client.post(
            APIGEE_ENDPOINT,
            headers={"Authorization": f"Bearer {AZURE_AOAI_KEY}"},
            json={
                "messages": [
                    {"role": "system", "content": "You are an AI assistant."},
                    {"role": "user", "content": prompt}
                ],
                "model": AZURE_AOAI_MODEL_DEPLOYMENT_NAME
            }
        )
        return response.json()["choices"][0]["message"]["content"]

# Preload the PDFs
directory_path = "/path/to/your/pdf/directory"  # Update this with the path to your PDF directory
docs = load_pdfs_from_directory(directory_path)

# Streamlit App
st.title("RAG Chatbot - DATA Procedures")
st.sidebar.title("Instructions")
st.sidebar.write("This chatbot answers questions using preloaded PDF documents.")

# Main input and output
question = st.text_input("Ask your question:")
if st.button("Submit"):
    if question:
        with st.spinner("Processing..."):
            response = ask_question(docs, question)
        st.success("Answer:")
        st.write(response)
    else:
        st.error("Please enter a question.")




import streamlit as st
from httpx_oauth import OAuth2ClientCredentials
import fitz  # PyMuPDF
from langchain.document_loaders import PyPDFDirectoryLoader
from sentence_transformers import SentenceTransformer
import httpx

# Azure API credentials
OIDC_CLIENT_ID = "Your_Client_ID"
OIDC_CLIENT_SECRET = "Your_Client_Secret"
OIDC_ENDPOINT = "Your_OIDC_Endpoint"
OIDC_SCOPE = "Your_OIDC_Scope"
APIGEE_ENDPOINT = "Your_APIGEE_Endpoint"
AZURE_AOAI_MODEL_DEPLOYMENT_NAME = "gpt-4"
AZURE_AOAI_API_VERSION = "2024-06-01"
AZURE_AOAI_KEY = "Your_API_Key"

# Initialize OAuth2
auth = OAuth2ClientCredentials(
    token_url=OIDC_ENDPOINT,
    client_id=OIDC_CLIENT_ID,
    client_secret=OIDC_CLIENT_SECRET,
    scope=OIDC_SCOPE
)

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    pdf_reader = fitz.open(pdf_path)
    full_text = []
    for page_num in range(len(pdf_reader)):
        page = pdf_reader[page_num]
        full_text.append(page.get_text())
    return "\n".join(full_text)

# Function to process the question and fetch response
def ask_question(docs, question):
    prompt = f"{docs}\n\nQuestion: {question}\n\nProvide the pages and sources for the information."
    with httpx.Client(verify=False) as client:
        response = client.post(
            APIGEE_ENDPOINT,
            headers={"Authorization": f"Bearer {AZURE_AOAI_KEY}"},
            json={
                "messages": [
                    {"role": "system", "content": "You are an AI assistant."},
                    {"role": "user", "content": prompt}
                ],
                "model": AZURE_AOAI_MODEL_DEPLOYMENT_NAME
            }
        )
        return response.json()["choices"][0]["message"]["content"]

# Streamlit App
st.title("RAG Chatbot - DATA Procedures")
st.sidebar.title("Options")
uploaded_file = st.sidebar.file_uploader("Upload a PDF file", type=["pdf"])

if uploaded_file:
    # Load and display the PDF file
    with open("uploaded_file.pdf", "wb") as f:
        f.write(uploaded_file.read())
    
    st.sidebar.success("PDF uploaded successfully!")
    
    docs = extract_text_from_pdf("uploaded_file.pdf")

    question = st.text_input("Ask your question:")
    if st.button("Submit"):
        if question:
            with st.spinner("Processing..."):
                response = ask_question(docs, question)
            st.success("Answer:")
            st.write(response)
        else:
            st.error("Please enter a question.")
else:
    st.warning("Please upload a PDF file to proceed.")









import PyPDF2
import httpx
from azure.ai.openai import AzureOpenAI

# Constants
OIDC_CLIENT_ID = "HP162k8eKJe3bkumdGc32a9UwyX90KhА"
OIDC_CLIENT_SECRET = "HXtu1PPlazqZ16ys"
OIDC_ENDPOINT = "https://alfactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_SCOPE = "genai-model"
APIGEE_ENDPOINT = "https://aifactory.api.staging.echonet/genai-model/v1"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
AZURE_ADAI_API_VERSION = "2024-02-15-preview"
AZURE_ADAI_API_KEY = "FAKE KEY"

oauth2_httpxclient = httpx.Client(verify=False)
auth = {
    "OIDC_ENDPOINT": OIDC_ENDPOINT,
    "client_id": OIDC_CLIENT_ID,
    "client_secret": OIDC_CLIENT_SECRET,
    "scope": OIDC_SCOPE,
    "client": oauth2_httpxclient,
}

print(APIGEE_ENDPOINT, OIDC_ENDPOINT)

# Extract text from PDF and include file name and page number
def extract_text_from_pdf(pdf_path):
    text_with_metadata = []
    with open(pdf_path, "rb") as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text = page.extract_text()
            text_with_metadata.append({
                "file_name": pdf_path,
                "page_number": page_num + 1,
                "text": text,
            })
    return text_with_metadata

# Ask a question about the PDF content
def ask_pdf_question(pdf_metadata, question):
    client = AzureOpenAI(
        api_version=AZURE_ADAI_API_VERSION,
        azure_endpoint=APIGEE_ENDPOINT,
        api_key=AZURE_ADAI_API_KEY,
        http_client=httpx.Client(auth=auth, verify=False),
    )
    with httpx.Client(verify=False) as http_client:
        for entry in pdf_metadata:
            prompt = (
                f"File: {entry['file_name']}, Page: {entry['page_number']}\n\n"
                f"Text:\n{entry['text']}\n\n"
                f"Question: {question}"
            )
            completion = client.chat.completions.create(
                model=AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
                messages=[
                    {"role": "system", "content": "You are an AI assistant."},
                    {"role": "user", "content": prompt},
                ],
            )
            print(f"Response for File: {entry['file_name']}, Page: {entry['page_number']}")
            print(completion.choices[0].message.content)

# Loader for PDF files
loader = PyPDFDirectoryLoader("/mnt/LMMS/Procedures")
docs = loader.load()

# Process loaded PDFs and extract metadata
pdf_metadata = []
for doc in docs:
    pdf_metadata.extend(extract_text_from_pdf(doc))

# Define the question
question = "Quelle est la définition de la donnée personnelle"

# Ask questions
ask_pdf_question(pdf_metadata, question)




Après analyse, nous n'avons pas identifié de justification ou de nécessité claire pour appliquer ce contrôle.


Bonjour [Nom du destinataire],

Je tiens à vous informer que j'ai bien reçu le fichier de manière automatique et qu'il fonctionne correctement. Merci pour votre envoi.



If the name (name_ar) contains the name of a company, such as "سونلغاز" (Sonelgaz) or "بي ان بي باريبا" (BNP Paribas), translate it as the Arabic company name and include it in the output following the same rules above.



pyinstaller --onedir --add-data "Petit_contentieux.py:." --name Petit_contentieux Petit_contentieux.py


cd dist/Petit_contentieux
./Petit_contentieux






pyinstaller --onefile --add-data "Petit_contentieux.py:." --hidden-import streamlit --hidden-import streamlit.components.v1 --hidden-import streamlit.logger Petit_contentieux.py




pyinstaller --onefile --hidden-import=streamlit --add-data "$(python -m site --user-site)/../lib/site-packages/streamlit:streamlit" Petit_contentieux.py



pyinstaller --onedir --add-data "nihad.py:." --hidden-import=streamlit nihad.py




Voici une traduction formelle en français :


---

Cher [Nom du Manager],

Merci pour ce retour détaillé et pour la reconnaissance de mes efforts et contributions tout au long de l’année 2024. J’apprécie sincèrement la reconnaissance de mes compétences techniques, de mon esprit d’initiative, et de ma passion pour l’intelligence artificielle.

Comme vous le savez, j’aspire à devenir data scientist, et je suis enthousiaste à l’idée de travailler sur davantage de projets de data science et d’intelligence artificielle générative afin de renforcer mes compétences et mon expertise. Au cours de l’année écoulée, j’ai mené à bien trois projets de data science, dont AI-DECAD, et assuré la continuité de deux cas d’usage IA (Sentinelia et Identification des Secteurs Sensibles). Ces expériences ont enrichi mes connaissances techniques et renforcé ma capacité à traduire les besoins métiers en solutions opérationnelles.

En me projetant vers l’avenir, je souhaite me concentrer sur des projets qui me permettront de continuer à évoluer dans le domaine de la data science. Mon objectif est de progresser vers un poste de data scientist au sein de l’organisation, et je suis convaincu qu’avec un accompagnement et les bonnes opportunités, je pourrai atteindre cet objectif. De plus, je m’engage à approfondir ma compréhension des concepts métiers, comme cela a été souligné dans l’évaluation, afin d’apporter des contributions encore plus significatives.

Je vous remercie encore pour votre soutien et vos encouragements. Je suis impatient de relever les opportunités à venir et de continuer à contribuer au succès de l’équipe.

Bien cordialement,
Nihad


---

Ce texte conserve le ton professionnel et met en valeur vos aspirations et votre engagement. Qu'en pensez-vous ?









import pandas as pd
import io
import streamlit as st

def save_results_to_files(df_transliterations):
    # Ensure the DataFrame is valid before processing
    if df_transliterations is None or df_transliterations.empty:
        st.warning("The DataFrame is empty. Please provide valid data.")
        return None

    # Display the DataFrame on the Streamlit app
    st.write("Preview of DataFrame:", df_transliterations.head())

    # Create a buffer to use for the Excel writer
    buffer = io.BytesIO()

    # Check if buffer is already in session state (to avoid re-generating)
    if 'buffer' in st.session_state:
        buffer = st.session_state.buffer
    else:
        # Write the DataFrame to an Excel worksheet
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df_transliterations.to_excel(writer, sheet_name="Petit_contentieux", index=False)
        
        # Store the buffer in session state for later use
        st.session_state.buffer = buffer
        buffer.seek(0)  # Ensure buffer is rewound to the beginning

    # Add a download button to download the DataFrame as an Excel file
    st.download_button(
        label="Download data as Excel",
        data=buffer,
        file_name="Petit_contentieux.xlsx",
        mime="application/vnd.ms-excel"
    )

    return "Petit_contentieux.xlsx"




import pandas as pd

# Sample DataFrame with mixed date formats
data = {'Date_Naiss': ['19-03-1983', '1983-03-19', '25-12-2000', '2000-12-25', 'invalid-date']}
df_transliterations = pd.DataFrame(data)

# First parsing attempt with "DD-MM-YYYY"
df_transliterations['Date_Naiss'] = pd.to_datetime(
    df_transliterations['Date_Naiss'], 
    format="%d-%m-%Y", 
    errors="coerce"
)

# Second parsing attempt for rows with NaT using "YYYY-MM-DD"
df_transliterations['Date_Naiss'] = df_transliterations['Date_Naiss'].fillna(
    pd.to_datetime(
        df_transliterations['Date_Naiss'].astype(str),  # Ensure strings for parsing
        format="%Y-%m-%d", 
        errors="coerce"
    )
)

# Display the result
print(df_transliterations)




Objet : Mise à jour du projet, aperçu de l'application web et demande de réunion

Bonjour [Nom du Manager],

Voici les points principaux concernant le projet :

Optimisation du temps d’exécution et amélioration de la précision du modèle.

Mise en place d’une méthode pour crypter les fichiers.

Installation réussie des packages nécessaires pour renforcer la sécurité.

Nouvelles améliorations mises en œuvre nécessitant validation et retours.

Préparation des prochaines étapes en fonction de vos feedbacks.


Vous trouverez ci-joint une capture d’écran de l’application web pour un aperçu des développements réalisés.

Je propose de faire une réunion demain pour discuter des avancées et recueillir vos retours. Merci de me confirmer un créneau qui vous conviendrait.

Cordialement,
Nihad


import pandas as pd

# Sample DataFrame
data = {'Text': ["['Hello World']", "['Python']", "['Data Science']"]}
df = pd.DataFrame(data)

# Remove '[' and ' from the Text column
df['Text'] = df['Text'].str.replace(r"[']", '', regex=True)

print(df)


import pandas as pd
from sentence_transformers import util
import torch

def find_highest_similarity2(df_transliterations, df, threshold, model):
    # Extract unique names for embeddings
    unique_names2 = df["raison_sociale_cleaned"].unique().tolist()
    names1 = df_transliterations["name"].tolist()
    
    # Compute embeddings (only for unique names)
    embeddings1 = model.encode(names1, convert_to_tensor=True)
    embeddings2 = model.encode(unique_names2, convert_to_tensor=True)
    
    # Compute similarity matrix
    similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2)
    
    # Collect similar pairs
    similar_pairs = []
    for i in range(similarity_matrix.shape[0]):  # Loop over transliteration names
        for j in range(similarity_matrix.shape[1]):  # Loop over unique raison sociale names
            if similarity_matrix[i, j] > threshold:
                matched_name = unique_names2[j]
                
                # Retrieve all rows from the original DataFrame with the matched name
                matched_rows = df[df["raison_sociale_cleaned"] == matched_name]
                for _, row in matched_rows.iterrows():
                    similar_pairs.append({
                        "French Traduction": df_transliterations.iloc[i]["name"],
                        "Date Naiss": df_transliterations.iloc[i]["Date_Naiss"],
                        "raison sociale": row["raison_sociale_cleaned"],
                        "Date de naissance": row["Date de naissance"],
                        "Id Tiers": row["Id Tiers"],
                        "Racine": row["Racine"],
                        "similarity_score": similarity_matrix[i, j].item()
                    })

    # Convert results to DataFrame
    result_df = pd.DataFrame(similar_pairs)
    return result_df






import pandas as pd
from sentence_transformers import util
import torch

def find_highest_similarity2(df_transliterations, df, threshold, model):
    # Extract unique names for embeddings
    unique_names2 = df["raison_sociale_cleaned"].unique().tolist()
    names1 = df_transliterations["name"].tolist()
    
    # Compute embeddings (only for unique names)
    embeddings1 = model.encode(names1, convert_to_tensor=True)
    embeddings2 = model.encode(unique_names2, convert_to_tensor=True)
    
    # Compute similarity matrix
    similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2)
    
    # Collect similar pairs
    indices = (similarity_matrix > threshold).nonzero(as_tuple=False)
    similar_pairs = []
    for i, j in indices:
        matched_name = unique_names2[j.item()]
        
        # Retrieve all rows from the original DataFrame with the matched name
        matched_rows = df[df["raison_sociale_cleaned"] == matched_name]
        for _, row in matched_rows.iterrows():
            similar_pairs.append({
                "French Traduction": df_transliterations.iloc[i.item()]["name"],
                "Date Naiss": df_transliterations.iloc[i.item()]["Date_Naiss"],
                "raison sociale": row["raison_sociale_cleaned"],
                "Date de naissance": row["Date de naissance"],
                "Id Tiers": row["Id Tiers"],
                "Racine": row["Racine"],
                "similarity_score": similarity_matrix[i, j].item()
            })

    # Convert results to DataFrame
    result_df = pd.DataFrame(similar_pairs)
    return result_df












import pandas as pd
from sentence_transformers import SentenceTransformer, util

def preprocess_remove_duplicates(df, name_col, dob_col):
    """
    Remove duplicate names but keep all rows to display later.
    Group by the name column and create a combined representation of the rows with different DOBs.
    """
    return df.groupby(name_col).apply(lambda x: x).reset_index(drop=True)

def find_highest_similarity(df_transliterations, df, threshold):
    """
    Find highest similarity between names in two datasets after removing duplicate names.
    """
    # Path to the model
    model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-sentence-transformers/all-MiniLM-L6-v2/main"
    model = SentenceTransformer(model_path)

    # Remove duplicate names from both datasets
    df_transliterations = preprocess_remove_duplicates(df_transliterations, 'name', 'Date_Naiss')
    df = preprocess_remove_duplicates(df, 'raison_sociale_cleaned', 'Date de naissance')

    # Extract names lists for similarity computation
    names1 = df_transliterations['name'].tolist()
    names2 = df['raison_sociale_cleaned'].tolist()

    # Encode names into embeddings
    embeddings1 = model.encode(names1, convert_to_tensor=True)
    embeddings2 = model.encode(names2, convert_to_tensor=True)

    # Compute similarity matrix
    similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2).cpu().numpy()

    # Collect similar pairs
    similar_pairs = []
    for i in range(len(names1)):
        for j in range(len(names2)):
            similarity_score = similarity_matrix[i][j]
            if similarity_score >= threshold:
                similar_pairs.append({
                    'French Traduction': names1[i],
                    'name': df_transliterations.iloc[i]['name'],
                    'name_ar': df_transliterations.iloc[i]['name_ar'],
                    'Date_Naiss': df_transliterations.iloc[i]['Date_Naiss'],
                    'type': df_transliterations.iloc[i]['type'],
                    'raison_sociale_cleaned': names2[j],
                    'Id Tiers': df.iloc[j]['Id Tiers'],
                    'raison sociale': df.iloc[j]['raison sociale'],
                    'Date de création (entreprise)': df.iloc[j]['Date de création (entreprise)'],
                    'Date d\'entree en relation': df.iloc[j]['Date d\'entree en relation'],
                    'Nom abrégé tiers': df.iloc[j]['Nom abrégé tiers'],
                    'Code nature juridique du compte': df.iloc[j]['Code nature juridique du compte'],
                    'Code catégorie client': df.iloc[j]['Code catégorie client'],
                    'Date de naissance': df.iloc[j]['Date de naissance'],
                    'similarity_score': similarity_score
                })

    # Create the result DataFrame
    result_df = pd.DataFrame(similar_pairs)
    return result_df





# Handle cases where "Date de naissance" is None
result_df["Date de naissance Match"] = result_df.apply(
    lambda row: 'No' if pd.isna(row["Date de naissance"]) else (
        'Yes' if row["Date de naissance"] == row["Date_Naiss"] else 'No'
    ), 
    axis=1
)













from transformers import MarianMTModel, MarianTokenizer

# Load the model and tokenizer
model_name = "Helsinki-NLP/opus-mt-fr-ar"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# List of French names to translate
french_names = ["Jean Dupont", "Marie Curie", "Jacques Cartier"]

# Tokenize and translate
translated_names = []
for name in french_names:
    inputs = tokenizer(name, return_tensors="pt", padding=True)
    outputs = model.generate(**inputs)
    translated_name = tokenizer.decode(outputs[0], skip_special_tokens=True)
    translated_names.append(translated_name)

# Display the results
for fr_name, ar_name in zip(french_names, translated_names):
    print(f"French: {fr_name} -> Arabic: {ar_name}")





"Ensure that the transliterations follow the exact order of the input name:\n"
        "The first part of the Arabic name (family name) should appear first in the output.\n"
        "The second part of the Arabic name (first name) should appear second in the output.\n"
        "For example, if the input is 'باش خالد', the output should 8always have the order 'Beche Khaled'.\n"




Bonjour Mohamed,

Vous trouverez ci-joint la deuxième version de la requête GESKYC relative à la "DATE DE DERNIÈRE RÉVISION DE DOSSIER" pour la période du 01/04/2024 au 30/09/2024.

N’hésitez pas à me contacter pour toute question.

Cordialement,
Nihad



import pandas as pd

# Sample data with 'address_1' only related to 'name_1'
data = {
    'address_1': ['A', 'B', None, None],  # Address only for 'name_1'
    'name_1': ['Alice', 'John', 'Emma', 'Sam'], 
    'birth_1': [1990, 1980, 1985, 1992], 
    'name_2': ['Bob', 'Alice', 'James', 'Nina'], 
    'birth_2': [1985, 1990, 1995, 1993]
}

# Create DataFrame
df = pd.DataFrame(data)

# Print the original DataFrame
print("Original DataFrame:")
print(df)

# Melt the dataframe to long format, keeping 'address_1', 'birth_1', and 'birth_2' as id variables
df_melted = pd.melt(df, id_vars=['address_1', 'birth_1', 'birth_2'], 
                    value_vars=['name_1', 'name_2'], 
                    var_name='type', value_name='name')

# Adjust the address_1 column: Assign address_1 only to name_1, otherwise NaN
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if row['type'] == 'name_1' else None, axis=1)

# Adjust the birth column based on the 'type' (name_1 or name_2)
df_melted['birth'] = df_melted.apply(lambda row: row['birth_1'] if row['type'] == 'name_1' else row['birth_2'], axis=1)

# Drop the original birth columns
df_melted = df_melted.drop(columns=['birth_1', 'birth_2'])

# Map 'type' to 1 for 'name_1' and 2 for 'name_2'
df_melted['type'] = df_melted['type'].map({'name_1': 1, 'name_2': 2})

# Print the final result
print("\nTransformed DataFrame:")
print(df_melted)






import pandas as pd

# Sample data with missing 'address_1' for C and D
data = {
    'address_1': ['A', 'B', None, None],  # No address for C and D
    'name_1': ['Alice', 'John', 'Emma', 'Sam'], 
    'birth_1': [1990, 1980, 1985, 1992], 
    'name_2': ['Bob', 'Alice', 'James', 'Nina'], 
    'birth_2': [1985, 1990, 1995, 1993]
}

# Create DataFrame
df = pd.DataFrame(data)

# Print the original DataFrame
print("Original DataFrame:")
print(df)

# Melt the dataframe to long format, keeping 'address_1', 'birth_1', and 'birth_2' as id variables
df_melted = pd.melt(df, id_vars=['address_1', 'birth_1', 'birth_2'], 
                    value_vars=['name_1', 'name_2'], 
                    var_name='type', value_name='name')

# Adjust the birth column based on the 'type' (name_1 or name_2)
df_melted['birth'] = df_melted.apply(lambda row: row['birth_1'] if row['type'] == 'name_1' else row['birth_2'], axis=1)

# Drop the original birth columns
df_melted = df_melted.drop(columns=['birth_1', 'birth_2'])

# Map 'type' to 1 for 'name_1' and 2 for 'name_2'
df_melted['type'] = df_melted['type'].map({'name_1': 1, 'name_2': 2})

# Print the final result
print("\nTransformed DataFrame:")
print(df_melted)





.import pandas as pd

# Example data
data = {
    "name_ar": ["دوايب كريمة", "دويب كريمة", None],
    "French Traduction": ["DOUAIB KARIMA", "DOUAIB KARIME", None],
    "Date_Naiss_AR": ["23/05/1976", "23/05/1976", None],
    "name_fr": ["HADJARI Nadjla", "HADJARI Nadjla", "NEZER Djahida"],
    "Date_Naiss_FR": ["24/06/1973", "24/06/1973", "06/12/1964"],
}
df = pd.DataFrame(data)

# Melt the columns `French Traduction` and `name_fr` into one column
names_melted = df.melt(
    id_vars=["name_ar", "Date_Naiss_AR", "Date_Naiss_FR"],
    value_vars=["French Traduction", "name_fr"],
    var_name="name_type",
    value_name="name"
)

# Melt the columns `Date_Naiss_AR` and `Date_Naiss_FR` into one column
dates_melted = df.melt(
    id_vars=["name_ar", "French Traduction", "name_fr"],
    value_vars=["Date_Naiss_AR", "Date_Naiss_FR"],
    var_name="date_type",
    value_name="birthday"
)

# Merge the melted results to combine `name` and `birthday`
result = names_melted.merge(
    dates_melted, 
    left_index=True, 
    right_index=True
)

# Clean up columns
result = result[["name_ar_x", "name", "birthday"]].rename(columns={"name_ar_x": "name_ar"})

# Adjust `name_ar` to be NaN where the name is from `name_fr`
result.loc[result['name'].isin(df['name_fr']), 'name_ar'] = None

# Drop duplicates caused by the melt process
result = result.drop_duplicates()

# Display the resulting DataFrame
print(result)




import pandas as pd

# Example data
data = {
    "name_ar": ["دوايب كريمة", "دويب كريمة", None],
    "French Traduction": ["DOUAIB KARIMA", "DOUAIB KARIME", None],
    "Date_Naiss_AR": ["23/05/1976", "23/05/1976", None],
    "name_fr": ["HADJARI Nadjla", "HADJARI Nadjla", "NEZER Djahida"],
    "Date_Naiss_FR": ["24/06/1973", "24/06/1973", "06/12/1964"],
}
df = pd.DataFrame(data)

# Create `name` by stacking `French Traduction` and `name_fr`
df['name'] = df['French Traduction'].combine_first(df['name_fr'])

# Create `birthday` by stacking `Date_Naiss_AR` and `Date_Naiss_FR`
df['birthday'] = df['Date_Naiss_AR'].combine_first(df['Date_Naiss_FR'])

# Ensure `name_ar` corresponds only to `French Traduction` rows
df['name_ar'] = df.apply(lambda row: row['name_ar'] if pd.notna(row['French Traduction']) else None, axis=1)

# Drop unnecessary columns
df = df.drop(columns=['French Traduction', 'name_fr', 'Date_Naiss_AR', 'Date_Naiss_FR'])

# Reorder columns (optional)
df = df[['name_ar', 'name', 'birthday']]

# Show the transformed DataFrame
print(df)






import pandas as pd

# Example data
data = {
    "name_ar": ["دوايب كريمة", "دويب كريمة", None],
    "French Traduction": ["DOUAIB KARIMA", "DOUAIB KARIME", None],
    "Date_Naiss_AR": ["23/05/1976", "23/05/1976", None],
    "name_fr": ["HADJARI Nadjla", "HADJARI Nadjla", "NEZER Djahida"],
    "Date_Naiss_FR": ["24/06/1973", "24/06/1973", "06/12/1964"],
}
df = pd.DataFrame(data)

# Create `name` by stacking `French Traduction` and `name_fr`
df['name'] = df['French Traduction'].combine_first(df['name_fr'])

# Create `birthday` by stacking `Date_Naiss_AR` and `Date_Naiss_FR`
df['birthday'] = df['Date_Naiss_AR'].combine_first(df['Date_Naiss_FR'])

# Ensure `name_ar` corresponds only to `French Traduction` rows
df.loc[df['French Traduction'].isna(), 'name_ar'] = None

# Drop unnecessary columns
df = df.drop(columns=['French Traduction', 'name_fr', 'Date_Naiss_AR', 'Date_Naiss_FR'])

# Reorder columns (optional)
df = df[['name_ar', 'name', 'birthday']]

# Show the transformed DataFrame
print(df)




import pandas as pd

# Sample dataset
data = {
    'name1': ['Alice', 'Bob'],
    'birth1': ['1990-01-01', '1985-05-12'],
    'name2': ['Charlie', 'Diana'],
    'birth2': ['1992-07-08', '1987-11-23'],
    'address_1': ['123 Street', '456 Avenue']
}

# Create a DataFrame
df = pd.DataFrame(data)

# Melt the columns name1, birth1 and name2, birth2
name_birth_df = pd.melt(df, 
                        id_vars=['address_1'], 
                        value_vars=['name1', 'name2'], 
                        var_name='type', 
                        value_name='name')

birth_df = pd.melt(df, 
                   id_vars=['address_1'], 
                   value_vars=['birth1', 'birth2'], 
                   var_name='type', 
                   value_name='birth')

# Combine the melted data
name_birth_df['birth'] = birth_df['birth']

# Add a "type" column to distinguish between 1 and 2
name_birth_df['type'] = name_birth_df['type'].str.extract('(\d)')

# Assign NaN to address_1 for rows corresponding to name2 and birth2
name_birth_df.loc[name_birth_df['type'] == '2', 'address_1'] = None

# Rename columns for clarity
name_birth_df = name_birth_df.rename(columns={'address_1': 'address'})

# Reset index
name_birth_df.reset_index(drop=True, inplace=True)

# Display the transformed DataFrame
print(name_birth_df)





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Create a new DataFrame for 'name1' and 'birth1'
df1 = df[['address_1', 'name1', 'birth1']].rename(columns={'name1': 'name', 'birth1': 'birth'})
df1['type'] = 1  # Type for 'name1' and 'birth1'

# Create a new DataFrame for 'name2' and 'birth2'
df2 = df[['name2', 'birth2']].rename(columns={'name2': 'name', 'birth2': 'birth'})
df2['type'] = 2  # Type for 'name2' and 'birth2'

# Add 'address_1' column to df2 with NaN (for rows coming from 'name2' and 'birth2')
df2['address_1'] = pd.NA

# Concatenate the two DataFrames (df1 and df2)
final_df = pd.concat([df1, df2], ignore_index=True)

# Display the final result
print(final_df[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])








import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe, keeping 'address_1' as an id variable
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'name2', 'birth1', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a 'type' column based on the variable names
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the intermediate 'variable' and 'value' columns
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows where 'name2' and 'birth2' are found
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if 'name1' in row['variable'] or 'birth1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'name2', 'birth1', 'birth2'], 
                    var_name='variable', value_name='value')

# Add 'type' column based on the variable names
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Split the melted data into separate columns for name and birth
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they are no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for the 'name2' and 'birth2' rows
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if 'name' in row['variable'] else pd.NA, axis=1)

# Display the result
print(df_melted[['address_1', 'name', 'type', 'birth']])




import pandas as pd

# Sample data with an additional 'group' column
data = {
    'group': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping 'group' as an id variable
df_melted = pd.melt(df, id_vars=['group', 'birth1', 'birth2'], value_vars=['name1', 'name2'], 
                    var_name='type', value_name='name')

# Adjust the birth column based on the type (name1 or name2)
df_melted['birth'] = df_melted.apply(lambda row: row['birth1'] if row['type'] == 'name1' else row['birth2'], axis=1)

# Drop the original birth columns
df_melted = df_melted.drop(columns=['birth1', 'birth2'])

# Map type to 1 for 'name1' and 2 for 'name2'
df_melted['type'] = df_melted['type'].map({'name1': 1, 'name2': 2})

# Display the final result
print(df_melted)




import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z'],
    'other_col_1': [100, 200, 300]
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30],
    'other_col_2': [500, 600, 700]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Melt both DataFrames, turning info_1 and info_2 into a single 'info' column
df1_melted = df1.melt(id_vars=['client_1', 'other_col_1'], value_vars=['info_1'], 
                      var_name='info_type', value_name='info')
df2_melted = df2.melt(id_vars=['client_1', 'other_col_2', 'value'], value_vars=['info_2'], 
                      var_name='info_type', value_name='info')

# Concatenate the two melted DataFrames
result = pd.concat([df1_melted, df2_melted], ignore_index=True)

# Display the result
print(result)




import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z']
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Melt the two DataFrames so that info_1 and info_2 are in the same column
df1_melted = df1.melt(id_vars=['client_1'], value_vars=['info_1'], var_name='info_type', value_name='info')
df2_melted = df2.melt(id_vars=['client_1'], value_vars=['info_2'], var_name='info_type', value_name='info')

# Concatenate the melted DataFrames
result = pd.concat([df1_melted, df2_melted], ignore_index=True)

# Display the result
print(result)





import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z']
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Concatenate the two DataFrames
result = pd.concat([df1, df2], ignore_index=True)

# Display the result
print(result)




import pandas as pd
import io
import streamlit as st

def save_results_to_files(df_transliterations):
    # Create a buffer to use for Excel writer
    buffer = io.BytesIO()
    
    # Ensure the DataFrame is valid before processing
    if df_transliterations is None or df_transliterations.empty:
        st.warning("The DataFrame is empty. Please provide valid data.")
        return None
    
    # Display the DataFrame on the Streamlit app
    st.write("Preview of DataFrame:", df_transliterations.head())

    # Write the DataFrame to an Excel worksheet
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df_transliterations.to_excel(writer, sheet_name='Petit_contentieux', index=False)

    # Rewind the buffer to prepare it for download
    buffer.seek(0)

    # Add a download button to download the DataFrame as an Excel file
    st.download_button(
        label="Download data as Excel",
        data=buffer,
        file_name='Petit_contentieux.xlsx',
        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )

    return 'Petit_contentieux.xlsx'






def get_name_transliterations(name_ar, num_variations=10):
    try:
        prompt = (
            f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. "
            "I want only the translations without any additional text, just the translated names. "
            "Don't list the translated names, just separate them with a semi-colon. "
            "Remove any number from the output, even the number 1. "
            "Make sure the variations are realistic."
        )
        
        # OpenAI API call (update with Azure OpenAI setup if needed)
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # Replace with the correct model or Azure deployment name
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=400,
            temperature=0,
        )
        
        # Extract content safely
        content = response.choices[0].message.content if response.choices else None
        
        if content and content.strip():  # Check if content is valid
            return [variation.strip() for variation in content.split(';') if variation.strip()]
        else:
            return []  # Return an empty list if no valid content
    except Exception as e:
        print(f"Error in API call: {e}")
        return []





Pour finaliser le projet, je propose une réunion la semaine prochaine



def get_name_transliterations(name_ar, num_variations=10):
    prompt = (
        f"Generate {num_variations} French transliterations of the Arabic name {name_ar}. "
        "I want only the translations without any additional text, just separated by semicolons. "
        "Remove any number from the output, even number 1. "
        "Ensure the variations are realistic."
    )
    
    for i in range(1, 4):  # Retry logic
        try:
            # Replace with actual API setup
            client = AzureOpenAI(
                api_version="AZURE_AOAI_API_VERSION",
                azure_endpoint="APIGEE_ENDPOINT",
                api_key="FAKE_KEY",
            )
            response = client.chat.completions.create(
                model="AZURE_AOAI_MODEL_DEPLOYMENT_NAME",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=4000,
                temperature=0.7,
            )
            # Debugging: Print the response to verify
            print("API Response:", response)
            
            # Check if the response has the expected structure
            content = response.choices[0].message.content if response.choices and response.choices[0].message.content else None
            
            if content:
                variations = content.split(';')
                return [variation.strip() for variation in variations if variation.strip()]
            else:
                print(f"Warning: Empty content in response for name: {name_ar}")
                return []  # Return an empty list if content is None
            
        except Exception as e:
            print(f"Retry {i}: Error occurred - {e}")
            continue
    
    return []  # Return an empty list if all retries fail




import datetime
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

# Define the function to get name transliterations
def get_name_transliterations(name_ar, num_variations=10):
    prompt = (
        f"Generate {num_variations} French transliterations of the Arabic name {name_ar}. "
        "I want only the translations without any additional text, just separated by semicolons. "
        "Remove any number from the output, even number 1. "
        "Ensure the variations are realistic."
    )
    
    for i in range(1, 4):  # Retry logic
        try:
            # Replace with actual API setup
            client = AzureOpenAI(
                api_version="AZURE_AOAI_API_VERSION",
                azure_endpoint="APIGEE_ENDPOINT",
                api_key="FAKE_KEY",
            )
            response = client.chat.completions.create(
                model="AZURE_AOAI_MODEL_DEPLOYMENT_NAME",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=4000,
                temperature=0.7,
            )
            variations = response.choices[0].message.content.split(';')
            return [variation.strip() for variation in variations if variation.strip()]
        except Exception as e:
            print(f"Retry {i}: Error occurred - {e}")
            continue
    return []  # Return an empty list if all retries fail

# Batch processing function
def process_batch(batch_df):
    transliterations = []
    for index, row in batch_df.iterrows():
        name = row['name_ar']
        variations = get_name_transliterations(name)
        transliterations.append({"name_ar": name, "French Transliterations": variations})
    return transliterations

# Main script
if __name__ == "__main__":
    print("Start Time:", datetime.datetime.now())
    
    # Load data
    batch_size = 5
    Train_data = Train_data.dropna(subset=['name_ar'])  # Ensure no NaN values
    total_names = len(Train_data)
    
    # Split data into batches
    batches = [Train_data[i:i + batch_size] for i in range(0, total_names, batch_size)]
    
    # Use ThreadPoolExecutor for parallel processing
    all_transliterations = []
    with ThreadPoolExecutor() as executor:
        results = executor.map(process_batch, batches)
        for batch_result in results:
            all_transliterations.extend(batch_result)
    
    # Save the results to an Excel file
    df_transliterations = pd.DataFrame(all_transliterations)
    df_transliterations.to_excel('name_transliterationsV8.xlsx', index=False)
    
    print("End Time:", datetime.datetime.now())







import datetime
import pandas as pd

# Log the start time
print(datetime.datetime.now())

# Assuming Train_data is your DataFrame with 100 Arabic names
batch_size = 5
total_names = len(Train_data)

for i in range(0, total_names, batch_size):
    # Get the current batch
    df = Train_data[i:i + batch_size]
    
    # Drop rows with missing Arabic names
    df = df.dropna(subset=['name_ar'])
    
    # Generate and display transliterations
    transliterations = []
    for index, row in df.iterrows():
        name = row['name_ar']
        variations = get_name_transliterations(name)  # Assuming this function is defined elsewhere
        transliterations.append({"name_ar": name, "French Transliterations": variations})
    
    # Save the transliterations to an Excel file
    df_transliterations = pd.DataFrame(transliterations)
    excel_file_name = f'name_transliterations_batch_{i//batch_size + 1}.xlsx'
    df_transliterations.to_excel(excel_file_name, index=False)

# Log the end time
print(datetime.datetime.now())





import pandas as pd
import datetime

# Function to simulate getting transliterations for a batch of names
def get_name_transliterations_batch(names):
    """
    Generate transliterations for a batch of names.
    Simulate the process with your actual implementation for Azure OpenAI.
    """
    # For simplicity, just append mock transliterations for now
    return [f"Transliteration {i}" for i in range(1, len(names) + 1)]

# Load and clean data
print(datetime.datetime.now())
df = Train_data.dropna(subset=['name_ar'])

# Initialize results list
transliterations = []

# Process in batches of 5
batch_size = 5
for i in range(0, len(df), batch_size):
    batch = df['name_ar'].iloc[i:i + batch_size].tolist()  # Get a batch of 5 names
    batch_transliterations = get_name_transliterations_batch(batch)  # Call the API for the batch

    # Append results for the batch
    for name, variations in zip(batch, batch_transliterations):
        transliterations.append({"name_ar": name, "French Transliterations": variations})

# Save results to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterationsV8_batch5.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

print(datetime.datetime.now())




import pandas as pd
import datetime

# Function to simulate getting transliterations for a batch of names
def get_name_transliterations_batch(names):
    """
    Generate transliterations for a batch of names.
    Simulate the process with your actual implementation for Azure OpenAI.
    """
    # For simplicity, just append mock transliterations for now
    return [f"Transliteration {i}" for i in range(1, len(names) + 1)]

# Load and clean data
print(datetime.datetime.now())
df = Train_data.dropna(subset=['name_ar'])

# Initialize results list
transliterations = []

# Process in batches of 5
batch_size = 5
for i in range(0, len(df), batch_size):
    batch = df['name_ar'].iloc[i:i + batch_size].tolist()  # Get a batch of 5 names
    batch_transliterations = get_name_transliterations_batch(batch)  # Call the API for the batch

    # Append results for the batch
    for name, variations in zip(batch, batch_transliterations):
        transliterations.append({"name_ar": name, "French Transliterations": variations})

# Save results to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterationsV8_batch5.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

print(datetime.datetime.now())




variations = response.choices[0].message.content.split('\n') formatted_variations = ' '.join([f"{i+1}. {variation.strip()}" for i, variation in enumerate(variations) if variation.strip()]) return formatted_variations




import pandas as pd
import openai
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Function to get French transliterations of Arabic names
def get_name_transliterations(name_ar, num_variations=5):
    prompt = f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. Make sure the variations are realistic."
    response = client.chat.completions.create(
        model='your-model-deployment-name',  # Replace with your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7
    )
    variations = response.choices[0].message.content.split('\n')
    formatted_variations = '\n'.join([f"{i+1}. {variation.strip()}" for i, variation in enumerate(variations)])
    return formatted_variations

# Load the CSV file
csv_file_name = 'arabic_names.csv'
df = pd.read_csv(csv_file_name)

# Generate and display transliterations
transliterations = []
for index, row in df.iterrows():
    name = row['Arabic Name']
    variations = get_name_transliterations(name)
    transliterations.append({"Arabic Name": name, "French Transliterations": variations})
    print(f"Arabic Name: {name} -> French Transliterations: \n{variations}\n")

# Save the transliterations to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterations.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

# Upload the Excel file to Azure OpenAI
try:
    with open(excel_file_name, "rb") as file:
        training_response = client.files.create(file=file, purpose="fine-tune")
    training_file_id = training_response.id
    print("Training file ID:", training_file_id)
except Exception as e:
    print(f"Error uploading training file: {e}")


















































import pandas as pd
import openai
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Function to get French transliterations of Arabic names
def get_name_transliterations(name_ar, num_variations=5):
    prompt = f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. Make sure the variations are realistic."
    response = client.chat.completions.create(
        model='your-model-deployment-name',  # Replace with your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7
    )
    return response.choices[0].message.content

# Load the CSV file
csv_file_name = 'arabic_names.csv'
df = pd.read_csv(csv_file_name)

# Generate and display transliterations
transliterations = []
for index, row in df.iterrows():
    name = row['Arabic Name']
    variations = get_name_transliterations(name)
    transliterations.append({"Arabic Name": name, "French Transliterations": variations})
    print(f"Arabic Name: {name} -> French Transliterations: \n{variations}\n")

# Save the transliterations to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterations.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

# Upload the Excel file to Azure OpenAI
try:
    with open(excel_file_name, "rb") as file:
        training_response = client.files.create(file=file, purpose="fine-tune")
    training_file_id = training_response.id
    print("Training file ID:", training_file_id)
except Exception as e:
    print(f"Error uploading training file: {e}")






# Upload the training dataset file to Azure OpenAI try: with open(training_file_name, "rb") as file: training_response = client.files.create(file=file, purpose="fine-tune") training_file_id = training_response.id print("Training file ID:", training_file_id) except Exception as e: print(f"Error uploading training file: {e}")



import pandas as pd
import openai
import json
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Create a DataFrame with Arabic names and their French translations
data = {
    'Arabic Name': ['محمد علي', 'فاطمة الزهراء', 'يوسف بن تاشفين'],
    'French Name': ['Mohamed Ali', 'Fatima Zahra', 'Youssef Ben Tachfine']
}
df = pd.DataFrame(data)

# Convert the DataFrame to JSONL format
training_data = []
for index, row in df.iterrows():
    training_data.append({
        "prompt": f"Translate the following Arabic name to French: '{row['Arabic Name']}'",
        "completion": row['French Name']
    })

# Save the training data to a JSONL file
training_file_name = 'training_data.jsonl'
with open(training_file_name, 'w') as f:
    for entry in training_data:
        f.write(json.dumps(entry) + '\n')

# Upload the training dataset file to Azure OpenAI
training_response = client.files.create(
    file=open(training_file_name, "rb"), purpose="fine-tune"
)
training_file_id = training_response.id

# Print the training file ID
print("Training file ID:", training_file_id)

# Define the new Arabic name and family name
arabic_name = "علي بن أبي طالب"

# Create the prompt for translation
prompt = f"Translate the following Arabic name to French: '{arabic_name}'"

# Make a request to the fine-tuned model
response = openai.Completion.create(
    model='chatgpt-turbo',
    prompt=prompt,
    max_tokens=50
)

# Extract and print the translation
translation = response.choices[0].text.strip()
print(f"Translation: {translation}")




Fine-tuning a model involves taking a pre-trained model and training it further on a specific dataset to adapt it to a particular task or domain. This process helps the model learn the nuances and specifics of the new data, improving its performance on the desired task. Fine-tuning is often used to customize models for specialized applications without needing to train a model from scratch.




import pandas as pd
import openai
import json

# Set your OpenAI API key
openai.api_key = 'your-api-key'

# Create a DataFrame with Arabic names and their French translations
data = {
    'Arabic Name': ['محمد علي', 'فاطمة الزهراء', 'يوسف بن تاشفين'],
    'French Name': ['Mohamed Ali', 'Fatima Zahra', 'Youssef Ben Tachfine']
}
df = pd.DataFrame(data)

# Convert the DataFrame to JSONL format
training_data = []
for index, row in df.iterrows():
    training_data.append({
        "prompt": f"Translate the following Arabic name to French: '{row['Arabic Name']}'",
        "completion": row['French Name']
    })

# Save the training data to a JSONL file
with open('training_data.jsonl', 'w') as f:
    for entry in training_data:
        f.write(json.dumps(entry) + '\n')

# Fine-tune the model
response = openai.FineTune.create(
    training_file=openai.File.create(file=open('training_data.jsonl'), purpose='fine-tune').id,
    model='chatgpt-turbo'
)

# Print the fine-tuning job ID
print(f"Fine-tuning job ID: {response['id']}")

# Define the new Arabic name and family name
arabic_name = "علي بن أبي طالب"

# Create the prompt for translation
prompt = f"Translate the following Arabic name to French: '{arabic_name}'"

# Make a request to the fine-tuned model
response = openai.Completion.create(
    model='chatgpt-turbo',
    prompt=prompt,
    max_tokens=50
)

# Extract and print the translation
translation = response.choices[0].text.strip()
print(f"Translation: {translation}")import pandas as pd

# Sample dataset
data = {
    'name1': ['Alice', 'Bob'],
    'birth1': ['1990-01-01', '1985-05-12'],
    'name2': ['Charlie', 'Diana'],
    'birth2': ['1992-07-08', '1987-11-23'],
    'address_1': ['123 Street', '456 Avenue']
}

# Create a DataFrame
df = pd.DataFrame(data)

# Melt the columns name1, birth1 and name2, birth2
name_birth_df = pd.melt(df, 
                        id_vars=['address_1'], 
                        value_vars=['name1', 'name2'], 
                        var_name='type', 
                        value_name='name')

birth_df = pd.melt(df, 
                   id_vars=['address_1'], 
                   value_vars=['birth1', 'birth2'], 
                   var_name='type', 
                   value_name='birth')

# Combine the melted data
name_birth_df['birth'] = birth_df['birth']

# Add a "type" column to distinguish between 1 and 2
name_birth_df['type'] = name_birth_df['type'].str.extract('(\d)')

# Assign NaN to address_1 for rows corresponding to name2 and birth2
name_birth_df.loc[name_birth_df['type'] == '2', 'address_1'] = None

# Rename columns for clarity
name_birth_df = name_birth_df.rename(columns={'address_1': 'address'})

# Reset index
name_birth_df.reset_index(drop=True, inplace=True)

# Display the transformed DataFrame
print(name_birth_df)





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Create a new DataFrame for 'name1' and 'birth1'
df1 = df[['address_1', 'name1', 'birth1']].rename(columns={'name1': 'name', 'birth1': 'birth'})
df1['type'] = 1  # Type for 'name1' and 'birth1'

# Create a new DataFrame for 'name2' and 'birth2'
df2 = df[['name2', 'birth2']].rename(columns={'name2': 'name', 'birth2': 'birth'})
df2['type'] = 2  # Type for 'name2' and 'birth2'

# Add 'address_1' column to df2 with NaN (for rows coming from 'name2' and 'birth2')
df2['address_1'] = pd.NA

# Concatenate the two DataFrames (df1 and df2)
final_df = pd.concat([df1, df2], ignore_index=True)

# Display the final result
print(final_df[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])








import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe, keeping 'address_1' as an id variable
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'name2', 'birth1', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a 'type' column based on the variable names
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the intermediate 'variable' and 'value' columns
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows where 'name2' and 'birth2' are found
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if 'name1' in row['variable'] or 'birth1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'name2', 'birth1', 'birth2'], 
                    var_name='variable', value_name='value')

# Add 'type' column based on the variable names
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Split the melted data into separate columns for name and birth
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they are no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for the 'name2' and 'birth2' rows
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if 'name' in row['variable'] else pd.NA, axis=1)

# Display the result
print(df_melted[['address_1', 'name', 'type', 'birth']])




import pandas as pd

# Sample data with an additional 'group' column
data = {
    'group': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping 'group' as an id variable
df_melted = pd.melt(df, id_vars=['group', 'birth1', 'birth2'], value_vars=['name1', 'name2'], 
                    var_name='type', value_name='name')

# Adjust the birth column based on the type (name1 or name2)
df_melted['birth'] = df_melted.apply(lambda row: row['birth1'] if row['type'] == 'name1' else row['birth2'], axis=1)

# Drop the original birth columns
df_melted = df_melted.drop(columns=['birth1', 'birth2'])

# Map type to 1 for 'name1' and 2 for 'name2'
df_melted['type'] = df_melted['type'].map({'name1': 1, 'name2': 2})

# Display the final result
print(df_melted)




import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z'],
    'other_col_1': [100, 200, 300]
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30],
    'other_col_2': [500, 600, 700]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Melt both DataFrames, turning info_1 and info_2 into a single 'info' column
df1_melted = df1.melt(id_vars=['client_1', 'other_col_1'], value_vars=['info_1'], 
                      var_name='info_type', value_name='info')
df2_melted = df2.melt(id_vars=['client_1', 'other_col_2', 'value'], value_vars=['info_2'], 
                      var_name='info_type', value_name='info')

# Concatenate the two melted DataFrames
result = pd.concat([df1_melted, df2_melted], ignore_index=True)

# Display the result
print(result)




import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z']
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Melt the two DataFrames so that info_1 and info_2 are in the same column
df1_melted = df1.melt(id_vars=['client_1'], value_vars=['info_1'], var_name='info_type', value_name='info')
df2_melted = df2.melt(id_vars=['client_1'], value_vars=['info_2'], var_name='info_type', value_name='info')

# Concatenate the melted DataFrames
result = pd.concat([df1_melted, df2_melted], ignore_index=True)

# Display the result
print(result)





import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z']
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Concatenate the two DataFrames
result = pd.concat([df1, df2], ignore_index=True)

# Display the result
print(result)




import pandas as pd
import io
import streamlit as st

def save_results_to_files(df_transliterations):
    # Create a buffer to use for Excel writer
    buffer = io.BytesIO()
    
    # Ensure the DataFrame is valid before processing
    if df_transliterations is None or df_transliterations.empty:
        st.warning("The DataFrame is empty. Please provide valid data.")
        return None
    
    # Display the DataFrame on the Streamlit app
    st.write("Preview of DataFrame:", df_transliterations.head())

    # Write the DataFrame to an Excel worksheet
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df_transliterations.to_excel(writer, sheet_name='Petit_contentieux', index=False)

    # Rewind the buffer to prepare it for download
    buffer.seek(0)

    # Add a download button to download the DataFrame as an Excel file
    st.download_button(
        label="Download data as Excel",
        data=buffer,
        file_name='Petit_contentieux.xlsx',
        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )

    return 'Petit_contentieux.xlsx'






def get_name_transliterations(name_ar, num_variations=10):
    try:
        prompt = (
            f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. "
            "I want only the translations without any additional text, just the translated names. "
            "Don't list the translated names, just separate them with a semi-colon. "
            "Remove any number from the output, even the number 1. "
            "Make sure the variations are realistic."
        )
        
        # OpenAI API call (update with Azure OpenAI setup if needed)
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # Replace with the correct model or Azure deployment name
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=400,
            temperature=0,
        )
        
        # Extract content safely
        content = response.choices[0].message.content if response.choices else None
        
        if content and content.strip():  # Check if content is valid
            return [variation.strip() for variation in content.split(';') if variation.strip()]
        else:
            return []  # Return an empty list if no valid content
    except Exception as e:
        print(f"Error in API call: {e}")
        return []





Pour finaliser le projet, je propose une réunion la semaine prochaine



def get_name_transliterations(name_ar, num_variations=10):
    prompt = (
        f"Generate {num_variations} French transliterations of the Arabic name {name_ar}. "
        "I want only the translations without any additional text, just separated by semicolons. "
        "Remove any number from the output, even number 1. "
        "Ensure the variations are realistic."
    )
    
    for i in range(1, 4):  # Retry logic
        try:
            # Replace with actual API setup
            client = AzureOpenAI(
                api_version="AZURE_AOAI_API_VERSION",
                azure_endpoint="APIGEE_ENDPOINT",
                api_key="FAKE_KEY",
            )
            response = client.chat.completions.create(
                model="AZURE_AOAI_MODEL_DEPLOYMENT_NAME",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=4000,
                temperature=0.7,
            )
            # Debugging: Print the response to verify
            print("API Response:", response)
            
            # Check if the response has the expected structure
            content = response.choices[0].message.content if response.choices and response.choices[0].message.content else None
            
            if content:
                variations = content.split(';')
                return [variation.strip() for variation in variations if variation.strip()]
            else:
                print(f"Warning: Empty content in response for name: {name_ar}")
                return []  # Return an empty list if content is None
            
        except Exception as e:
            print(f"Retry {i}: Error occurred - {e}")
            continue
    
    return []  # Return an empty list if all retries fail




import datetime
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

# Define the function to get name transliterations
def get_name_transliterations(name_ar, num_variations=10):
    prompt = (
        f"Generate {num_variations} French transliterations of the Arabic name {name_ar}. "
        "I want only the translations without any additional text, just separated by semicolons. "
        "Remove any number from the output, even number 1. "
        "Ensure the variations are realistic."
    )
    
    for i in range(1, 4):  # Retry logic
        try:
            # Replace with actual API setup
            client = AzureOpenAI(
                api_version="AZURE_AOAI_API_VERSION",
                azure_endpoint="APIGEE_ENDPOINT",
                api_key="FAKE_KEY",
            )
            response = client.chat.completions.create(
                model="AZURE_AOAI_MODEL_DEPLOYMENT_NAME",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=4000,
                temperature=0.7,
            )
            variations = response.choices[0].message.content.split(';')
            return [variation.strip() for variation in variations if variation.strip()]
        except Exception as e:
            print(f"Retry {i}: Error occurred - {e}")
            continue
    return []  # Return an empty list if all retries fail

# Batch processing function
def process_batch(batch_df):
    transliterations = []
    for index, row in batch_df.iterrows():
        name = row['name_ar']
        variations = get_name_transliterations(name)
        transliterations.append({"name_ar": name, "French Transliterations": variations})
    return transliterations

# Main script
if __name__ == "__main__":
    print("Start Time:", datetime.datetime.now())
    
    # Load data
    batch_size = 5
    Train_data = Train_data.dropna(subset=['name_ar'])  # Ensure no NaN values
    total_names = len(Train_data)
    
    # Split data into batches
    batches = [Train_data[i:i + batch_size] for i in range(0, total_names, batch_size)]
    
    # Use ThreadPoolExecutor for parallel processing
    all_transliterations = []
    with ThreadPoolExecutor() as executor:
        results = executor.map(process_batch, batches)
        for batch_result in results:
            all_transliterations.extend(batch_result)
    
    # Save the results to an Excel file
    df_transliterations = pd.DataFrame(all_transliterations)
    df_transliterations.to_excel('name_transliterationsV8.xlsx', index=False)
    
    print("End Time:", datetime.datetime.now())







import datetime
import pandas as pd

# Log the start time
print(datetime.datetime.now())

# Assuming Train_data is your DataFrame with 100 Arabic names
batch_size = 5
total_names = len(Train_data)

for i in range(0, total_names, batch_size):
    # Get the current batch
    df = Train_data[i:i + batch_size]
    
    # Drop rows with missing Arabic names
    df = df.dropna(subset=['name_ar'])
    
    # Generate and display transliterations
    transliterations = []
    for index, row in df.iterrows():
        name = row['name_ar']
        variations = get_name_transliterations(name)  # Assuming this function is defined elsewhere
        transliterations.append({"name_ar": name, "French Transliterations": variations})
    
    # Save the transliterations to an Excel file
    df_transliterations = pd.DataFrame(transliterations)
    excel_file_name = f'name_transliterations_batch_{i//batch_size + 1}.xlsx'
    df_transliterations.to_excel(excel_file_name, index=False)

# Log the end time
print(datetime.datetime.now())





import pandas as pd
import datetime

# Function to simulate getting transliterations for a batch of names
def get_name_transliterations_batch(names):
    """
    Generate transliterations for a batch of names.
    Simulate the process with your actual implementation for Azure OpenAI.
    """
    # For simplicity, just append mock transliterations for now
    return [f"Transliteration {i}" for i in range(1, len(names) + 1)]

# Load and clean data
print(datetime.datetime.now())
df = Train_data.dropna(subset=['name_ar'])

# Initialize results list
transliterations = []

# Process in batches of 5
batch_size = 5
for i in range(0, len(df), batch_size):
    batch = df['name_ar'].iloc[i:i + batch_size].tolist()  # Get a batch of 5 names
    batch_transliterations = get_name_transliterations_batch(batch)  # Call the API for the batch

    # Append results for the batch
    for name, variations in zip(batch, batch_transliterations):
        transliterations.append({"name_ar": name, "French Transliterations": variations})

# Save results to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterationsV8_batch5.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

print(datetime.datetime.now())




import pandas as pd
import datetime

# Function to simulate getting transliterations for a batch of names
def get_name_transliterations_batch(names):
    """
    Generate transliterations for a batch of names.
    Simulate the process with your actual implementation for Azure OpenAI.
    """
    # For simplicity, just append mock transliterations for now
    return [f"Transliteration {i}" for i in range(1, len(names) + 1)]

# Load and clean data
print(datetime.datetime.now())
df = Train_data.dropna(subset=['name_ar'])

# Initialize results list
transliterations = []

# Process in batches of 5
batch_size = 5
for i in range(0, len(df), batch_size):
    batch = df['name_ar'].iloc[i:i + batch_size].tolist()  # Get a batch of 5 names
    batch_transliterations = get_name_transliterations_batch(batch)  # Call the API for the batch

    # Append results for the batch
    for name, variations in zip(batch, batch_transliterations):
        transliterations.append({"name_ar": name, "French Transliterations": variations})

# Save results to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterationsV8_batch5.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

print(datetime.datetime.now())




variations = response.choices[0].message.content.split('\n') formatted_variations = ' '.join([f"{i+1}. {variation.strip()}" for i, variation in enumerate(variations) if variation.strip()]) return formatted_variations




import pandas as pd
import openai
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Function to get French transliterations of Arabic names
def get_name_transliterations(name_ar, num_variations=5):
    prompt = f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. Make sure the variations are realistic."
    response = client.chat.completions.create(
        model='your-model-deployment-name',  # Replace with your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7
    )
    variations = response.choices[0].message.content.split('\n')
    formatted_variations = '\n'.join([f"{i+1}. {variation.strip()}" for i, variation in enumerate(variations)])
    return formatted_variations

# Load the CSV file
csv_file_name = 'arabic_names.csv'
df = pd.read_csv(csv_file_name)

# Generate and display transliterations
transliterations = []
for index, row in df.iterrows():
    name = row['Arabic Name']
    variations = get_name_transliterations(name)
    transliterations.append({"Arabic Name": name, "French Transliterations": variations})
    print(f"Arabic Name: {name} -> French Transliterations: \n{variations}\n")

# Save the transliterations to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterations.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

# Upload the Excel file to Azure OpenAI
try:
    with open(excel_file_name, "rb") as file:
        training_response = client.files.create(file=file, purpose="fine-tune")
    training_file_id = training_response.id
    print("Training file ID:", training_file_id)
except Exception as e:
    print(f"Error uploading training file: {e}")


















































import pandas as pd
import openai
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Function to get French transliterations of Arabic names
def get_name_transliterations(name_ar, num_variations=5):
    prompt = f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. Make sure the variations are realistic."
    response = client.chat.completions.create(
        model='your-model-deployment-name',  # Replace with your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7
    )
    return response.choices[0].message.content

# Load the CSV file
csv_file_name = 'arabic_names.csv'
df = pd.read_csv(csv_file_name)

# Generate and display transliterations
transliterations = []
for index, row in df.iterrows():
    name = row['Arabic Name']
    variations = get_name_transliterations(name)
    transliterations.append({"Arabic Name": name, "French Transliterations": variations})
    print(f"Arabic Name: {name} -> French Transliterations: \n{variations}\n")

# Save the transliterations to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterations.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

# Upload the Excel file to Azure OpenAI
try:
    with open(excel_file_name, "rb") as file:
        training_response = client.files.create(file=file, purpose="fine-tune")
    training_file_id = training_response.id
    print("Training file ID:", training_file_id)
except Exception as e:
    print(f"Error uploading training file: {e}")






# Upload the training dataset file to Azure OpenAI try: with open(training_file_name, "rb") as file: training_response = client.files.create(file=file, purpose="fine-tune") training_file_id = training_response.id print("Training file ID:", training_file_id) except Exception as e: print(f"Error uploading training file: {e}")



import pandas as pd
import openai
import json
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Create a DataFrame with Arabic names and their French translations
data = {
    'Arabic Name': ['محمد علي', 'فاطمة الزهراء', 'يوسف بن تاشفين'],
    'French Name': ['Mohamed Ali', 'Fatima Zahra', 'Youssef Ben Tachfine']
}
df = pd.DataFrame(data)

# Convert the DataFrame to JSONL format
training_data = []
for index, row in df.iterrows():
    training_data.append({
        "prompt": f"Translate the following Arabic name to French: '{row['Arabic Name']}'",
        "completion": row['French Name']
    })

# Save the training data to a JSONL file
training_file_name = 'training_data.jsonl'
with open(training_file_name, 'w') as f:
    for entry in training_data:
        f.write(json.dumps(entry) + '\n')

# Upload the training dataset file to Azure OpenAI
training_response = client.files.create(
    file=open(training_file_name, "rb"), purpose="fine-tune"
)
training_file_id = training_response.id

# Print the training file ID
print("Training file ID:", training_file_id)

# Define the new Arabic name and family name
arabic_name = "علي بن أبي طالب"

# Create the prompt for translation
prompt = f"Translate the following Arabic name to French: '{arabic_name}'"

# Make a request to the fine-tuned model
response = openai.Completion.create(
    model='chatgpt-turbo',
    prompt=prompt,
    max_tokens=50
)

# Extract and print the translation
translation = response.choices[0].text.strip()
print(f"Translation: {translation}")




Fine-tuning a model involves taking a pre-trained model and training it further on a specific dataset to adapt it to a particular task or domain. This process helps the model learn the nuances and specifics of the new data, improving its performance on the desired task. Fine-tuning is often used to customize models for specialized applications without needing to train a model from scratch.




import pandas as pd
import openai
import json

# Set your OpenAI API key
openai.api_key = 'your-api-key'

# Create a DataFrame with Arabic names and their French translations
data = {
    'Arabic Name': ['محمد علي', 'فاطمة الزهراء', 'يوسف بن تاشفين'],
    'French Name': ['Mohamed Ali', 'Fatima Zahra', 'Youssef Ben Tachfine']
}
df = pd.DataFrame(data)

# Convert the DataFrame to JSONL format
training_data = []
for index, row in df.iterrows():
    training_data.append({
        "prompt": f"Translate the following Arabic name to French: '{row['Arabic Name']}'",
        "completion": row['French Name']
    })

# Save the training data to a JSONL file
with open('training_data.jsonl', 'w') as f:
    for entry in training_data:
        f.write(json.dumps(entry) + '\n')

# Fine-tune the model
response = openai.FineTune.create(
    training_file=openai.File.create(file=open('training_data.jsonl'), purpose='fine-tune').id,
    model='chatgpt-turbo'
)

# Print the fine-tuning job ID
print(f"Fine-tuning job ID: {response['id']}")

# Define the new Arabic name and family name
arabic_name = "علي بن أبي طالب"

# Create the prompt for translation
prompt = f"Translate the following Arabic name to French: '{arabic_name}'"

# Make a request to the fine-tuned model
response = openai.Completion.create(
    model='chatgpt-turbo',
    prompt=prompt,
    max_tokens=50
)

# Extract and print the translation
translation = response.choices[0].text.strip()
print(f"Translation: {translation}")

import pandas as pd

# Sample dataset
data = {
    'name1': ['Alice', 'Bob'],
    'birth1': ['1990-01-01', '1985-05-12'],
    'name2': ['Charlie', 'Diana'],
    'birth2': ['1992-07-08', '1987-11-23'],
    'address_1': ['123 Street', '456 Avenue']
}

# Create a DataFrame
df = pd.DataFrame(data)

# Melt the columns name1, birth1 and name2, birth2
name_birth_df = pd.melt(df, 
                        id_vars=['address_1'], 
                        value_vars=['name1', 'name2'], 
                        var_name='type', 
                        value_name='name')

birth_df = pd.melt(df, 
                   id_vars=['address_1'], 
                   value_vars=['birth1', 'birth2'], 
                   var_name='type', 
                   value_name='birth')

# Combine the melted data
name_birth_df['birth'] = birth_df['birth']

# Add a "type" column to distinguish between 1 and 2
name_birth_df['type'] = name_birth_df['type'].str.extract('(\d)')

# Assign NaN to address_1 for rows corresponding to name2 and birth2
name_birth_df.loc[name_birth_df['type'] == '2', 'address_1'] = None

# Rename columns for clarity
name_birth_df = name_birth_df.rename(columns={'address_1': 'address'})

# Reset index
name_birth_df.reset_index(drop=True, inplace=True)

# Display the transformed DataFrame
print(name_birth_df)





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Create a new DataFrame for 'name1' and 'birth1'
df1 = df[['address_1', 'name1', 'birth1']].rename(columns={'name1': 'name', 'birth1': 'birth'})
df1['type'] = 1  # Type for 'name1' and 'birth1'

# Create a new DataFrame for 'name2' and 'birth2'
df2 = df[['name2', 'birth2']].rename(columns={'name2': 'name', 'birth2': 'birth'})
df2['type'] = 2  # Type for 'name2' and 'birth2'

# Add 'address_1' column to df2 with NaN (for rows coming from 'name2' and 'birth2')
df2['address_1'] = pd.NA

# Concatenate the two DataFrames (df1 and df2)
final_df = pd.concat([df1, df2], ignore_index=True)

# Display the final result
print(final_df[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])








import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping address_1
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'birth1', 'name2', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a new 'type' column based on whether the variable is a name or birth
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth based on the 'variable' column
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they're no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows with 'name2' and 'birth2'
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if '1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe, keeping 'address_1' as an id variable
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'name2', 'birth1', 'birth2'], 
                    var_name='variable', value_name='value')

# Create a 'type' column based on the variable names
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Create separate columns for name and birth
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the intermediate 'variable' and 'value' columns
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for rows where 'name2' and 'birth2' are found
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if 'name1' in row['variable'] or 'birth1' in row['variable'] else pd.NA, axis=1)

# Display the final result
print(df_melted[['address_1', 'name', 'type', 'birth']])





import pandas as pd

# Sample data with 'address_1'
data = {
    'address_1': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format
df_melted = pd.melt(df, id_vars=['address_1'], value_vars=['name1', 'name2', 'birth1', 'birth2'], 
                    var_name='variable', value_name='value')

# Add 'type' column based on the variable names
df_melted['type'] = df_melted['variable'].apply(lambda x: 1 if 'name' in x else 2)

# Split the melted data into separate columns for name and birth
df_melted['name'] = df_melted.apply(lambda row: row['value'] if 'name' in row['variable'] else None, axis=1)
df_melted['birth'] = df_melted.apply(lambda row: row['value'] if 'birth' in row['variable'] else None, axis=1)

# Drop the 'variable' and 'value' columns as they are no longer needed
df_melted = df_melted.drop(columns=['variable', 'value'])

# Set 'address_1' to NaN for the 'name2' and 'birth2' rows
df_melted['address_1'] = df_melted.apply(lambda row: row['address_1'] if 'name' in row['variable'] else pd.NA, axis=1)

# Display the result
print(df_melted[['address_1', 'name', 'type', 'birth']])




import pandas as pd

# Sample data with an additional 'group' column
data = {
    'group': ['A', 'B'],
    'name1': ['Alice', 'John'],
    'birth1': [1990, 1980],
    'name2': ['Bob', 'Emma'],
    'birth2': [1985, 1995]
}

df = pd.DataFrame(data)

# Melt the dataframe to long format, keeping 'group' as an id variable
df_melted = pd.melt(df, id_vars=['group', 'birth1', 'birth2'], value_vars=['name1', 'name2'], 
                    var_name='type', value_name='name')

# Adjust the birth column based on the type (name1 or name2)
df_melted['birth'] = df_melted.apply(lambda row: row['birth1'] if row['type'] == 'name1' else row['birth2'], axis=1)

# Drop the original birth columns
df_melted = df_melted.drop(columns=['birth1', 'birth2'])

# Map type to 1 for 'name1' and 2 for 'name2'
df_melted['type'] = df_melted['type'].map({'name1': 1, 'name2': 2})

# Display the final result
print(df_melted)




import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z'],
    'other_col_1': [100, 200, 300]
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30],
    'other_col_2': [500, 600, 700]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Melt both DataFrames, turning info_1 and info_2 into a single 'info' column
df1_melted = df1.melt(id_vars=['client_1', 'other_col_1'], value_vars=['info_1'], 
                      var_name='info_type', value_name='info')
df2_melted = df2.melt(id_vars=['client_1', 'other_col_2', 'value'], value_vars=['info_2'], 
                      var_name='info_type', value_name='info')

# Concatenate the two melted DataFrames
result = pd.concat([df1_melted, df2_melted], ignore_index=True)

# Display the result
print(result)




import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z']
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Melt the two DataFrames so that info_1 and info_2 are in the same column
df1_melted = df1.melt(id_vars=['client_1'], value_vars=['info_1'], var_name='info_type', value_name='info')
df2_melted = df2.melt(id_vars=['client_1'], value_vars=['info_2'], var_name='info_type', value_name='info')

# Concatenate the melted DataFrames
result = pd.concat([df1_melted, df2_melted], ignore_index=True)

# Display the result
print(result)





import pandas as pd

# Example data for the first DataFrame
df1 = pd.DataFrame({
    'client_1': ['A', 'B', 'C'],
    'info_1': ['info_X', 'info_Y', 'info_Z']
})

# Example data for the second DataFrame
df2 = pd.DataFrame({
    'client_A': ['A', 'B', 'D'],
    'info_2': ['info_W', 'info_V', 'info_U'],
    'value': [10, 20, 30]
})

# Rename the column in df2 to match df1
df2.rename(columns={'client_A': 'client_1'}, inplace=True)

# Concatenate the two DataFrames
result = pd.concat([df1, df2], ignore_index=True)

# Display the result
print(result)




import pandas as pd
import io
import streamlit as st

def save_results_to_files(df_transliterations):
    # Create a buffer to use for Excel writer
    buffer = io.BytesIO()
    
    # Ensure the DataFrame is valid before processing
    if df_transliterations is None or df_transliterations.empty:
        st.warning("The DataFrame is empty. Please provide valid data.")
        return None
    
    # Display the DataFrame on the Streamlit app
    st.write("Preview of DataFrame:", df_transliterations.head())

    # Write the DataFrame to an Excel worksheet
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df_transliterations.to_excel(writer, sheet_name='Petit_contentieux', index=False)

    # Rewind the buffer to prepare it for download
    buffer.seek(0)

    # Add a download button to download the DataFrame as an Excel file
    st.download_button(
        label="Download data as Excel",
        data=buffer,
        file_name='Petit_contentieux.xlsx',
        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )

    return 'Petit_contentieux.xlsx'






def get_name_transliterations(name_ar, num_variations=10):
    try:
        prompt = (
            f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. "
            "I want only the translations without any additional text, just the translated names. "
            "Don't list the translated names, just separate them with a semi-colon. "
            "Remove any number from the output, even the number 1. "
            "Make sure the variations are realistic."
        )
        
        # OpenAI API call (update with Azure OpenAI setup if needed)
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # Replace with the correct model or Azure deployment name
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=400,
            temperature=0,
        )
        
        # Extract content safely
        content = response.choices[0].message.content if response.choices else None
        
        if content and content.strip():  # Check if content is valid
            return [variation.strip() for variation in content.split(';') if variation.strip()]
        else:
            return []  # Return an empty list if no valid content
    except Exception as e:
        print(f"Error in API call: {e}")
        return []





Pour finaliser le projet, je propose une réunion la semaine prochaine



def get_name_transliterations(name_ar, num_variations=10):
    prompt = (
        f"Generate {num_variations} French transliterations of the Arabic name {name_ar}. "
        "I want only the translations without any additional text, just separated by semicolons. "
        "Remove any number from the output, even number 1. "
        "Ensure the variations are realistic."
    )
    
    for i in range(1, 4):  # Retry logic
        try:
            # Replace with actual API setup
            client = AzureOpenAI(
                api_version="AZURE_AOAI_API_VERSION",
                azure_endpoint="APIGEE_ENDPOINT",
                api_key="FAKE_KEY",
            )
            response = client.chat.completions.create(
                model="AZURE_AOAI_MODEL_DEPLOYMENT_NAME",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=4000,
                temperature=0.7,
            )
            # Debugging: Print the response to verify
            print("API Response:", response)
            
            # Check if the response has the expected structure
            content = response.choices[0].message.content if response.choices and response.choices[0].message.content else None
            
            if content:
                variations = content.split(';')
                return [variation.strip() for variation in variations if variation.strip()]
            else:
                print(f"Warning: Empty content in response for name: {name_ar}")
                return []  # Return an empty list if content is None
            
        except Exception as e:
            print(f"Retry {i}: Error occurred - {e}")
            continue
    
    return []  # Return an empty list if all retries fail




import datetime
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

# Define the function to get name transliterations
def get_name_transliterations(name_ar, num_variations=10):
    prompt = (
        f"Generate {num_variations} French transliterations of the Arabic name {name_ar}. "
        "I want only the translations without any additional text, just separated by semicolons. "
        "Remove any number from the output, even number 1. "
        "Ensure the variations are realistic."
    )
    
    for i in range(1, 4):  # Retry logic
        try:
            # Replace with actual API setup
            client = AzureOpenAI(
                api_version="AZURE_AOAI_API_VERSION",
                azure_endpoint="APIGEE_ENDPOINT",
                api_key="FAKE_KEY",
            )
            response = client.chat.completions.create(
                model="AZURE_AOAI_MODEL_DEPLOYMENT_NAME",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=4000,
                temperature=0.7,
            )
            variations = response.choices[0].message.content.split(';')
            return [variation.strip() for variation in variations if variation.strip()]
        except Exception as e:
            print(f"Retry {i}: Error occurred - {e}")
            continue
    return []  # Return an empty list if all retries fail

# Batch processing function
def process_batch(batch_df):
    transliterations = []
    for index, row in batch_df.iterrows():
        name = row['name_ar']
        variations = get_name_transliterations(name)
        transliterations.append({"name_ar": name, "French Transliterations": variations})
    return transliterations

# Main script
if __name__ == "__main__":
    print("Start Time:", datetime.datetime.now())
    
    # Load data
    batch_size = 5
    Train_data = Train_data.dropna(subset=['name_ar'])  # Ensure no NaN values
    total_names = len(Train_data)
    
    # Split data into batches
    batches = [Train_data[i:i + batch_size] for i in range(0, total_names, batch_size)]
    
    # Use ThreadPoolExecutor for parallel processing
    all_transliterations = []
    with ThreadPoolExecutor() as executor:
        results = executor.map(process_batch, batches)
        for batch_result in results:
            all_transliterations.extend(batch_result)
    
    # Save the results to an Excel file
    df_transliterations = pd.DataFrame(all_transliterations)
    df_transliterations.to_excel('name_transliterationsV8.xlsx', index=False)
    
    print("End Time:", datetime.datetime.now())







import datetime
import pandas as pd

# Log the start time
print(datetime.datetime.now())

# Assuming Train_data is your DataFrame with 100 Arabic names
batch_size = 5
total_names = len(Train_data)

for i in range(0, total_names, batch_size):
    # Get the current batch
    df = Train_data[i:i + batch_size]
    
    # Drop rows with missing Arabic names
    df = df.dropna(subset=['name_ar'])
    
    # Generate and display transliterations
    transliterations = []
    for index, row in df.iterrows():
        name = row['name_ar']
        variations = get_name_transliterations(name)  # Assuming this function is defined elsewhere
        transliterations.append({"name_ar": name, "French Transliterations": variations})
    
    # Save the transliterations to an Excel file
    df_transliterations = pd.DataFrame(transliterations)
    excel_file_name = f'name_transliterations_batch_{i//batch_size + 1}.xlsx'
    df_transliterations.to_excel(excel_file_name, index=False)

# Log the end time
print(datetime.datetime.now())





import pandas as pd
import datetime

# Function to simulate getting transliterations for a batch of names
def get_name_transliterations_batch(names):
    """
    Generate transliterations for a batch of names.
    Simulate the process with your actual implementation for Azure OpenAI.
    """
    # For simplicity, just append mock transliterations for now
    return [f"Transliteration {i}" for i in range(1, len(names) + 1)]

# Load and clean data
print(datetime.datetime.now())
df = Train_data.dropna(subset=['name_ar'])

# Initialize results list
transliterations = []

# Process in batches of 5
batch_size = 5
for i in range(0, len(df), batch_size):
    batch = df['name_ar'].iloc[i:i + batch_size].tolist()  # Get a batch of 5 names
    batch_transliterations = get_name_transliterations_batch(batch)  # Call the API for the batch

    # Append results for the batch
    for name, variations in zip(batch, batch_transliterations):
        transliterations.append({"name_ar": name, "French Transliterations": variations})

# Save results to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterationsV8_batch5.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

print(datetime.datetime.now())




import pandas as pd
import datetime

# Function to simulate getting transliterations for a batch of names
def get_name_transliterations_batch(names):
    """
    Generate transliterations for a batch of names.
    Simulate the process with your actual implementation for Azure OpenAI.
    """
    # For simplicity, just append mock transliterations for now
    return [f"Transliteration {i}" for i in range(1, len(names) + 1)]

# Load and clean data
print(datetime.datetime.now())
df = Train_data.dropna(subset=['name_ar'])

# Initialize results list
transliterations = []

# Process in batches of 5
batch_size = 5
for i in range(0, len(df), batch_size):
    batch = df['name_ar'].iloc[i:i + batch_size].tolist()  # Get a batch of 5 names
    batch_transliterations = get_name_transliterations_batch(batch)  # Call the API for the batch

    # Append results for the batch
    for name, variations in zip(batch, batch_transliterations):
        transliterations.append({"name_ar": name, "French Transliterations": variations})

# Save results to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterationsV8_batch5.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

print(datetime.datetime.now())




variations = response.choices[0].message.content.split('\n') formatted_variations = ' '.join([f"{i+1}. {variation.strip()}" for i, variation in enumerate(variations) if variation.strip()]) return formatted_variations




import pandas as pd
import openai
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Function to get French transliterations of Arabic names
def get_name_transliterations(name_ar, num_variations=5):
    prompt = f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. Make sure the variations are realistic."
    response = client.chat.completions.create(
        model='your-model-deployment-name',  # Replace with your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7
    )
    variations = response.choices[0].message.content.split('\n')
    formatted_variations = '\n'.join([f"{i+1}. {variation.strip()}" for i, variation in enumerate(variations)])
    return formatted_variations

# Load the CSV file
csv_file_name = 'arabic_names.csv'
df = pd.read_csv(csv_file_name)

# Generate and display transliterations
transliterations = []
for index, row in df.iterrows():
    name = row['Arabic Name']
    variations = get_name_transliterations(name)
    transliterations.append({"Arabic Name": name, "French Transliterations": variations})
    print(f"Arabic Name: {name} -> French Transliterations: \n{variations}\n")

# Save the transliterations to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterations.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

# Upload the Excel file to Azure OpenAI
try:
    with open(excel_file_name, "rb") as file:
        training_response = client.files.create(file=file, purpose="fine-tune")
    training_file_id = training_response.id
    print("Training file ID:", training_file_id)
except Exception as e:
    print(f"Error uploading training file: {e}")


















































import pandas as pd
import openai
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Function to get French transliterations of Arabic names
def get_name_transliterations(name_ar, num_variations=5):
    prompt = f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. Make sure the variations are realistic."
    response = client.chat.completions.create(
        model='your-model-deployment-name',  # Replace with your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7
    )
    return response.choices[0].message.content

# Load the CSV file
csv_file_name = 'arabic_names.csv'
df = pd.read_csv(csv_file_name)

# Generate and display transliterations
transliterations = []
for index, row in df.iterrows():
    name = row['Arabic Name']
    variations = get_name_transliterations(name)
    transliterations.append({"Arabic Name": name, "French Transliterations": variations})
    print(f"Arabic Name: {name} -> French Transliterations: \n{variations}\n")

# Save the transliterations to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterations.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

# Upload the Excel file to Azure OpenAI
try:
    with open(excel_file_name, "rb") as file:
        training_response = client.files.create(file=file, purpose="fine-tune")
    training_file_id = training_response.id
    print("Training file ID:", training_file_id)
except Exception as e:
    print(f"Error uploading training file: {e}")






# Upload the training dataset file to Azure OpenAI try: with open(training_file_name, "rb") as file: training_response = client.files.create(file=file, purpose="fine-tune") training_file_id = training_response.id print("Training file ID:", training_file_id) except Exception as e: print(f"Error uploading training file: {e}")



import pandas as pd
import openai
import json
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Create a DataFrame with Arabic names and their French translations
data = {
    'Arabic Name': ['محمد علي', 'فاطمة الزهراء', 'يوسف بن تاشفين'],
    'French Name': ['Mohamed Ali', 'Fatima Zahra', 'Youssef Ben Tachfine']
}
df = pd.DataFrame(data)

# Convert the DataFrame to JSONL format
training_data = []
for index, row in df.iterrows():
    training_data.append({
        "prompt": f"Translate the following Arabic name to French: '{row['Arabic Name']}'",
        "completion": row['French Name']
    })

# Save the training data to a JSONL file
training_file_name = 'training_data.jsonl'
with open(training_file_name, 'w') as f:
    for entry in training_data:
        f.write(json.dumps(entry) + '\n')

# Upload the training dataset file to Azure OpenAI
training_response = client.files.create(
    file=open(training_file_name, "rb"), purpose="fine-tune"
)
training_file_id = training_response.id

# Print the training file ID
print("Training file ID:", training_file_id)

# Define the new Arabic name and family name
arabic_name = "علي بن أبي طالب"

# Create the prompt for translation
prompt = f"Translate the following Arabic name to French: '{arabic_name}'"

# Make a request to the fine-tuned model
response = openai.Completion.create(
    model='chatgpt-turbo',
    prompt=prompt,
    max_tokens=50
)

# Extract and print the translation
translation = response.choices[0].text.strip()
print(f"Translation: {translation}")




Fine-tuning a model involves taking a pre-trained model and training it further on a specific dataset to adapt it to a particular task or domain. This process helps the model learn the nuances and specifics of the new data, improving its performance on the desired task. Fine-tuning is often used to customize models for specialized applications without needing to train a model from scratch.




import pandas as pd
import openai
import json

# Set your OpenAI API key
openai.api_key = 'your-api-key'

# Create a DataFrame with Arabic names and their French translations
data = {
    'Arabic Name': ['محمد علي', 'فاطمة الزهراء', 'يوسف بن تاشفين'],
    'French Name': ['Mohamed Ali', 'Fatima Zahra', 'Youssef Ben Tachfine']
}
df = pd.DataFrame(data)

# Convert the DataFrame to JSONL format
training_data = []
for index, row in df.iterrows():
    training_data.append({
        "prompt": f"Translate the following Arabic name to French: '{row['Arabic Name']}'",
        "completion": row['French Name']
    })

# Save the training data to a JSONL file
with open('training_data.jsonl', 'w') as f:
    for entry in training_data:
        f.write(json.dumps(entry) + '\n')

# Fine-tune the model
response = openai.FineTune.create(
    training_file=openai.File.create(file=open('training_data.jsonl'), purpose='fine-tune').id,
    model='chatgpt-turbo'
)

# Print the fine-tuning job ID
print(f"Fine-tuning job ID: {response['id']}")

# Define the new Arabic name and family name
arabic_name = "علي بن أبي طالب"

# Create the prompt for translation
prompt = f"Translate the following Arabic name to French: '{arabic_name}'"

# Make a request to the fine-tuned model
response = openai.Completion.create(
    model='chatgpt-turbo',
    prompt=prompt,
    max_tokens=50
)

# Extract and print the translation
translation = response.choices[0].text.strip()
print(f"Translation: {translation}")

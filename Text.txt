import pandas as pd
import json
import openai

# Step 1: Read the dataset from the Excel file
# Make sure to provide the correct path to your Excel file
df = pd.read_excel('path_to_your_file.xlsx')  # Replace with the actual path to your Excel file

# Step 2: Prepare the dataset for fine-tuning
name_pairs = []
for _, row in df.iterrows():
    # Create prompt and completion pairs from Arabic and French names
    prompt = f"Arabic Name: {row['name_ar']} {row['family_name_ar']}\nFrench transliterations:"
    completion = f" {row['name_fr']} {row['family_name_fr']}"
    
    name_pairs.append({
        "prompt": prompt,
        "completion": completion
    })

# Save the prepared data into a JSONL file (JSON Lines format)
with open('name_transliterations.jsonl', 'w', encoding='utf-8') as f:
    for pair in name_pairs:
        json.dump(pair, f, ensure_ascii=False)
        f.write('\n')

# Step 3: Upload your dataset to OpenAI for fine-tuning
openai.api_key = "your-api-key"  # Replace with your OpenAI API key

# Upload the dataset file to OpenAI
response = openai.File.create(
    file=open("name_transliterations.jsonl"),
    purpose='fine-tune'
)

# Print the file upload response
print("File uploaded successfully:", response)

# Step 4: Fine-tune the GPT-3 model using the uploaded dataset
fine_tune_response = openai.FineTune.create(
    training_file=response['id'],
    model="davinci"  # You can use other models like "curie" or "babbage" depending on your needs
)

# Print fine-tune response
print("Fine-tuning started:", fine_tune_response)

# Step 5: Monitor the fine-tuning process (check the fine-tuning status)
status = openai.FineTune.retrieve(id=fine_tune_response['id'])
print("Fine-tuning status:", status)

# Step 6: Use the fine-tuned model to generate French variations for Arabic names
def generate_french_variations(name_arabic):
    response = openai.Completion.create(
        model=fine_tune_response['fine_tuned_model'],  # Use the fine-tuned model ID
        prompt=f"Arabic Name: {name_arabic}\nFrench transliterations:",
        max_tokens=100,
        n=5,  # Generate 5 variations
        temperature=0.7
    )

    # Print out the variations generated by the fine-tuned model
    for i, choice in enumerate(response.choices):
        print(f"Variation {i + 1}: {choice.text.strip()}")

# Example usage: Generate variations for an Arabic name
generate_french_variations("محمد بن علي")





import pandas as pd
import json
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import load_dataset

# Step 1: Read the dataset from the Excel file
# Make sure to provide the correct path to your Excel file
df = pd.read_excel('path_to_your_file.xlsx')

# Step 2: Prepare the dataset for fine-tuning
name_pairs = []
for _, row in df.iterrows():
    # Create prompt and completion pairs from Arabic and French names
    prompt = f"Arabic: {row['name_ar']} {row['family_name_ar']}\nFrench transliterations:"
    completion = f" {row['name_fr']} {row['family_name_fr']}"
    
    name_pairs.append({
        "prompt": prompt,
        "completion": completion
    })

# Save the prepared data into a JSONL file (JSON Lines format)
with open('name_transliterations.jsonl', 'w', encoding='utf-8') as f:
    for pair in name_pairs:
        json.dump(pair, f, ensure_ascii=False)
        f.write('\n')

# Step 3: Fine-tune the GPT-2 model on the dataset
# Load the dataset from the JSONL file
dataset = load_dataset('json', data_files={'train': 'name_transliterations.jsonl'})

# Load the pre-trained GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['prompt'], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Define the training arguments for fine-tuning
training_args = TrainingArguments(
    output_dir="./results",  # Directory where the fine-tuned model will be saved
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_steps=10_000,
    save_total_limit=2,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
trainer.save_model("fine_tuned_model")

# Step 4: Generate French transliterations for Arabic names using the fine-tuned model

def generate_french_variations(arabic_name):
    # Prepare the prompt for generation
    prompt = f"Arabic: {arabic_name}\nFrench transliterations:"
    inputs = tokenizer(prompt, return_tensors="pt")

    # Generate French transliterations
    outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=5, temperature=0.7)

    # Decode and display the generated variations
    for i, output in enumerate(outputs):
        generated_text = tokenizer.decode(output, skip_special_tokens=True)
        print(f"Variation {i + 1}: {generated_text}")

# Example: Generate French variations for an Arabic name
generate_french_variations("محمد")

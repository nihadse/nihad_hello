def find_highest_similarity(interdicted_checks, client_database, threshold=0):
    model_path = "/mnt/Sentence_model_path"  # Model path to your Sentence Transformer
    model = SentenceTransformer(model_path)
    
    names1 = interdicted_checks['Name'].tolist()
    names2 = client_database['Name'].tolist()

    # Convert names to embeddings
    embeddings1 = model.encode(names1, convert_to_tensor=True)
    embeddings2 = model.encode(names2, convert_to_tensor=True)
    
    # Calculate similarity matrix
    similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2).cpu().numpy()
    
    similar_pairs = []

    for i in range(len(names1)):
        client_name = names1[i]
        for j in range(len(names2)):
            similarity_score = similarity_matrix[i][j]
            # If the score is above the threshold, store it
            if similarity_score >= threshold:
                similar_pairs.append({
                    'Name interdicted_checks': client_name,
                    'Matched name in client database': client_database.iloc[j]['Name'],
                    'Similarity Score': similarity_score,
                    'Details': {
                        'nom': interdicted_checks.iloc[i]['nom'],
                        'nom_clean': interdicted_checks.iloc[i]['nom_clean'],
                        'prenom': interdicted_checks.iloc[i]['prenom'],
                        'Date de naissance interdicted_checks': interdicted_checks.iloc[i]['Date de naissance'],
                        'Lieu de naissance interdicted_checks': interdicted_checks.iloc[i]['Lieu de naissance'],
                        'Date d’effet': interdicted_checks.iloc[i]['Date d’effet'],
                        'code wilaya': interdicted_checks.iloc[i]['code wilaya'],
                        'Raison sociale client': client_database.iloc[j]['raison sociale'],
                        'Date de naissance client_database': client_database.iloc[j]['Date de naissance'],
                        'Lieu de naissance client_database': client_database.iloc[j]['Lieu de naissance'],
                        'Code juridique client': client_database.iloc[j]['code nature juridique compte']
                    }
                })
    
    # Convert the results into a DataFrame
    result_df = pd.DataFrame(similar_pairs)
    
    # Display all matches
    result_df['Date de naissance Match'] = result_df['Date de naissance interdicted_checks'] == result_df['Date de naissance client_database']
    result_df['Lieu de naissance Match'] = result_df['Lieu de naissance interdicted_checks'] == result_df['Lieu de naissance client_database']

    # Return the complete dataframe with all matches
    return result_df




import pandas as pd

# Sample DataFrame
data = {
    'name': ['Mohamed Ali', 'Ali Mohamed', 'John Doe', 'Mohamed Salah'],
    'age': [30, 25, 22, 35]
}

df = pd.DataFrame(data)

# Filter rows where 'name' contains 'mohamed' (case insensitive)
filtered_df = df[df['name'].str.contains('mohamed', case=False)]

# Display the filtered DataFrame
print(filtered_df)



def find_highest_similarity(interdicted_checks, client_database, threshold=0):
    model_path = "/mnt/Sentence_model/model"  # Assuming your model is here
    model = SentenceTransformer(model_path)
    
    names1 = interdicted_checks['Name'].tolist()
    names2 = client_database['Name'].tolist()
    
    # Embedding both lists
    embeddings1 = model.encode(names1, convert_to_tensor=True)
    embeddings2 = model.encode(names2, convert_to_tensor=True)
    
    # Calculate similarity matrix between the two sets
    similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2).cpu().numpy()
    
    similar_pairs = []
    
    # Loop over each name in interdicted_checks
    for i in range(len(names1)):
        client_name = names1[i]
        
        # For each client, loop through the database to find all matches above threshold
        for j in range(len(names2)):
            similarity_score = similarity_matrix[i][j]
            
            # If the similarity score is above the threshold, append all possible matches
            if similarity_score >= threshold:
                similar_pairs.append({
                    'Name interdicted_checks': client_name,
                    'Name client_database': client_database.iloc[j]['Name'],
                    'Nom': interdicted_checks.iloc[i]['nom'],
                    'Prenom': interdicted_checks.iloc[i]['prenom'],
                    'Date de naissance Match': interdicted_checks.iloc[i]['Date de naissance'],
                    'Lieu de naissance Match': interdicted_checks.iloc[i]['Lieu de naissance'],
                    'Sociale': client_database.iloc[j]['raison sociale'],
                    'Category': client_database.iloc[j]['catégorie client'],
                    'Date de création (entreprise)': client_database.iloc[j]['date_creation'],
                    'Code nature juridique compte': client_database.iloc[j]['code nature juridique compte'],
                    'Similarity Score': similarity_score
                })

    # Creating a DataFrame for output
    result_df = pd.DataFrame(similar_pairs)
    
    return result_df




def find_highest_similarity(interdicted_checks, client_database, threshold=0):
    model_path = "/ant/Sentence_model_transformer"
    model = SentenceTransformer(model_path)
    
    names1 = interdicted_checks['Name'].tolist()
    names2 = client_database['Name'].tolist()

    embeddings1 = model.encode(names1, convert_to_tensor=True)
    embeddings2 = model.encode(names2, convert_to_tensor=True)

    similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2).cpu().numpy()

    best_pairs = []  # Store all matches, not just the highest
    for i in range(len(names1)):
        client_name = names1[i]
        for j in range(len(names2)):
            similarity_score = similarity_matrix[i][j]
            if similarity_score > threshold:
                # Append all matching pairs with similarity score above threshold
                best_pairs.append({
                    'interdicted_check_name': client_name,
                    'client_name': client_database.iloc[j]['Name'],
                    'similarity_score': similarity_score,
                    'client_details': client_database.iloc[j]
                })

    # Format the results for all matches
    similar_pairs = []
    for pair in best_pairs:
        interdicted_index = interdicted_checks[interdicted_checks['Name'] == pair['interdicted_check_name']].index[0]
        client_match = pair['client_details']

        similar_pairs.append({
            "Name Interdicted checks": pair['interdicted_check_name'],
            "nom": interdicted_checks.iloc[interdicted_index]['nom'],
            "prenom": interdicted_checks.iloc[interdicted_index]['prenom'],
            "Date de naissance interdicted checks": interdicted_checks.iloc[interdicted_index]['Date de naissance'],
            "Lieu de naissance interdicted checks": interdicted_checks.iloc[interdicted_index]['Lieu de naissance'],
            "Name client database": pair['client_name'],
            "Date de naissance client_database": client_match['Date de naissance'],
            "Lieu de naissance client_database": client_match['Lieu de naissance'],
            "Similarity Score": pair['similarity_score']
        })

    # Convert results to a DataFrame for easier output
    result_df = pd.DataFrame(similar_pairs)
    
    # Perform matching checks for Date and Place of Birth
    result_df['Date de naissance Match'] = result_df['Date de naissance interdicted checks'] == result_df['Date de naissance client_database']
    result_df['Lieu de naissance Match'] = result_df['Lieu de naissance interdicted checks'] == result_df['Lieu de naissance client_database']

    result_df['Date de naissance Match'] = result_df['Date de naissance Match'].map({True: 'yes', False: 'no'})
    result_df['Lieu de naissance Match'] = result_df['Lieu de naissance Match'].map({True: 'yes', False: 'no'})

    return result_df






import PyPDF2
import httpx
from oauthlib.oauth2 import BackendApplicationClient
from requests_oauthlib import OAuth2Session

# OIDC Configuration
OIDC_CLIENT_ID = "your_client_id"
OIDC_CLIENT_SECRET = "your_client_secret"
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_SCOPE = "genai-model"
AZURE_ADAI_API_VERSION = "2024-02-15-preview"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
APIGEE_ENDPOINT = "https://alfactory.api.staging.echonet/genai-model/v1"
FAKE_KEY = "your_azure_api_key"  # Replace with your actual Azure OpenAI API key

# OAuth2 Authentication
client = BackendApplicationClient(client_id=OIDC_CLIENT_ID)
oauth = OAuth2Session(client=client)
token = oauth.fetch_token(token_url=OIDC_ENDPOINT, client_id=OIDC_CLIENT_ID, client_secret=OIDC_CLIENT_SECRET, scope=OIDC_SCOPE)

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text += page.extract_text()
    return text

# Function to interact with Azure OpenAI's GPT-4 turbo model
def ask_pdf_question(pdf_text, question):
    # Prepare the prompt with the PDF content and the user's question
    prompt = f"{pdf_text}\n\nQuestion: {question}"

    # Create a custom HTTP client with the necessary authentication and token
    with httpx.Client(verify=False) as http_client:
        # Use the client instance to make the chat completion request
        completion = client.chat.completions.create(
            model=AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": "You are an AI assistant."},
                {"role": "user", "content": prompt},
            ],
            api_key=FAKE_KEY,
            api_version=AZURE_ADAI_API_VERSION,
            http_client=http_client,
        )
    
    return completion.choices[0].message['content']

# Example usage
pdf_text = extract_text_from_pdf("example.pdf")  # Replace with your actual PDF file
question = "What is the main point of this document?"
response = ask_pdf_question(pdf_text, question)

print(response)



import openai
import PyPDF2
import httpx
from oauthlib.oauth2 import BackendApplicationClient
from requests_oauthlib import OAuth2Session

# OIDC Configuration
OIDC_CLIENT_ID = "your_client_id"
OIDC_CLIENT_SECRET = "your_client_secret"
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_SCOPE = "genai-model"
AZURE_ADAI_API_VERSION = "2024-02-15-preview"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
APIGEE_ENDPOINT = "https://alfactory.api.staging.echonet/genai-model/v1"
FAKE_KEY = "your_azure_api_key"  # Replace with your actual Azure OpenAI API key

# OAuth2 Authentication
client = BackendApplicationClient(client_id=OIDC_CLIENT_ID)
oauth = OAuth2Session(client=client)
token = oauth.fetch_token(token_url=OIDC_ENDPOINT, client_id=OIDC_CLIENT_ID, client_secret=OIDC_CLIENT_SECRET, scope=OIDC_SCOPE)

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text += page.extract_text()
    return text

# Function to interact with Azure OpenAI's GPT-4 model
def ask_pdf_question(pdf_text, question):
    # Prepare the prompt with the PDF content and the user's question
    prompt = f"{pdf_text}\n\nQuestion: {question}"
    
    # Make the API call to the model
    with httpx.Client(verify=False) as http_client:
        response = openai.ChatCompletion.create(
            api_key=FAKE_KEY,
            model=AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": "You are an AI assistant."},
                {"role": "user", "content": prompt},
            ],
            api_base=APIGEE_ENDPOINT,
            api_version=AZURE_ADAI_API_VERSION,
        )
    
    return response.choices[0].message['content']

# Example usage
pdf_text = extract_text_from_pdf("example.pdf")  # Replace with your actual PDF file
question = "What is the main point of this document?"
response = ask_pdf_question(pdf_text, question)

print(response)




import chromadb
from langchain.embeddings import SentenceTransformerEmbeddings

def insert_embeddings_in_chroma(embeddings, chunk_texts):
    # Initialize Chroma client
    client = chromadb.Client()

    # Retrieve the existing collection
    collection = client.get_collection(name="pdf_embeddings")

    # Add embeddings and text data to the existing collection
    collection.add(
        embeddings=embeddings, 
        metadatas=[{"text": chunk} for chunk in chunk_texts], 
        ids=[f"chunk_{i}" for i in range(len(chunk_texts))]
    )

# Example usage
def main():
    # Load your PDF and extract text
    pdf_file_path = '/path/to/your/pdf/file.pdf'
    extracted_text = extract_text_from_pdf(pdf_file_path)  # Your function to extract text
    chunks = split_text_into_chunks(extracted_text, chunk_size=500)  # Your function to split text
    
    # Use your SentenceTransformer to get embeddings
    embeddings_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    embeddings = embeddings_model.embed_documents(chunks)

    # Insert embeddings into the existing Chroma collection
    insert_embeddings_in_chroma(embeddings, chunks)

if __name__ == "__main__":
    main()




from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma

# Create a Chroma vector store
def create_vector_store(texts):
    # Initialize embeddings using SentenceTransformer
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")  # Use a suitable model
    # Create the Chroma vector store
    docsearch = Chroma.from_texts(texts, embeddings.embed_documents)  # Use embed_documents method
    return docsearch


import os
import PyPDF2
from langchain.embeddings.sentence_transformers import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma
from langchain_openai import AzureOpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set your Azure OpenAI API configuration
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_MODEL_DEPLOYMENT_NAME = "gpt-4-turbo"  # Change to your model deployment name

# Function to read and extract text from a PDF
def read_pdf(file_path):
    raw_text = ''
    with open(file_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text = page.extract_text()
            if text:
                raw_text += text
    return raw_text

# Function to split text into chunks
def split_text(text):
    # Splitting text into manageable chunks
    chunks = text.split('\n\n')  # Split based on paragraph breaks; you can customize this
    return chunks

# Create a Chroma vector store
def create_vector_store(texts):
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")  # Use a suitable model
    docsearch = Chroma(embedding_function=embeddings)
    docsearch.add_texts(texts)
    return docsearch

# Function to get an answer from Azure OpenAI
def ask_azure_openai(llm, query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = llm({"prompt": prompt, "max_tokens": 150})
    return response['choices'][0]['text'].strip()

# Main function to run the chatbot
def main():
    # Load your PDF
    pdf_path = '/content/gdrive/My Drive/data/2023_GPT4All_Technical_Report.pdf'  # Change path as needed
    raw_text = read_pdf(pdf_path)
    texts = split_text(raw_text)
    docsearch = create_vector_store(texts)
    
    # Set up the Azure OpenAI client
    llm = AzureOpenAI(
        deployment_name=AZURE_MODEL_DEPLOYMENT_NAME,
        openai_api_key=AZURE_OPENAI_API_KEY,
        openai_api_base=AZURE_OPENAI_ENDPOINT
    )

    # Example query
    query = "Who are the authors of the article?"
    
    # Step 1: Retrieve relevant documents
    docs = docsearch.similarity_search(query)  # Find relevant documents

    # Step 2: Combine the texts of the retrieved documents
    context = " ".join([doc.page_content for doc in docs])  # Adjust how you combine based on your needs
    
    # Step 3: Get answer using Azure OpenAI
    answer = ask_azure_openai(llm, query, context)
    print(answer)

# Run the main function
if __name__ == "__main__":
    main()





import os
import httpx
import json
import PyPDF2
from langchain.chains.question_answering import load_qa_chain
from langchain_openai import AzureOpenAI  # Use AzureOpenAI for Azure deployment
from langchain.embeddings.sentence_transformers import SentenceTransformerEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set your Azure OpenAI API configuration
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")  # Ensure your key is set in .env
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")  # Set your Azure endpoint
AZURE_MODEL_DEPLOYMENT_NAME = "gpt-4-turbo"  # Change to your model deployment name

# Function to read and extract text from a PDF
def read_pdf(file_path):
    raw_text = ''
    with open(file_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text = page.extract_text()
            if text:
                raw_text += text
    return raw_text

# Function to split text into chunks
def split_text(text):
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    return text_splitter.split_text(text)

# Create a Chroma vector store
def create_vector_store(texts):
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")  # Use a suitable model
    docsearch = Chroma(embedding_function=embeddings)
    docsearch.add_texts(texts)
    return docsearch

# Load the QA chain with Azure OpenAI
def load_qa_chain_with_azure():
    llm = AzureOpenAI(deployment_name=AZURE_MODEL_DEPLOYMENT_NAME, openai_api_key=AZURE_OPENAI_API_KEY, openai_api_base=AZURE_OPENAI_ENDPOINT)
    
    # Load the QA chain
    return load_qa_chain(llm, chain_type="stuff")

# Perform a query
def perform_query(docsearch, query):
    chain = load_qa_chain_with_azure()
    docs = docsearch.similarity_search(query)  # Find relevant documents
    answer = chain.run(input_documents=docs, question=query)  # Get answer
    return answer

# Main function to run the chatbot
def main():
    pdf_path = '/content/gdrive/My Drive/data/2023_GPT4All_Technical_Report.pdf'  # Change path as needed
    raw_text = read_pdf(pdf_path)
    texts = split_text(raw_text)
    docsearch = create_vector_store(texts)
    
    # Example query
    query = "Who are the authors of the article?"
    answer = perform_query(docsearch, query)
    print(answer)

# Run the main function
if __name__ == "__main__":
    main()




import os
import httpx
import json
from langchain_openai import OpenAI  # Updated import for OpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.embeddings.sentence_transformers import SentenceTransformerEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
import PyPDF2
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set your OpenAI API Key
os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"  # Ensure you replace with your actual key

# Other necessary setup code...

# Function to read and extract text from a PDF
def read_pdf(file_path):
    raw_text = ''
    with open(file_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text = page.extract_text()
            if text:
                raw_text += text
    return raw_text

# Function to split text into chunks
def split_text(text):
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    return text_splitter.split_text(text)

# Create a Chroma vector store
def create_vector_store(texts):
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")  # Use suitable model
    docsearch = Chroma(embedding_function=embeddings)
    docsearch.add_texts(texts)
    return docsearch

# Load the QA chain
def load_qa_chain_with_openai():
    # Create an instance of OpenAI with the API key
    llm = OpenAI(model_name="gpt-4", openai_api_key=os.getenv("OPENAI_API_KEY"))
    
    # Load the QA chain
    return load_qa_chain(llm, chain_type="stuff")

# Perform a query
def perform_query(docsearch, query):
    chain = load_qa_chain_with_openai()
    docs = docsearch.similarity_search(query)  # Find relevant documents
    answer = chain.run(input_documents=docs, question=query)  # Get answer
    return answer

# Main function to run the chatbot
def main():
    pdf_path = '/content/gdrive/My Drive/data/2023_GPT4All_Technical_Report.pdf'  # Change path as needed
    raw_text = read_pdf(pdf_path)
    texts = split_text(raw_text)
    docsearch = create_vector_store(texts)
    
    # Example query
    query = "Who are the authors of the article?"
    answer = perform_query(docsearch, query)
    print(answer)

# Run the main function
if __name__ == "__main__":
    main()



# 1. Install necessary packages
!pip install langchain
!pip install sentence_transformers
!pip install PyPDF2
!pip install chromadb
!pip install httpx
!pip install python-dotenv
!pip install httpx-auth

# 2. Import libraries
import os
import httpx
import json
from openai import AzureOpenAI  # Corrected import for Azure OpenAI
import PyPDF2
from dotenv import load_dotenv
from langchain.embeddings.sentence_transformers import SentenceTransformerEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains.question_answering import load_qa_chain

# Load environment variables from a .env file
load_dotenv()

# 3. Set up OAuth2 credentials and endpoints
OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID", "YOUR_CLIENT_ID")
OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET", "YOUR_CLIENT_SECRET")
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_SCOPE = "genai-model"
APIGEE_ENDPOINT = "https://aifactory.api.staging.echonet/genai-model/v1"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
AZURE_ADAI_EMBEDDING_DEPLOYMENT_NAME = "text-embedding-ada"
AZURE_ADAI_API_VERSION = "2024-02-15-preview"

# 4. Authenticate using OAuth2
async def authenticate():
    async with httpx.AsyncClient() as client:
        response = await client.post(OIDC_ENDPOINT, data={
            'client_id': OIDC_CLIENT_ID,
            'client_secret': OIDC_CLIENT_SECRET,
            'scope': OIDC_SCOPE,
            'grant_type': 'client_credentials'
        })
        response.raise_for_status()
        token_info = response.json()
        return token_info['access_token']

# 5. Connect to Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"

# 6. Read PDF file
reader = PyPDF2.PdfReader('/content/gdrive/My Drive/data/2023_GPT4All_Technical_Report.pdf')

# 7. Extract text from PDF
raw_text = ''
for page in reader.pages:
    text = page.extract_text()
    if text:
        raw_text += text

# 8. Split text into chunks
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
)
texts = text_splitter.split_text(raw_text)

# 9. Use Sentence Transformers for embeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")  # Specify the model you want

# 10. Create a Chroma vector store
docsearch = Chroma(embedding_function=embeddings)  # Initialize Chroma with the embedding function
docsearch.add_texts(texts)  # Add your texts

# 11. Load the QA chain using Azure OpenAI
async def load_qa_chain_with_auth():
    access_token = await authenticate()
    
    # Set up the Azure OpenAI client
    openai.api_type = "azure"
    openai.api_base = APIGEE_ENDPOINT
    openai.api_version = AZURE_ADAI_API_VERSION
    openai.api_key = access_token

    # Load the QA chain with Azure OpenAI
    return load_qa_chain(AzureOpenAI(deployment_name=AZURE_ADAI_MODEL_DEPLOYMENT_NAME), chain_type="stuff")

# 12. Perform a query
async def perform_query(query):
    chain = await load_qa_chain_with_auth()
    docs = docsearch.similarity_search(query)
    answer = chain.run(input_documents=docs, question=query)
    return answer

# 13. Example query
import asyncio

query = "Who are the authors of the article?"
answer = asyncio.run(perform_query(query))
print(answer)


https://github.com/alejandro-ao/langchain-ask-pdf/blob/main/app.py


https://github.com/pik1989/pdfGPT/blob/main/app.py



import pdfplumber
from sentence_transformers import SentenceTransformer
import numpy as np
import chromadb

# Step 1: Extract text from PDF
def extract_text_from_pdf(pdf_file_path):
    text = ""
    with pdfplumber.open(pdf_file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

# Step 2: Split text into chunks
def split_text_into_chunks(text, chunk_size=500):
    words = text.split()
    chunks = []
    current_chunk = []
    
    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= chunk_size:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

# Step 3: Generate embeddings
def generate_embeddings(chunks):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)
    return embeddings

# Step 4: Save embeddings to a file (optional)
def save_embeddings(embeddings, file_name='document_embeddings.npy'):
    np.save(file_name, embeddings)

# Step 5: Initialize Chroma DB and insert embeddings
def insert_embeddings_in_chroma(embeddings, chunk_texts):
    # Initialize Chroma client
    client = chromadb.Client()
    
    # Create or get a collection to store embeddings
    collection = client.create_collection(name="pdf_embeddings")
    
    # Add embeddings and text data to the collection
    collection.add(
        embeddings=embeddings,
        metadatas=[{"text": chunk} for chunk in chunk_texts],
        ids=[f"chunk_{i}" for i in range(len(chunk_texts))]
    )

# Example usage
pdf_file_path = 'your_document.pdf'
extracted_text = extract_text_from_pdf(pdf_file_path)
chunks = split_text_into_chunks(extracted_text, chunk_size=500)
embeddings = generate_embeddings(chunks)

# Save embeddings locally (optional)
save_embeddings(embeddings, 'document_embeddings.npy')

# Insert embeddings into Chroma DB
insert_embeddings_in_chroma(embeddings, chunks)




import pdfplumber
from sentence_transformers import SentenceTransformer
import numpy as np
import httpx

# Define your Azure OpenAI parameters
AZURE_AOAI_API_VERSION = "2023-05-15"  # Update to the current version if needed
APIGEE_ENDPOINT = "https://<YOUR_AZURE_OPENAI_ENDPOINT>.openai.azure.com/"  # Replace with your endpoint
AZURE_AOAI_MODEL_DEPLOYMENT_NAME = "gpt-35-turbo"  # Replace with your model name

# Create an Azure OpenAI client
class AzureOpenAI:
    def __init__(self, api_version, azure_endpoint, api_key):
        self.api_version = api_version
        self.azure_endpoint = azure_endpoint
        self.api_key = api_key
        self.client = httpx.Client(
            headers={
                "Content-Type": "application/json",
                "api-key": self.api_key
            }
        )

    def chat_completion(self, messages):
        response = self.client.post(
            f"{self.azure_endpoint}chat/completions?api-version={self.api_version}",
            json={
                "model": AZURE_AOAI_MODEL_DEPLOYMENT_NAME,
                "messages": messages
            }
        )
        response.raise_for_status()
        return response.json()

# Function to extract text from PDF
def extract_text_from_pdf(pdf_file_path):
    text = ""
    with pdfplumber.open(pdf_file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

# Function to split text into chunks
def split_text_into_chunks(text, chunk_size=500):
    words = text.split()
    chunks = []
    current_chunk = []
    
    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= chunk_size:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

# Function to generate embeddings (not used directly for ChatGPT but kept for context)
def generate_embeddings(chunks):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)
    return embeddings

# Function to save embeddings to a file
def save_embeddings(embeddings, file_name='document_embeddings.npy'):
    np.save(file_name, embeddings)

# Initialize the client with your API key
client = AzureOpenAI(
    api_version=AZURE_AOAI_API_VERSION,
    azure_endpoint=APIGEE_ENDPOINT,
    api_key="YOUR_API_KEY"  # Replace with your actual API key
)

# Function to query ChatGPT
def query_chatgpt(question):
    messages = [
        {
            "role": "user",
            "content": question
        }
    ]
    
    completion = client.chat_completion(messages)
    response_content = completion['choices'][0]['message']['content']
    return response_content.strip()

# Example usage
pdf_file_path = 'your_document.pdf'  # Replace with your PDF file path
extracted_text = extract_text_from_pdf(pdf_file_path)
chunks = split_text_into_chunks(extracted_text, chunk_size=500)
embeddings = generate_embeddings(chunks)  # This is optional depending on your use case
save_embeddings(embeddings, 'document_embeddings.npy')  # This is optional

# Ask a question about the PDF
question = "What are the main themes discussed in the document?"
response = query_chatgpt(question)
print("Response from ChatGPT:", response)



Let me explain each step in more detail. In the Query Transformation stage, we're taking the original question (or query) and transforming it in various ways to make the search process more effective and intelligent. This helps retrieve better answers from a large collection of documents (like a PDF repository or a database).

Here’s a breakdown of the query transformation steps with detailed explanations:

1. Step-Back Prompting

This is the process of taking the original query and transforming it into a broader or more general question. The goal here is to "step back" from a specific query and ask something that is broader but still relevant. Why do this? Because sometimes, broader questions yield more comprehensive information.

Example:

Original Query: "How does deep learning help in image classification?"


If you want to broaden this query, you can ask a more general question:

Generalized Query: "How can machine learning techniques improve image analysis?"


This broader query may help retrieve results not only related to deep learning but also other machine learning techniques relevant to image classification.

How do you do this with Python?

You can use a language model (like GPT) to automatically generate this broader query. Here's a code snippet that illustrates how to take an original query and generate a broader one:

from transformers import pipeline

# Initialize a language model (GPT) to generate a broader query
rephrase_pipeline = pipeline('text-generation', model='gpt-2')

# Define a function for Step-Back Prompting
def step_back_prompting(query):
    prompt = f"Create a more general question based on this specific query: {query}"
    generalized_query = rephrase_pipeline(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']
    return generalized_query.strip()

# Example usage
original_query = "How does deep learning help in image classification?"
generalized_query = step_back_prompting(original_query)
print("Original query:", original_query)
print("Generalized query:", generalized_query)

Here, the model takes the specific query, broadens it, and gives back a more general query.

2. Multi-Query Retrieval

Sometimes a single question might not retrieve all the information you need. To deal with this, you can break the query into several sub-questions. This technique helps retrieve more diverse and related information by looking at the problem from multiple angles.

Example:

Original Query: "How does deep learning help in image classification?"

Sub-Queries:

"What are the benefits of deep learning in computer vision?"

"How does deep learning improve accuracy in classifying images?"

"What are the challenges of using deep learning for image classification?"



Each sub-query focuses on a different aspect of the original question and might retrieve different types of useful information.

How to implement this in Python:

Again, we can use a language model to help generate sub-questions. The language model will take the original query and generate variations of the query that focus on different parts of the problem.

# Function to generate multiple sub-queries based on the original query
def generate_sub_questions(query):
    prompt = f"Generate sub-questions or variations for this query: {query}"
    sub_questions = rephrase_pipeline(prompt, max_length=100, num_return_sequences=3)
    return [sq['generated_text'].strip() for sq in sub_questions]

# Example usage
sub_queries = generate_sub_questions(original_query)
print("Sub-queries:")
for sq in sub_queries:
    print(sq)

In this example:

The language model generates a list of sub-questions that focus on different aspects of the original query.


3. HyDE (Hypothetical Document Engine)

In this step, we take the original query and generate a hypothetical document based on the query. This document is like a detailed answer that you could get from an expert. This technique is useful when you want to simulate how a document answering the query might look. You can then use this document to guide further retrieval.

Example:

For the query, "How does deep learning help in image classification?", the hypothetical document might look like this:

> "Deep learning plays a key role in image classification by using convolutional neural networks (CNNs). CNNs allow deep learning models to identify key features in images automatically without manual feature extraction. This leads to improved accuracy and scalability compared to traditional machine learning models..."



This "hypothetical" document can then be used as a template to retrieve similar real documents from your database.

How to implement in Python:

Here’s how to use a language model to generate this hypothetical document:

# Function to generate a hypothetical document based on the query
def generate_hypothetical_document(query):
    prompt = f"Write a detailed document that answers the question: {query}"
    hypothetical_document = rephrase_pipeline(prompt, max_length=200, num_return_sequences=1)[0]['generated_text']
    return hypothetical_document.strip()

# Example usage
hypothetical_document = generate_hypothetical_document(original_query)
print("Hypothetical document:\n", hypothetical_document)

The language model generates a detailed explanation or document based on the original query. You can then use this generated document to help guide your retrieval system by searching for documents that resemble this hypothetical response.

4. Combine All Queries for Multi-Retrieval

Once you have the original query, the generalized query, sub-queries, and the hypothetical document, you can combine them to perform multi-query retrieval. This means you’re not relying on just one query to search for information; instead, you’re searching with multiple variations of the query to get more comprehensive results.

How to implement in Python:

Here’s how to combine all the queries into one list for retrieval:

# Combine all generated queries for retrieval
def combine_queries_for_retrieval(original_query, generalized_query, sub_queries):
    all_queries = [original_query, generalized_query] + sub_queries
    return all_queries

# Example usage
all_queries = combine_queries_for_retrieval(original_query, generalized_query, sub_queries)
print("All queries for multi-query retrieval:")
for q in all_queries:
    print(q)

Full Example Summary:

from transformers import pipeline

# Initialize a language model (GPT) to generate rephrased queries
rephrase_pipeline = pipeline('text-generation', model='gpt-2')

# Step 1: Step-Back Prompting
def step_back_prompting(query):
    prompt = f"Create a more general question based on this specific query: {query}"
    generalized_query = rephrase_pipeline(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']
    return generalized_query.strip()

# Step 2: Multi-Query Retrieval (Generate sub-questions)
def generate_sub_questions(query):
    sub_question_prompt = f"Generate sub-questions or variations for this query: {query}"
    sub_questions = rephrase_pipeline(sub_question_prompt, max_length=100, num_return_sequences=3)
    return [sq['generated_text'].strip() for sq in sub_questions]

# Step 3: HyDE (Generate hypothetical document)
def generate_hypothetical_document(query):
    hypothetical_prompt = f"Write a detailed document that answers the question: {query}"
    hypothetical_document = rephrase_pipeline(hypothetical_prompt, max_length=200, num_return_sequences=1)[0]['generated_text']
    return hypothetical_document.strip()

# Step 4: Combine all queries for retrieval
def combine_queries_for_retrieval(original_query, generalized_query, sub_queries):
    all_queries = [original_query, generalized_query] + sub_queries
    return all_queries

# Example usage
original_query = "How does deep learning help in image classification?"
generalized_query = step_back_prompting(original_query)
sub_queries = generate_sub_questions(original_query)
hypothetical_document = generate_hypothetical_document(original_query)

all_queries = combine_queries_for_retrieval(original_query, generalized_query, sub_queries)

# Output results
print("Original query:", original_query)
print("Generalized query:", generalized_query)
print("Sub-queries:")
for sq in sub_queries:
    print(sq)

print("\nHypothetical document:\n", hypothetical_document)

Recap of Each Step:

1. Step-Back Prompting: Broadens the query to a more general level.


2. Multi-Query Retrieval: Generates sub-questions or variations of the original query to search more effectively.


3. HyDE: Generates a hypothetical document that mimics what an expert response would look like.


4. Combine Queries: Puts all these queries together for more comprehensive retrieval.



By using all these transformations, you improve the chances of retrieving more accurate and relevant documents. Let me know if you'd like further clarification or additional steps!











import pdfplumber
from sentence_transformers import SentenceTransformer
import numpy as np

# Step 1: Extract text from PDF
def extract_text_from_pdf(pdf_file_path):
    text = ""
    with pdfplumber.open(pdf_file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

# Step 2: Split text into chunks
def split_text_into_chunks(text, chunk_size=500):
    words = text.split()
    chunks = []
    current_chunk = []
    
    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= chunk_size:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

# Step 3: Generate embeddings
def generate_embeddings(chunks):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)
    return embeddings

# Step 4: Save embeddings to a file
def save_embeddings(embeddings, file_name='document_embeddings.npy'):
    np.save(file_name, embeddings)

# Example usage
pdf_file_path = 'your_document.pdf'
extracted_text = extract_text_from_pdf(pdf_file_path)
chunks = split_text_into_chunks(extracted_text, chunk_size=500)
embeddings = generate_embeddings(chunks)
save_embeddings(embeddings, 'document_embeddings.npy')








"Chaque mois, la Banque d'Algérie (BA) envoie un fichier DECAD des interdits de chéquier, contenant en moyenne 2 200 lignes. Ce fichier est non structuré, ce qui complique son traitement. Les noms des clients dans notre base de données sont souvent écrits différemment par rapport à ceux du fichier DECAD, avec parfois des inversions entre le prénom et le nom de famille. L'équipe Flux Domestique - Activité de marché procède à une recherche manuelle pour identifier les clients interdits de chéquier dans ce fichier. Ces informations sont fournies par la Centrale des Impayés via la transaction INTIER sur le système Atlas."






Chaque mois, la Banque d'Algérie (BA) transmet un fichier DECAD d'environ 2 200 lignes. L'équipe Flux Domestique effectue une vérification manuelle pour identifier les clients interdits de chéquier. Ces informations sont obtenues via la transaction INTIER sur le système Atlas, en consultation des données fournies par la Centrale des Impayés.



Chaque mois, la Banque d'Algérie (BA) envoie un fichier DECAD comportant en moyenne 2 200 lignes. L'équipe Flux Domestique - Activité de marché procède à une recherche manuelle afin d'identifier les clients interdits de chéquier dans ce fichier, ces informations étant fournies par la Centrale des Impayés à travers la transaction INTIER sur le système Atlas.



Chaque mois, la Banque d'Algérie (BA) envoie un fichier DECAD comportant en moyenne 2 200 lignes. L'équipe Flux Domestique - Activité de marché procède à une recherche manuelle afin d'identifier les clients interdits de chéquier dans ce fichier, ces informations étant fournies par la Centrale des Impayés à travers la transaction INTIER sur le système Atlas.


"Réécrivez la procédure existante en vous basant uniquement sur la documentation du nouvel outil, en vous assurant que le document final ait le même nombre de pages que l'original. Maintenez le même format et la même structure, et reproduisez le style d'écriture tout au long du document. Modifiez le contenu uniquement là où il y a des mises à jour ou des changements pertinents dans la documentation du nouvel outil. Par exemple, si le nom de l'outil change, remplacez simplement les noms, comme dans la phrase 'Nous utilisons l'outil Minsa pour les nouveaux clients', que vous devez modifier en 'Nous utilisons Nostra pour les nouveaux clients'. Si aucun changement n'est nécessaire, conservez l'écriture originale intacte. Assurez-vous que la procédure mise à jour soit claire, précise et cohérente avec les capacités du nouvel outil, tout en préservant la longueur et le style globaux du document original."



import os
import httpx
import json
from PyPDF2 import PdfReader
from fpdf import FPDF
from dotenv import load_dotenv
from httpx import OAuth2ClientCredentials

# Step 1: Load environment variables from .env file (if using dotenv)
load_dotenv()

# Set up OAuth2 and Azure API credentials
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID")
OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET")
OIDC_SCOPE = "genai-model"
APIGEE_ENDPOINT = "https://aifactory.api.staging.echonet/genai-model/v1"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
AZURE_AOAI_API_VERSION = "2024-02-15-preview"
AZURE_AOAI_API_KEY = os.getenv("AZURE_AOAI_API_KEY")

# Step 2: Set up the OAuth2 client with httpx
oauth2_httpxclient = httpx.Client(verify=False)

auth = OAuth2ClientCredentials(
    OIDC_ENDPOINT,
    client_id=OIDC_CLIENT_ID,
    client_secret=OIDC_CLIENT_SECRET,
    scope=OIDC_SCOPE,
    client=oauth2_httpxclient
)

# Step 3: Function to extract text from a PDF
def extract_pdf_text(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PdfReader(file)
        text = ''
        for page in reader.pages:
            text += page.extract_text()
        return text

# Step 4: Generate the prompt for GPT-4
def generate_prompt(old_procedure, new_tool_docs):
    prompt = f"""
    You are tasked with migrating an old tool's procedure to a new tool's process.
    Here is the procedure from the old tool:

    {old_procedure}

    Here is the documentation for the new tool:

    {new_tool_docs}

    Rewrite the procedure using the new tool's instructions and processes.
    """
    return prompt

# Step 5: Function to interact with Azure OpenAI GPT-4 via OAuth2 authentication
def update_procedure_with_gpt4(old_procedure, new_tool_docs):
    prompt = generate_prompt(old_procedure, new_tool_docs)

    # Create the HTTP request to Azure OpenAI via Apigee with OAuth2
    response = httpx.post(
        f"{APIGEE_ENDPOINT}/openai/deployments/{AZURE_ADAI_MODEL_DEPLOYMENT_NAME}/chat/completions?api-version={AZURE_AOAI_API_VERSION}",
        headers={"Authorization": f"Bearer {auth}", "Content-Type": "application/json"},
        json={
            "model": AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
            "messages": [
                {"role": "system", "content": "You are an assistant helping to rewrite procedures."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1500,
            "temperature": 0.7
        }
    )
    
    # Check for errors in response
    if response.status_code != 200:
        raise Exception(f"API request failed: {response.status_code} - {response.text}")

    updated_procedure = response.json()['choices'][0]['message']['content'].strip()
    return updated_procedure

# --- Character encoding fixes ---

# 1. Clean unsupported characters by replacing them with simpler ones
def clean_text(text):
    return text.replace("\u2013", "-")  # Replace en-dash with a regular dash

# 2. Strip non-ASCII characters as a fallback option
def remove_non_ascii(text):
    return ''.join([char if ord(char) < 128 else ' ' for char in text])

# 3. Use a Unicode-friendly font (DejaVuSans) with FPDF
def save_to_pdf(updated_text, output_pdf_path, font_path=None):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()

    # If a Unicode-friendly font is provided, use it
    if font_path:
        pdf.add_font('DejaVu', '', font_path, uni=True)
        pdf.set_font("DejaVu", size=12)
    else:
        pdf.set_font("Arial", size=12)

    # Split text by lines and add each line separately
    lines = updated_text.split('\n')

    for line in lines:
        # Handle empty lines
        if line.strip() == "":
            pdf.ln(10)
        else:
            pdf.multi_cell(0, 10, txt=line, align='L')

    pdf.output(output_pdf_path)

# --- Example usage ---

# Old and new procedure files
old_pdf_path = "old_procedures.pdf"
new_tool_docs = "Insert new tool's documentation here or load dynamically"

# Step 7: Extract text from old PDF
old_procedures = extract_pdf_text(old_pdf_path)

# Step 8: Use GPT-4 to update the procedures
updated_procedures = update_procedure_with_gpt4(old_procedures, new_tool_docs)

# Choose your encoding fix:
# cleaned_procedures = clean_text(updated_procedures)  # Option 1: Clean special characters
# ascii_procedures = remove_non_ascii(updated_procedures)  # Option 2: Strip non-ASCII characters

# Step 9: Save the updated procedures to a new PDF
output_pdf_path = "updated_procedures.pdf"
font_path = "/path/to/DejaVuSans.ttf"  # Adjust path to your font file

# Save using Unicode-supporting font or default Arial
save_to_pdf(updated_procedures, output_pdf_path, font_path=font_path)  # With Unicode support
# save_to_pdf(cleaned_procedures, output_pdf_path)  # With cleaned text
# save_to_pdf(ascii_procedures, output_pdf_path)  # With ASCII-only text

print(f"Updated procedures saved to {output_pdf_path}")






import os
import httpx
import json
from PyPDF2 import PdfReader
from fpdf import FPDF
from dotenv import load_dotenv
from azure.ai.openai import OpenAIClient
from azure.identity import ClientSecretCredential

# Load environment variables from .env file (if using dotenv)
load_dotenv()

# Set up OAuth2 and Azure API credentials
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID")
OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET")
OIDC_SCOPE = "genai-model"
APIGEE_ENDPOINT = "https://aifactory.api.staging.echonet/genai-model/v1"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
AZURE_AOAI_API_VERSION = "2024-02-15-preview"
AZURE_AOAI_API_KEY = os.getenv("AZURE_AOAI_API_KEY")

# Function to fetch OAuth2 token
def fetch_oauth2_token(client):
    try:
        response = client.post(
            OIDC_ENDPOINT,
            data={
                "grant_type": "client_credentials",
                "client_id": OIDC_CLIENT_ID,
                "client_secret": OIDC_CLIENT_SECRET,
                "scope": OIDC_SCOPE
            }
        )
        response.raise_for_status()  # Raise an error for bad responses
        return response.json()  # Return the token response as JSON
    except Exception as e:
        print(f"Error fetching token: {e}")
        return None

# Step 5: Function to interact with Azure OpenAI GPT-4 via OAuth2 authentication
def update_procedure_with_gpt4(old_procedure, new_tool_docs):
    prompt = generate_prompt(old_procedure, new_tool_docs)

    # Create an httpx client for OAuth2 token retrieval
    http_client = httpx.Client(verify=False)
    access_token = None

    # Loop to check if the token is retrieved from cache (simulated)
    for i in range(3):  # Retry fetching token up to 3 times
        if access_token is None:
            token_response = fetch_oauth2_token(http_client)
            if token_response:
                access_token = token_response.get("access_token")
                if access_token:
                    break  # Exit the loop if token is retrieved

    if not access_token:
        raise Exception("Failed to retrieve access token after multiple attempts.")

    # Create the Azure OpenAI client
    openai_client = OpenAIClient(
        api_version=AZURE_AOAI_API_VERSION,
        azure_endpoint=APIGEE_ENDPOINT,
        api_key=AZURE_AOAI_API_KEY,
        http_client=http_client
    )

    # Create the chat completion request
    completion = openai_client.chat.completions.create(
        model=AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
        messages=[
            {"role": "system", "content": "You are an assistant helping to rewrite procedures."},
            {"role": "user", "content": prompt}
        ]
    )

    updated_procedure = completion.choices[0].message.content.strip()
    return updated_procedure

# Example usage
old_pdf_path = "old_procedures.pdf"
new_tool_docs = "Insert new tool's documentation here or load dynamically"

# Step 7: Extract text from old PDF
old_procedures = import os
import httpx
from PyPDF2 import PdfReader
from fpdf import FPDF
from dotenv import load_dotenv
from azure.ai.openai import OpenAIClient

# Load environment variables from .env file
load_dotenv()

# Set up OAuth2 and Azure API credentials
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID")
OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET")
OIDC_SCOPE = "genai-model"
APIGEE_ENDPOINT = "https://aifactory.api.staging.echonet/genai-model/v1"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
AZURE_AOAI_API_VERSION = "2024-02-15-preview"
AZURE_AOAI_API_KEY = os.getenv("AZURE_AOAI_API_KEY")

# Function to fetch OAuth2 token
def fetch_oauth2_token(client):
    try:
        response = client.post(
            OIDC_ENDPOINT,
            data={
                "grant_type": "client_credentials",
                "client_id": OIDC_CLIENT_ID,
                "client_secret": OIDC_CLIENT_SECRET,
                "scope": OIDC_SCOPE
            }
        )
        response.raise_for_status()
        return response.json()  # Return the token response as JSON
    except Exception as e:
        print(f"Error fetching token: {e}")
        return None

# Step 5: Function to interact with Azure OpenAI GPT-4 via OAuth2 authentication
def update_procedure_with_gpt4(old_procedure, new_tool_docs):
    prompt = generate_prompt(old_procedure, new_tool_docs)

    # Create an httpx client for OAuth2 token retrieval
    http_client = httpx.Client(verify=False)
    access_token = None

    # Loop to check if the token is retrieved from cache
    for i in range(1, 4):
        # Attempt to fetch the OAuth2 token
        token_response = fetch_oauth2_token(http_client)
        if token_response:
            access_token = token_response.get("access_token")
            if access_token:
                print(f"Access token retrieved on attempt {i}")
                break  # Exit the loop if the token is retrieved
        print(f"Attempt {i} to retrieve access token failed.")

    if not access_token:
        raise Exception("Failed to retrieve access token after multiple attempts.")

    # Initialize the Azure OpenAI client with the retrieved token
    azure_openai_client = OpenAIClient(
        api_version=AZURE_AOAI_API_VERSION,
        azure_endpoint=APIGEE_ENDPOINT,
        api_key=AZURE_AOAI_API_KEY,
        http_client=httpx.Client(auth=access_token, verify=False)  # Use access token for auth
    )

    # Create the chat completion request
    completion = azure_openai_client.chat.completions.create(
        model=AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )

    updated_procedure = completion.choices[0].message.content.strip()
    print(updated_procedure)  # Print the updated procedure
    return updated_procedure

# Function to extract text from a PDF
def extract_pdf_text(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PdfReader(file)
        text = ''
        for page in reader.pages:
            text += page.extract_text()
        return text

# Generate the prompt for GPT-4
def generate_prompt(old_procedure, new_tool_docs):
    prompt = f"""
    You are tasked with migrating an old tool's procedure to a new tool's process.
    Here is the procedure from the old tool:

    {old_procedure}

    Here is the documentation for the new tool:

    {new_tool_docs}

    Rewrite the procedure using the new tool's instructions and processes.
    """
    return prompt

# Function to save updated text into a PDF
def save_to_pdf(updated_text, output_pdf_path):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=12)

    # Add updated procedure line by line
    for line in updated_text.split('\n'):
        pdf.cell(200, 10, txt=line, ln=True, align='L')

    pdf.output(output_pdf_path)

# Example usage
old_pdf_path = "old_procedures.pdf"
new_tool_docs = "Insert new tool's documentation here or load dynamically"

# Step 7: Extract text from old PDF
old_procedures = extract_pdf_text(old_pdf_path)

# Step 8: Use GPT-4 to update the procedures
updated_procedures = update_procedure_with_gpt4(old_procedures, new_tool_docs)

# Step 9: Save the updated procedures to a new PDF
output_pdf_path = "/mnt/LMMS/updated_procedures.pdf"
save_to_pdf(updated_procedures, output_pdf_path)

print(f"Updated procedures saved to {output_pdf_path}")




extract_pdf_text(old_pdf_path)

# Step 8: Use GPT-4 to update the procedures
updated_procedures = update_procedure_with_gpt4(old_procedures, new_tool_docs)

# Step 9: Save the updated procedures to a new PDF
output_pdf_path = "/mnt/LMMS/updated_procedures.pdf"
save_to_pdf(updated_procedures, output_pdf_path)

print(f"Updated procedures saved to {output_pdf_path}")


# Step 2: Set up the OAuth2 client with httpx
oauth2_httpxclient = httpx.Client(verify=False)

# Step 3: Function to fetch OAuth2 token
def fetch_oauth2_token():
    response = oauth2_httpxclient.post(
        OIDC_ENDPOINT,
        data={
            "grant_type": "client_credentials",
            "client_id": OIDC_CLIENT_ID,
            "client_secret": OIDC_CLIENT_SECRET,
            "scope": OIDC_SCOPE
        }
    )
    response.raise_for_status()  # Raise an error for bad responses
    return response.json()  # Return the token response as JSON

# Step 5: Function to interact with Azure OpenAI GPT-4 via OAuth2 authentication
def update_procedure_with_gpt4(old_procedure, new_tool_docs):
    prompt = generate_prompt(old_procedure, new_tool_docs)

    # Fetch OAuth2 token
    token_response = fetch_oauth2_token()  # Fetch OAuth2 token using the new function
    access_token = token_response["access_token"]  # Extract the access token

    # Create the HTTP request to Azure OpenAI via Apigee with OAuth2
    response = httpx.post(
        f"{APIGEE_ENDPOINT}/openai/deployments/{AZURE_ADAI_MODEL_DEPLOYMENT_NAME}/chat/completions?api-version={AZURE_AOAI_API_VERSION}",
        headers={"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"},
        json={
            "model": AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
            "messages": [
                {"role": "system", "content": "You are an assistant helping to rewrite procedures."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1500,
            "temperature": 0.7
        }
    )
    
    # Check for errors in response
    if response.status_code != 200:
        raise Exception(f"API request failed: {response.status_code} - {response.text}")

    updated_procedure = response.json()['choices'][0]['message']['content'].strip()
    return updated_procedure


import os
import httpx
import json
from PyPDF2 import PdfReader
from fpdf import FPDF
from dotenv import load_dotenv
from httpx import OAuth2ClientCredentials

# Step 1: Load environment variables from .env file (if using dotenv)
load_dotenv()

# Set up OAuth2 and Azure API credentials
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID")
OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET")
OIDC_SCOPE = "genai-model"
APIGEE_ENDPOINT = "https://aifactory.api.staging.echonet/genai-model/v1"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
AZURE_AOAI_API_VERSION = "2024-02-15-preview"
AZURE_AOAI_API_KEY = os.getenv("AZURE_AOAI_API_KEY")

# Step 2: Set up the OAuth2 client with httpx
oauth2_httpxclient = httpx.Client(verify=False)

auth = OAuth2ClientCredentials(
    OIDC_ENDPOINT,
    client_id=OIDC_CLIENT_ID,
    client_secret=OIDC_CLIENT_SECRET,
    scope=OIDC_SCOPE,
    client=oauth2_httpxclient
)

# Step 3: Function to extract text from a PDF
def extract_pdf_text(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PdfReader(file)
        text = ''
        for page in reader.pages:
            text += page.extract_text()
        return text

# Step 4: Generate the prompt for GPT-4
def generate_prompt(old_procedure, new_tool_docs):
    prompt = f"""
    You are tasked with migrating an old tool's procedure to a new tool's process.
    Here is the procedure from the old tool:

    {old_procedure}

    Here is the documentation for the new tool:

    {new_tool_docs}

    Rewrite the procedure using the new tool's instructions and processes.
    """
    return prompt

# Step 5: Function to interact with Azure OpenAI GPT-4 via OAuth2 authentication
def update_procedure_with_gpt4(old_procedure, new_tool_docs):
    prompt = generate_prompt(old_procedure, new_tool_docs)

    # Fetch OAuth2 token
    token_response = auth.fetch_token()  # Fetch OAuth2 token
    access_token = token_response["access_token"]  # Extract the access token

    # Create the HTTP request to Azure OpenAI via Apigee with OAuth2
    response = httpx.post(
        f"{APIGEE_ENDPOINT}/openai/deployments/{AZURE_ADAI_MODEL_DEPLOYMENT_NAME}/chat/completions?api-version={AZURE_AOAI_API_VERSION}",
        headers={"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"},
        json={
            "model": AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
            "messages": [
                {"role": "system", "content": "You are an assistant helping to rewrite procedures."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1500,
            "temperature": 0.7
        }
    )
    
    # Check for errors in response
    if response.status_code != 200:
        raise Exception(f"API request failed: {response.status_code} - {response.text}")

    updated_procedure = response.json()['choices'][0]['message']['content'].strip()
    return updated_procedure

# Step 6: Function to save updated text into a PDF
def save_to_pdf(updated_text, output_pdf_path):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=12)

    # Add updated procedure line by line
    for line in updated_text.split('\n'):
        pdf.cell(200, 10, txt=line, ln=True, align='L')

    pdf.output(output_pdf_path)

# Example usage
old_pdf_path = "old_procedures.pdf"
new_tool_docs = "Insert new tool's documentation here or load dynamically"

# Step 7: Extract text from old PDF
old_procedures = extract_pdf_text(old_pdf_path)

# Step 8: Use GPT-4 to update the procedures
updated_procedures = update_procedure_with_gpt4(old_procedures, new_tool_docs)

# Step 9: Save the updated procedures to a new PDF
output_pdf_path = "updated_procedures.pdf"
save_to_pdf(updated_procedures, output_pdf_path)

print(f"Updated procedures saved to {output_pdf_path}")




import os
import httpx
import json
from PyPDF2 import PdfReader
from fpdf import FPDF
from dotenv import load_dotenv
from httpx import OAuth2ClientCredentials

# Step 1: Load environment variables from .env file (if using dotenv)
load_dotenv()

# Set up OAuth2 and Azure API credentials
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID")
OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET")
OIDC_SCOPE = "genai-model"
APIGEE_ENDPOINT = "https://aifactory.api.staging.echonet/genai-model/v1"
AZURE_ADAI_MODEL_DEPLOYMENT_NAME = "gpt4turbo"
AZURE_AOAI_API_VERSION = "2024-02-15-preview"
AZURE_AOAI_API_KEY = os.getenv("AZURE_AOAI_API_KEY")

# Step 2: Set up the OAuth2 client with httpx
oauth2_httpxclient = httpx.Client(verify=False)

auth = OAuth2ClientCredentials(
    OIDC_ENDPOINT,
    client_id=OIDC_CLIENT_ID,
    client_secret=OIDC_CLIENT_SECRET,
    scope=OIDC_SCOPE,
    client=oauth2_httpxclient
)

# Step 3: Function to extract text from a PDF
def extract_pdf_text(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PdfReader(file)
        text = ''
        for page in reader.pages:
            text += page.extract_text()
        return text

# Step 4: Generate the prompt for GPT-4
def generate_prompt(old_procedure, new_tool_docs):
    prompt = f"""
    You are tasked with migrating an old tool's procedure to a new tool's process.
    Here is the procedure from the old tool:

    {old_procedure}

    Here is the documentation for the new tool:

    {new_tool_docs}

    Rewrite the procedure using the new tool's instructions and processes.
    """
    return prompt

# Step 5: Function to interact with Azure OpenAI GPT-4 via OAuth2 authentication
def update_procedure_with_gpt4(old_procedure, new_tool_docs):
    prompt = generate_prompt(old_procedure, new_tool_docs)

    # Create the HTTP request to Azure OpenAI via Apigee with OAuth2
    response = httpx.post(
        f"{APIGEE_ENDPOINT}/openai/deployments/{AZURE_ADAI_MODEL_DEPLOYMENT_NAME}/chat/completions?api-version={AZURE_AOAI_API_VERSION}",
        headers={"Authorization": f"Bearer {auth}", "Content-Type": "application/json"},
        json={
            "model": AZURE_ADAI_MODEL_DEPLOYMENT_NAME,
            "messages": [
                {"role": "system", "content": "You are an assistant helping to rewrite procedures."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1500,
            "temperature": 0.7
        }
    )
    
    # Check for errors in response
    if response.status_code != 200:
        raise Exception(f"API request failed: {response.status_code} - {response.text}")

    updated_procedure = response.json()['choices'][0]['message']['content'].strip()
    return updated_procedure

# Step 6: Function to save updated text into a PDF
def save_to_pdf(updated_text, output_pdf_path):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=12)

    # Add updated procedure line by line
    for line in updated_text.split('\n'):
        pdf.cell(200, 10, txt=line, ln=True, align='L')

    pdf.output(output_pdf_path)

# Example usage
old_pdf_path = "old_procedures.pdf"
new_tool_docs = "Insert new tool's documentation here or load dynamically"

# Step 7: Extract text from old PDF
old_procedures = extract_pdf_text(old_pdf_path)

# Step 8: Use GPT-4 to update the procedures
updated_procedures = update_procedure_with_gpt4(old_procedures, new_tool_docs)

# Step 9: Save the updated procedures to a new PDF
output_pdf_path = "updated_procedures.pdf"
save_to_pdf(updated_procedures, output_pdf_path)

print(f"Updated procedures saved to {output_pdf_path}")





import openai
import PyPDF2
from fpdf import FPDF

# Step 1: Set up Azure OpenAI API credentials
openai.api_type = "azure"
openai.api_base = "https://YOUR_AZURE_OPENAI_ENDPOINT/"
openai.api_version = "2023-03-15-preview"
openai.api_key = "YOUR_AZURE_OPENAI_API_KEY"

# Step 2: Function to extract text from a PDF
def extract_pdf_text(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ''
        for page in reader.pages:
            text += page.extract_text()
        return text

# Step 3: GPT-4 prompt generation for procedure rewriting
def generate_prompt(old_procedure, new_tool_docs):
    prompt = f"""
    You are tasked with migrating an old tool's procedure to a new tool's process.
    Here is the procedure from the old tool:

    {old_procedure}

    Here is the documentation for the new tool:

    {new_tool_docs}

    Rewrite the procedure using the new tool's instructions and processes.
    """
    return prompt

# Step 4: Function to interact with GPT-4 and get the updated procedures
def update_procedure_with_gpt4(old_procedure, new_tool_docs):
    prompt = generate_prompt(old_procedure, new_tool_docs)
    
    response = openai.Completion.create(
        engine="gpt-4",  # Use "gpt-4" engine on Azure OpenAI
        prompt=prompt,
        max_tokens=1500,
        n=1,
        stop=None,
        temperature=0.7
    )
    
    updated_procedure = response['choices'][0]['text'].strip()
    return updated_procedure

# Step 5: Function to save updated text into a PDF
def save_to_pdf(updated_text, output_pdf_path):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=12)

    # Add updated procedure line by line
    for line in updated_text.split('\n'):
        pdf.cell(200, 10, txt=line, ln=True, align='L')

    pdf.output(output_pdf_path)

# Example usage
old_pdf_path = "old_procedures.pdf"
new_tool_docs = "Insert new tool's documentation here or load dynamically"

# Step 6: Extract text from old PDF
old_procedures = extract_pdf_text(old_pdf_path)

# Step 7: Use GPT-4 to update the procedures
updated_procedures = update_procedure_with_gpt4(old_procedures, new_tool_docs)

# Step 8: Save the updated procedures to a new PDF
output_pdf_path = "updated_procedures.pdf"
save_to_pdf(updated_procedures, output_pdf_path)

print(f"Updated procedures saved to {output_pdf_path}")




import pdfplumber
import openai
from PyPDF2 import PdfReader, PdfWriter

# Step 1: Extract Text from PDFs
def extract_pdf_text(pdf_path):
    """
    Extracts the text from a PDF file.
    
    :param pdf_path: Path to the PDF file
    :return: Extracted text as a string
    """
    with pdfplumber.open(pdf_path) as pdf:
        full_text = ''
        for page in pdf.pages:
            full_text += page.extract_text()
    return full_text

# Step 2: Set up Azure OpenAI API
openai.api_type = "azure"
openai.api_base = "https://<your-resource-name>.openai.azure.com/"  # Replace with your Azure resource URL
openai.api_version = "2023-03-15-preview"
openai.api_key = "<your-azure-openai-api-key>"  # Replace with your Azure API key

def adjust_modifications_with_documentation(target_text, documentation_text):
    """
    Uses GPT-4 to suggest modifications to a target PDF based on a documentation PDF.

    :param target_text: Text from the target PDF
    :param documentation_text: Text from the documentation PDF
    :return: Suggested modifications from GPT-4
    """
    prompt = (
        f"Based on the following documentation, suggest modifications for the target contract "
        f"to ensure compliance and alignment with best practices:\n\n"
        f"Documentation:\n{documentation_text}\n\n"
        f"Target Contract:\n{target_text}\n\n"
        f"Please suggest the necessary changes."
    )
    
    response = openai.Completion.create(
        engine="gpt-4",  # You can also use gpt-35-turbo if you prefer
        prompt=prompt,
        temperature=0.5,
        max_tokens=1500
    )
    return response.choices[0].text.strip()

# Step 3: Modify PDF Based on GPT-4 Suggestions
def apply_modifications_to_pdf(pdf_path, output_path, suggestions):
    """
    Adds the GPT-4 suggestions to the PDF as metadata or applies modifications.
    
    :param pdf_path: Path to the target PDF
    :param output_path: Path to save the modified PDF
    :param suggestions: Text containing modification suggestions
    """
    reader = PdfReader(pdf_path)
    writer = PdfWriter()

    # Add the suggestions as metadata (you can modify this to add annotations or comments)
    writer.add_metadata({
        '/Modifications_Suggestions': suggestions
    })

    for page in reader.pages:
        writer.add_page(page)

    with open(output_path, 'wb') as f:
        writer.write(f)

    print(f"Modified PDF saved at: {output_path}")

# Step 4: Main Workflow
def main(target_pdf_path, documentation_pdf_path, output_pdf_path):
    """
    Orchestrates the process of extracting text, sending to GPT-4, and applying the modifications.

    :param target_pdf_path: Path to the target PDF to be modified
    :param documentation_pdf_path: Path to the reference documentation PDF
    :param output_pdf_path: Path to save the modified PDF
    """
    # Extract text from the target PDF and the documentation PDF
    target_text = extract_pdf_text(target_pdf_path)
    documentation_text = extract_pdf_text(documentation_pdf_path)

    # Get modification suggestions from GPT-4
    suggestions = adjust_modifications_with_documentation(target_text, documentation_text)
    print("GPT-4 Suggestions:", suggestions)

    # Apply modifications or add comments to the PDF
    apply_modifications_to_pdf(target_pdf_path, output_pdf_path, suggestions)

# Example usage
if __name__ == "__main__":
    # Replace these with the actual paths to your files
    target_pdf_path = "target_contract.pdf"  # Target PDF file
    documentation_pdf_path = "documentation.pdf"  # Documentation PDF file
    output_pdf_path = "modified_contract_with_suggestions.pdf"  # Output file

    main(target_pdf_path, documentation_pdf_path, output_pdf_path)



Inaccurate Identification: AI may incorrectly match names


This use case involves implementing an AI-based system using NLP to compare the similarity between names and surnames from Atlas' customer database and the BA cheque defaulters blacklist. We will verify if it is the same individual by checking the date of birth (or date of creation for companies) and the place of birth. The results will be compiled into an Excel file, with one sheet for matches with a similarity score ≥ 95% and another for scores < 95%. This ensures a thorough verification process, reducing false matches and accurately identifying defaulters.




This use case involves implementing an AI-based system using NLP to compare the similarity between names and surnames from Atlas' customer database and the BA cheque defaulters blacklist. When a potential match is identified, the system will confirm if it is the same individual by verifying the date of birth (or date of creation for companies) and the place of birth. This approach ensures a thorough verification process, reducing false positives and accurately identifying defaulters.




https://forms.office.com/Pages/ResponsePage.aspx?id=JZxPYfq_x0KG2JZBAfVfom6FMzcQ6YZGjty1QNO8NZtUQzJUWjdWUVZHNE42M00yR0FKUlVBMkJXOC4u&origin=QRCode&qrcodeorigin=presentation



Objet : Version finale du quiz

Bonjour [Nom du Manager],

Voici la version finale du quiz. Vous pouvez également consulter la vidéo "DECAD AI" en suivant ce lien : [chemin/URL].

Cordialement,
Nihad




Avez-vous une idée du nombre de lignes traitées mensuellement pour le flux domestique, provenant du fichier DECAD envoyé par la BA ?


DECAD traite les interdits de chéquier par la vérification manuelle des clients signalés comme interdits de chéquier

"Savez-vous combien de fois la Banque d'Algérie envoie les fichiers des interdits de chéquiers ?"



"Savez-vous combien de temps il faut pour traiter les fichiers des interdits de chéquiers par le flux Domestique ?"

"Savez-vous combien de temps la Banque d'Algérie met pour envoyer les fichiers des interdits de chéquiers ?"



[Scène 1 : Ouverture - Écran titre]
Texte à l'écran : "Révolutionner les processus manuels avec l'IA chez BNP Paribas El Djazair"

[Scène 2 : Présentation du problème - Visuel du traitement manuel des données]
Narrateur : "Chaque mois, nous traitons plus de 2 200 enregistrements, en vérifiant manuellement les clients interdits de chéquiers. Ce processus est long et comporte un risque élevé d'erreurs à cause des variations de noms et des homonymes."

[Scène 3 : Présentation de la solution IA - Visuel du traitement des données par l'IA]
Narrateur : "Avec notre solution d'IA, DECAD, alimentée par le traitement avancé du langage naturel, nous avons automatisé ce processus complexe. L'IA identifie les similarités phonétiques, même avec des variations de noms."

[Scène 4 : Les avantages - Visuel d'un tableau de bord montrant les gains d'efficacité]
Narrateur : "Cette solution réduit le travail manuel de 93%, augmente la précision, et garantit la conformité avec les réglementations bancaires—le tout en seulement 42 jours/homme."

[Scène 5 : Conclusion - Logo de la banque et slogan]
Narrateur : "L'IA chez BNP Paribas El Djazair : transformer les tâches manuelles pour un avenir plus 

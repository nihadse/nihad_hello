import datetime
import pandas as pd

# Log the start time
print(datetime.datetime.now())

# Assuming Train_data is your DataFrame with 100 Arabic names
batch_size = 5
total_names = len(Train_data)

for i in range(0, total_names, batch_size):
    # Get the current batch
    df = Train_data[i:i + batch_size]
    
    # Drop rows with missing Arabic names
    df = df.dropna(subset=['name_ar'])
    
    # Generate and display transliterations
    transliterations = []
    for index, row in df.iterrows():
        name = row['name_ar']
        variations = get_name_transliterations(name)  # Assuming this function is defined elsewhere
        transliterations.append({"name_ar": name, "French Transliterations": variations})
    
    # Save the transliterations to an Excel file
    df_transliterations = pd.DataFrame(transliterations)
    excel_file_name = f'name_transliterations_batch_{i//batch_size + 1}.xlsx'
    df_transliterations.to_excel(excel_file_name, index=False)

# Log the end time
print(datetime.datetime.now())





import pandas as pd
import datetime

# Function to simulate getting transliterations for a batch of names
def get_name_transliterations_batch(names):
    """
    Generate transliterations for a batch of names.
    Simulate the process with your actual implementation for Azure OpenAI.
    """
    # For simplicity, just append mock transliterations for now
    return [f"Transliteration {i}" for i in range(1, len(names) + 1)]

# Load and clean data
print(datetime.datetime.now())
df = Train_data.dropna(subset=['name_ar'])

# Initialize results list
transliterations = []

# Process in batches of 5
batch_size = 5
for i in range(0, len(df), batch_size):
    batch = df['name_ar'].iloc[i:i + batch_size].tolist()  # Get a batch of 5 names
    batch_transliterations = get_name_transliterations_batch(batch)  # Call the API for the batch

    # Append results for the batch
    for name, variations in zip(batch, batch_transliterations):
        transliterations.append({"name_ar": name, "French Transliterations": variations})

# Save results to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterationsV8_batch5.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

print(datetime.datetime.now())




import pandas as pd
import datetime

# Function to simulate getting transliterations for a batch of names
def get_name_transliterations_batch(names):
    """
    Generate transliterations for a batch of names.
    Simulate the process with your actual implementation for Azure OpenAI.
    """
    # For simplicity, just append mock transliterations for now
    return [f"Transliteration {i}" for i in range(1, len(names) + 1)]

# Load and clean data
print(datetime.datetime.now())
df = Train_data.dropna(subset=['name_ar'])

# Initialize results list
transliterations = []

# Process in batches of 5
batch_size = 5
for i in range(0, len(df), batch_size):
    batch = df['name_ar'].iloc[i:i + batch_size].tolist()  # Get a batch of 5 names
    batch_transliterations = get_name_transliterations_batch(batch)  # Call the API for the batch

    # Append results for the batch
    for name, variations in zip(batch, batch_transliterations):
        transliterations.append({"name_ar": name, "French Transliterations": variations})

# Save results to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterationsV8_batch5.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

print(datetime.datetime.now())




variations = response.choices[0].message.content.split('\n') formatted_variations = ' '.join([f"{i+1}. {variation.strip()}" for i, variation in enumerate(variations) if variation.strip()]) return formatted_variations




import pandas as pd
import openai
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Function to get French transliterations of Arabic names
def get_name_transliterations(name_ar, num_variations=5):
    prompt = f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. Make sure the variations are realistic."
    response = client.chat.completions.create(
        model='your-model-deployment-name',  # Replace with your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7
    )
    variations = response.choices[0].message.content.split('\n')
    formatted_variations = '\n'.join([f"{i+1}. {variation.strip()}" for i, variation in enumerate(variations)])
    return formatted_variations

# Load the CSV file
csv_file_name = 'arabic_names.csv'
df = pd.read_csv(csv_file_name)

# Generate and display transliterations
transliterations = []
for index, row in df.iterrows():
    name = row['Arabic Name']
    variations = get_name_transliterations(name)
    transliterations.append({"Arabic Name": name, "French Transliterations": variations})
    print(f"Arabic Name: {name} -> French Transliterations: \n{variations}\n")

# Save the transliterations to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterations.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

# Upload the Excel file to Azure OpenAI
try:
    with open(excel_file_name, "rb") as file:
        training_response = client.files.create(file=file, purpose="fine-tune")
    training_file_id = training_response.id
    print("Training file ID:", training_file_id)
except Exception as e:
    print(f"Error uploading training file: {e}")


















































import pandas as pd
import openai
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Function to get French transliterations of Arabic names
def get_name_transliterations(name_ar, num_variations=5):
    prompt = f"Generate {num_variations} French transliterations of the Arabic name '{name_ar}'. Make sure the variations are realistic."
    response = client.chat.completions.create(
        model='your-model-deployment-name',  # Replace with your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7
    )
    return response.choices[0].message.content

# Load the CSV file
csv_file_name = 'arabic_names.csv'
df = pd.read_csv(csv_file_name)

# Generate and display transliterations
transliterations = []
for index, row in df.iterrows():
    name = row['Arabic Name']
    variations = get_name_transliterations(name)
    transliterations.append({"Arabic Name": name, "French Transliterations": variations})
    print(f"Arabic Name: {name} -> French Transliterations: \n{variations}\n")

# Save the transliterations to an Excel file
df_transliterations = pd.DataFrame(transliterations)
excel_file_name = 'name_transliterations.xlsx'
df_transliterations.to_excel(excel_file_name, index=False)

# Upload the Excel file to Azure OpenAI
try:
    with open(excel_file_name, "rb") as file:
        training_response = client.files.create(file=file, purpose="fine-tune")
    training_file_id = training_response.id
    print("Training file ID:", training_file_id)
except Exception as e:
    print(f"Error uploading training file: {e}")






# Upload the training dataset file to Azure OpenAI try: with open(training_file_name, "rb") as file: training_response = client.files.create(file=file, purpose="fine-tune") training_file_id = training_response.id print("Training file ID:", training_file_id) except Exception as e: print(f"Error uploading training file: {e}")



import pandas as pd
import openai
import json
import os
from openai import AzureOpenAI

# Set your Azure OpenAI API key and endpoint
client = AzureOpenAI(
  azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"), 
  api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
  api_version="2024-05-01-preview"  # This API version or later is required to access seed/events/checkpoint capabilities
)

# Create a DataFrame with Arabic names and their French translations
data = {
    'Arabic Name': ['محمد علي', 'فاطمة الزهراء', 'يوسف بن تاشفين'],
    'French Name': ['Mohamed Ali', 'Fatima Zahra', 'Youssef Ben Tachfine']
}
df = pd.DataFrame(data)

# Convert the DataFrame to JSONL format
training_data = []
for index, row in df.iterrows():
    training_data.append({
        "prompt": f"Translate the following Arabic name to French: '{row['Arabic Name']}'",
        "completion": row['French Name']
    })

# Save the training data to a JSONL file
training_file_name = 'training_data.jsonl'
with open(training_file_name, 'w') as f:
    for entry in training_data:
        f.write(json.dumps(entry) + '\n')

# Upload the training dataset file to Azure OpenAI
training_response = client.files.create(
    file=open(training_file_name, "rb"), purpose="fine-tune"
)
training_file_id = training_response.id

# Print the training file ID
print("Training file ID:", training_file_id)

# Define the new Arabic name and family name
arabic_name = "علي بن أبي طالب"

# Create the prompt for translation
prompt = f"Translate the following Arabic name to French: '{arabic_name}'"

# Make a request to the fine-tuned model
response = openai.Completion.create(
    model='chatgpt-turbo',
    prompt=prompt,
    max_tokens=50
)

# Extract and print the translation
translation = response.choices[0].text.strip()
print(f"Translation: {translation}")




Fine-tuning a model involves taking a pre-trained model and training it further on a specific dataset to adapt it to a particular task or domain. This process helps the model learn the nuances and specifics of the new data, improving its performance on the desired task. Fine-tuning is often used to customize models for specialized applications without needing to train a model from scratch.




import pandas as pd
import openai
import json

# Set your OpenAI API key
openai.api_key = 'your-api-key'

# Create a DataFrame with Arabic names and their French translations
data = {
    'Arabic Name': ['محمد علي', 'فاطمة الزهراء', 'يوسف بن تاشفين'],
    'French Name': ['Mohamed Ali', 'Fatima Zahra', 'Youssef Ben Tachfine']
}
df = pd.DataFrame(data)

# Convert the DataFrame to JSONL format
training_data = []
for index, row in df.iterrows():
    training_data.append({
        "prompt": f"Translate the following Arabic name to French: '{row['Arabic Name']}'",
        "completion": row['French Name']
    })

# Save the training data to a JSONL file
with open('training_data.jsonl', 'w') as f:
    for entry in training_data:
        f.write(json.dumps(entry) + '\n')

# Fine-tune the model
response = openai.FineTune.create(
    training_file=openai.File.create(file=open('training_data.jsonl'), purpose='fine-tune').id,
    model='chatgpt-turbo'
)

# Print the fine-tuning job ID
print(f"Fine-tuning job ID: {response['id']}")

# Define the new Arabic name and family name
arabic_name = "علي بن أبي طالب"

# Create the prompt for translation
prompt = f"Translate the following Arabic name to French: '{arabic_name}'"

# Make a request to the fine-tuned model
response = openai.Completion.create(
    model='chatgpt-turbo',
    prompt=prompt,
    max_tokens=50
)

# Extract and print the translation
translation = response.choices[0].text.strip()
print(f"Translation: {translation}")

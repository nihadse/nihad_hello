import openai
import pandas as pd
import json
import httpx
from openai import oauth2
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments

# Azure setup variables
OIDC_CLIENT_ID = "HP102kBeK3e3bicundGc32кмухокна"
OIDC_CLIENT_SECRET = "HEEU1PPiaZoi6ys"
OIDC_ENDPOINT = "https://aifactory.api.staging.echonet/auth/oauth2/v2/token"
APIGEE_ENDPOINT = "https://alfactory.api.staging.echonet/genal-model/v1"
AZURE_AGAI_MODEL_DEPLOYMENT_NAME = "gpt-3.5-turbo"  # GPT-3 model name for Azure
AZURE_ADAI_API_VERSION = "2024-02-15-preview"
AZURE_ADAI_API_KEY = "FAKE_KEY"
AZURE_ADAI_EMBEDDING_DEPLOYMENT_NAME = "text-embedding-ada"

# OAuth2 client setup for Azure OpenAI API authentication
auth = oauth2.OAuth2ClientCredentials(
    OIDC_ENDPOINT, OIDC_CLIENT_ID, OIDC_CLIENT_SECRET, scope="genai model", client=httpx.Client(verify=False)
)

# Initialize the API client
client = openai.ChatCompletion.create(
    api_key=AZURE_ADAI_API_KEY,
    api_base=APIGEE_ENDPOINT,
    api_version=AZURE_ADAI_API_VERSION,
    model=AZURE_AGAI_MODEL_DEPLOYMENT_NAME,
    http_client=httpx.Client(auth=auth, verify=False)
)

# Load the dataset from the Excel file
df = pd.read_excel("path_to_your_file.xlsx")

# Prepare the dataset for fine-tuning
name_pairs = []
for _, row in df.iterrows():
    # Create prompt-completion pairs from Arabic and French names
    prompt = f"Arabic: {row['name_ar']} {row['family_name_ar']}\nFrench transliterations:"
    completion = f" {row['name_fr']} {row['family_name_fr']}"
    
    name_pairs.append({
        "prompt": prompt,
        "completion": completion
    })

# Save the dataset as a JSONL file for fine-tuning
with open('name_transliterations.jsonl', 'w', encoding='utf-8') as f:
    for pair in name_pairs:
        json.dump(pair, f, ensure_ascii=False)
        f.write('\n')

# Fine-tuning the GPT-3 model using Azure OpenAI
def fine_tune_model():
    response = openai.FineTune.create(
        training_file='name_transliterations.jsonl',
        model="gpt-3.5-turbo",  # Fine-tune GPT-3.5 Turbo model
    )
    print(f"Fine-tuning started: {response}")

fine_tune_model()

# Monitor fine-tuning status
def check_fine_tune_status():
    status = openai.FineTune.retrieve(id="fine-tune-id")  # Use actual fine-tune ID
    print(f"Fine-tuning status: {status}")

check_fine_tune_status()

# Function to generate French transliterations for Arabic names
def generate_french_variations(name_arabic):
    prompt = f"Arabic: {name_arabic}\nFrench transliterations:"
    response = client.chat.completions.create(
        model=AZURE_AGAI_MODEL_DEPLOYMENT_NAME,  # Fine-tuned model
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.7,
        n=5  # Generate 5 variations
    )

    for i, choice in enumerate(response.choices):
        print(f"Variation {i + 1}: {choice['text'].strip()}")

# Example: Generate variations for an Arabic name
generate_french_variations("محمد")






import pandas as pd
import json
import openai

# Step 1: Read the dataset from the Excel file
# Make sure to provide the correct path to your Excel file
df = pd.read_excel('path_to_your_file.xlsx')  # Replace with the actual path to your Excel file

# Step 2: Prepare the dataset for fine-tuning
name_pairs = []
for _, row in df.iterrows():
    # Create prompt and completion pairs from Arabic and French names
    prompt = f"Arabic Name: {row['name_ar']} {row['family_name_ar']}\nFrench transliterations:"
    completion = f" {row['name_fr']} {row['family_name_fr']}"
    
    name_pairs.append({
        "prompt": prompt,
        "completion": completion
    })

# Save the prepared data into a JSONL file (JSON Lines format)
with open('name_transliterations.jsonl', 'w', encoding='utf-8') as f:
    for pair in name_pairs:
        json.dump(pair, f, ensure_ascii=False)
        f.write('\n')

# Step 3: Upload your dataset to OpenAI for fine-tuning
openai.api_key = "your-api-key"  # Replace with your OpenAI API key

# Upload the dataset file to OpenAI
response = openai.File.create(
    file=open("name_transliterations.jsonl"),
    purpose='fine-tune'
)

# Print the file upload response
print("File uploaded successfully:", response)

# Step 4: Fine-tune the GPT-3 model using the uploaded dataset
fine_tune_response = openai.FineTune.create(
    training_file=response['id'],
    model="davinci"  # You can use other models like "curie" or "babbage" depending on your needs
)

# Print fine-tune response
print("Fine-tuning started:", fine_tune_response)

# Step 5: Monitor the fine-tuning process (check the fine-tuning status)
status = openai.FineTune.retrieve(id=fine_tune_response['id'])
print("Fine-tuning status:", status)

# Step 6: Use the fine-tuned model to generate French variations for Arabic names
def generate_french_variations(name_arabic):
    response = openai.Completion.create(
        model=fine_tune_response['fine_tuned_model'],  # Use the fine-tuned model ID
        prompt=f"Arabic Name: {name_arabic}\nFrench transliterations:",
        max_tokens=100,
        n=5,  # Generate 5 variations
        temperature=0.7
    )

    # Print out the variations generated by the fine-tuned model
    for i, choice in enumerate(response.choices):
        print(f"Variation {i + 1}: {choice.text.strip()}")

# Example usage: Generate variations for an Arabic name
generate_french_variations("محمد بن علي")





import pandas as pd
import json
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import load_dataset

# Step 1: Read the dataset from the Excel file
# Make sure to provide the correct path to your Excel file
df = pd.read_excel('path_to_your_file.xlsx')

# Step 2: Prepare the dataset for fine-tuning
name_pairs = []
for _, row in df.iterrows():
    # Create prompt and completion pairs from Arabic and French names
    prompt = f"Arabic: {row['name_ar']} {row['family_name_ar']}\nFrench transliterations:"
    completion = f" {row['name_fr']} {row['family_name_fr']}"
    
    name_pairs.append({
        "prompt": prompt,
        "completion": completion
    })

# Save the prepared data into a JSONL file (JSON Lines format)
with open('name_transliterations.jsonl', 'w', encoding='utf-8') as f:
    for pair in name_pairs:
        json.dump(pair, f, ensure_ascii=False)
        f.write('\n')

# Step 3: Fine-tune the GPT-2 model on the dataset
# Load the dataset from the JSONL file
dataset = load_dataset('json', data_files={'train': 'name_transliterations.jsonl'})

# Load the pre-trained GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['prompt'], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Define the training arguments for fine-tuning
training_args = TrainingArguments(
    output_dir="./results",  # Directory where the fine-tuned model will be saved
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_steps=10_000,
    save_total_limit=2,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
trainer.save_model("fine_tuned_model")

# Step 4: Generate French transliterations for Arabic names using the fine-tuned model

def generate_french_variations(arabic_name):
    # Prepare the prompt for generation
    prompt = f"Arabic: {arabic_name}\nFrench transliterations:"
    inputs = tokenizer(prompt, return_tensors="pt")

    # Generate French transliterations
    outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=5, temperature=0.7)

    # Decode and display the generated variations
    for i, output in enumerate(outputs):
        generated_text = tokenizer.decode(output, skip_special_tokens=True)
        print(f"Variation {i + 1}: {generated_text}")

# Example: Generate French variations for an Arabic name
generate_french_variations("محمد")

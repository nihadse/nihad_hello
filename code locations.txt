099i# detect the lontitude and latitude fron the place name 
from geopy.geocoders import Nominatim

def get_coordinates_from_place(place):
    geolocator = Nominatim(user_agent="coordinate_finder")
    location = geolocator.geocode(place)
    latitude = location.latitude
    longitude = location.longitude
    return latitude, longitude

place = "Statue of Liberty, New York"

latitude, longitude = get_coordinates_from_place(place)
print(f"The coordinates of {place} are Latitude: {latitude}, Longitude: {longitude}")

#detect country from lat and lon

from geopy.geocoders import Nominatim

def get_country_from_coordinates(latitude, longitude):
    geolocator = Nominatim(user_agent="country_detector")
    location = geolocator.reverse(f"{latitude}, {longitude}", exactly_one=True)
    address = location.raw['address']
    country = address.get('country', '')
    return country

latitude = 51.5074
longitude = -0.1278

country = get_country_from_coordinates(latitude, longitude)
print(f"The location is in {country}")



import pandas as pd

def remove_character_from_column(df, column, character):
    df[column] = df[column].str.replace(character, "")
    return df

# Assuming your DataFrame is named 'df' and the column is named 'text_column'
df = pd.DataFrame({'text_column': ['Hello*', 'How*are*you', 'Excited*']})

cleaned_df = remove_character_from_column(df, 'text_column', '*')
print(cleaned_df)






import geocoder

def get_coordinates_from_place(place):
    location = geocoder.osm(place)
    if location.ok:
        latitude = location.lat
        longitude = location.lng
        return latitude, longitude
    else:
        return None, None

place = "Statue of Liberty, New York"

latitude, longitude = get_coordinates_from_place(place)
print(f"The coordinates of {place} are Latitude: {latitude}, Longitude: {longitude}")













from sklearn.cluster import KMeans

# Assuming your DataFrame is named 'df' and contains a 'country' column


unique_countries = data_restaurant['wilaya_res'].unique()
k = len(unique_countries)


# Step 1: Create a numeric representation for each unique country
country_codes = data_restaurant['wilaya_res'].astype('category').cat.codes



kmeans = KMeans(n_clusters=k)
kmeans.fit(country_codes.values.reshape(-1, 1))

# Step 3: Assign cluster labels to DataFrame
data_restaurant['cluster_label'] = kmeans.labels_

# Step 4: Print the resulting DataFrame
data_restaurant['cluster_label'].unique()




CountryLatitudeLongitudeAllemagne51.165710.4515Russie61.5240105.3188Ukraine48.379431.1656Egypte26.820630.8025Portugal39.3999-8.2245Malaisie4.2105101.9758Pays-Bas52.36764.9041Chine35.8617104.1954Romanie45.943224.9668Suisse46.81828.2275Qatar25.354851.1839Grèce39.074221.8243Autriche47.516214.5501Côte d'Ivoire7.5399-5.5471Thailande15.8700100.9925Jordanie30.585236.2384Bulgarie42.733925.4858Indonesie-0.7893113.9213Japan36.2048138.2529Hongrie47.162519.5033Senegal14.4974-14.4524Benin9.30772.3158Pologne51.9194





Country, Latitude, Longitude
Allemagne (Germany), 51.1657, 10.4515
Russie (Russia), 61.5240, 105.3188
Ukraine, 48.3794, 31.1656
Egypte (Egypt), 26.8206, 30.8025
Portugal, 39.3999, -8.2245
Malaisie (Malaysia), 4.2105, 101.9758
Pays-Bas (Netherlands), 52.1326, 5.2913
Chine (China), 35.8617, 104.1954
Romanie (Romania), 45.9432, 24.9668
Suisse (Switzerland), 46.8182, 8.2275
Qatar, 25.3548, 51.1839
Grèce (Greece), 39.0742, 21.8243
Autriche (Austria), 47.5162, 14.5501
Côte d'Ivoire, 7.5400, -5.5471
Thailande (Thailand), 15.8700, 100.9925
Jordanie (Jordan), 30.5852, 36.2384
Bulgarie (Bulgaria), 42.7339, 25.4858
Indonesie (Indonesia), -0.7893, 113.9213
Japan, 36.2048, 138.2529
Hongrie (Hungary), 47.1625, 19.5033
Senegal, 14.4974, -14.4524
Benin, 9.3077, 2.3158
Pologne (Poland), 51.9194, 19.1451
Mauritanie (Mauritania), 21.0079, -10.9408
Norvège (Norway), 60.4720, 8.4689
Oman, 21.5126, 55.9233
Liban (Lebanon), 33.8547, 35.8623
Tchéquie (Czechia), 49.8175, 15.4730
Finlande (Finland), 61.9241, 25.7482
Mali, 17.5707, -3.9962
Honduras, 15.1994, -86.2419
Chypre (Cyprus), 35.1264, 33.4299
Vietnam, 14.0583, 108.2772
Costa Rica, 9.7489, -83.7534
Malte (Malta), 35.9375, 14.3754
Corée du Sud (South Korea), 35.9078, 127.7669
Bresil (Brazil), -14.2350, -51.9253
Guinée (Guinea), 9.9456, -9.6966
Congo, -0.2280, 15.8277
Philippines, 12.8797, 121.7740
Slovenia, 46.1512, 14.9955
Koweit (Kuwait), 29.3117, 47.4818
Irlande (Ireland), 53.1424, -7.6921
Denmark, 56.2639, 9.5018
Inde (India), 20.5937, 78.9629
Croatie (Croatia), 45.1000, 15.2000
Suède (Sweden), 60.1282, 18.6435
Biélorussie (Belarus), 53.7098, 27.9534
Slovakia, 48.6690, 19.6990
Serbie (Serbia), 44.0165, 21.0059
Moldavie (Moldova), 47.4116, 28.3699
Cameroun (Cameroon), 7.3697, 12.3547
Nigeria, 9.0820, 8.6753
Afrique du Sud (South Africa), -30.5595, 22.9375
Niger, 17.6078, 8.0817
Soudan (Sudan), 12.8628, 30.2176
Singapore, 1.3521, 103.8198
Népal (Nepal), 28.3949, 84.1240
Australie (Australia), -25.2744, 133.7751












[(36.8033059, 2.9216786), (36.7024722, 3.0801951), (36.3932061, 3.8246043), (36.4597538, 4.5289243), (36.8982165, 7.7549272), (35.856348, -0.313678), (36.72280205, 3.18388630275581), (36.6943783, 2.9718223), (47.0275167, 0.6200547), (36.7511783, 5.0643687), (47.6379599, 6.8628942), (42.0927922, -83.1125787), (36.759271, 3.0197169), (36.716075, 3.0497361), (34.784563500000004, 5.812435334419206), (36.4705658, 2.8274937), (36.8046778, 3.0426168), (36.5755523, 2.9128963), (36.2316481, 3.9082579191011253), (36.7358032, 3.6163045700585252), (36.790372, 3.0166197), (36.7669269, 2.9602571), (36.203419999999994, 1.2680696005416272), (36.3641642, 6.6084281), (36.7149768, 3.2094611), (36.7520316, 2.9815101), (36.7137875, 3.0016025479563098), (36.7341624, 2.9905236), (36.7688818, 3.030651), (36.7429388, 3.094027), (36.7474259, 3.0401832), (36.72921875, 5.960777606253644), (36.6373657, 2.769126), (23.0131338, -80.8328748), (45.7510888, -71.7670146), (35.397838500000006, 0.24301949927219488), (32.72105455, -117.17424172665355), (35.9282659, 0.118707), (34.4444796, -4.176394839428059), (35.7044415, -0.6502981), (35.707778149999996, -0.5792550049164042), (36.7364237, 3.2826417), (49.222747, 6.3331945), (36.1895852, 5.4024656), (36.19064, 5.4104537), (36.5906898, 4.6037292), (36.75451150000001, 6.885625492591183), (36.7687167, 3.0496817), (34.8947575, 1.594579173136212), (36.527157, 2.1672011802712086), (36.6816175, 4.237186047040007)]





import sqlite3

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'

def get_latitude_longitude(place_name):
    connection = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True)
    cursor = connection.cursor()

    try:
        # Search for the place name in the 'place' table
        query = f"SELECT lat, lon FROM place WHERE name='{place_name}' LIMIT 1;"
        cursor.execute(query)

        row = cursor.fetchone()
        if row:
            latitude, longitude = row
            return latitude, longitude
        else:
            print(f"Could not find coordinates for '{place_name}'.")
            return None, None

    except Exception as e:
        print(f"Error occurred while querying '{place_name}': {e}")
        return None, None

    finally:
        cursor.close()
        connection.close()

# Example usage:
place_name = "New York City"
latitude, longitude = get_latitude_longitude(place_name)
if latitude and longitude:
    print(f"Latitude: {latitude}, Longitude: {longitude}")


# tables

import sqlite3

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'

def explore_nominatim_database():
    connection = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True)
    cursor = connection.cursor()

    try:
        # Get a list of all tables in the database
        query = "SELECT name FROM sqlite_master WHERE type='table';"
        cursor.execute(query)

        table_names = [row[0] for row in cursor.fetchall()]

        # Print the table names
        print("Tables in the database:")
        for table_name in table_names:
            print(table_name)

        # Explore the contents of each table
        for table_name in table_names:
            print(f"\nContents of '{table_name}':")
            query = f"SELECT * FROM {table_name} LIMIT 5;"
            cursor.execute(query)
            rows = cursor.fetchall()

            # Print the column names
            column_names = [description[0] for description in cursor.description]
            print(column_names)

            # Print the data
            for row in rows:
                print(row)

    except Exception as e:
        print(f"Error occurred while exploring the database: {e}")

    finally:
        cursor.close()
        connection.close()

# Call the function to explore the Nominatim database
explore_nominatim_database()







import sqlite3
import gzip

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
gzipped_file = 'path/to/country_grid.sql.gz'

def explore_nominatim_database(db_file):
    with gzip.open(db_file, 'rb') as f:
        # Read the compressed data and decode it as UTF-8
        uncompressed_data = f.read().decode('utf-8')

    # Create an in-memory database to connect and query the data
    connection = sqlite3.connect(':memory:')
    cursor = connection.cursor()

    try:
        # Execute the uncompressed data as a SQL script to populate the in-memory database
        cursor.executescript(uncompressed_data)

        # Get a list of all tables in the database
        query = "SELECT name FROM sqlite_master WHERE type='table';"
        cursor.execute(query)

        table_names = [row[0] for row in cursor.fetchall()]

        # Print the table names
        print("Tables in the database:")
        for table_name in table_names:
            print(table_name)

            # Explore the contents of each table
            query = f"SELECT * FROM {table_name} LIMIT 5;"
            cursor.execute(query)
            rows = cursor.fetchall()

            # Print the column names
            column_names = [description[0] for description in cursor.description]
            print(column_names)

            # Print the data
            for row in rows:
                print(row)
            print("\n")

    except Exception as e:
        print(f"Error occurred while exploring the database: {e}")

    finally:
        cursor.close()
        connection.close()

# Call the function to explore the Nominatim database
explore_nominatim_database(gzipped_file)







from geopy.geocoders import Nominatim

geolocator = Nominatim(user_agent="geoapiExercises", database="path/to/country_grid.sql.gz")

countries = ["Algeria", "United States", "France"]

for country in countries:
    location = geolocator.geocode(country)
    print((location.latitude, location.longitude))


import sqlite3
from geopy.geocoders import Nominatim

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'

def get_country_coordinates(country_name):
    geolocator = Nominatim(user_agent="myGeocoder")

    # Establish a connection to the SQLite database
    connection = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True)
    cursor = connection.cursor()

    # Search for the country name in the 'country_info' table
    query = f"SELECT lat, lon FROM country_info WHERE country_name='{country_name}' LIMIT 1;"
    cursor.execute(query)

    row = cursor.fetchone()
    if row:
        latitude, longitude = row
        return latitude, longitude
    else:
        print(f"Could not find coordinates for '{country_name}'.")
        return None, None

    cursor.close()
    connection.close()

# Example usage:
country_name = "United States"
latitude, longitude = get_country_coordinates(country_name)
if latitude and longitude:
    print(f"Latitude: {latitude}, Longitude: {longitude}")


import sqlite3
import pandas as pd
import gzip

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'

def sqlite_to_excel(sqlite_file, output_excel):
    with gzip.open(sqlite_file, 'rb') as f:
        # Read the compressed data and decode it as UTF-8
        uncompressed_data = f.read().decode('utf-8')

    # Create an in-memory database to connect and query the data
    connection = sqlite3.connect(':memory:')
    cursor = connection.cursor()

    try:
        # Execute the uncompressed data as a SQL script to populate the in-memory database
        cursor.executescript(uncompressed_data)

        # Get a list of all tables in the database
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        table_names = [row[0] for row in cursor.fetchall()]

        # Create an Excel writer object to write data to Excel file
        writer = pd.ExcelWriter(output_excel, engine='xlsxwriter')

        # Loop through each table and convert it to an Excel sheet
        for table_name in table_names:
            # Read table data into a pandas DataFrame
            df = pd.read_sql_query(f"SELECT * FROM {table_name}", connection)

            # Write the DataFrame to the Excel file
            df.to_excel(writer, sheet_name=table_name, index=False)

        # Save the Excel file
        writer.save()

        print(f"Successfully exported database to '{output_excel}'.")

    except Exception as e:
        print(f"Error occurred while exporting the database: {e}")

    finally:
        cursor.close()
        connection.close()

# Example usage:
output_excel_file = 'output.xlsx'
sqlite_to_excel(db_file, output_excel_file)




import sqlite3
import pandas as pd
import gzip

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'
output_excel_file = 'output_data.xlsx'  # Replace with the desired output Excel file name

def export_sqlite_to_excel(db_file, output_excel_file):
    with gzip.open(db_file, 'rb') as f:
        # Read the compressed data and decode it as UTF-8
        uncompressed_data = f.read().decode('utf-8')

    # Create an in-memory database to connect and query the data
    connection = sqlite3.connect(':memory:')
    cursor = connection.cursor()

    try:
        # Execute the uncompressed data as a SQL script to populate the in-memory database
        cursor.executescript(uncompressed_data)

        # Get the table names in the database
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        table_names = [row[0] for row in cursor.fetchall()]

        # Export each table to an Excel file
        with pd.ExcelWriter(output_excel_file) as writer:
            for table_name in table_names:
                # Get the column names of the table
                cursor.execute(f"PRAGMA table_info({table_name});")
                columns = [col[1] for col in cursor.fetchall()]

                # Generate the SELECT query with explicitly set column names
                select_query = f"SELECT {', '.join(columns)} FROM {table_name};"
                df = pd.read_sql_query(select_query, connection)
                df.to_excel(writer, sheet_name=table_name, index=False)

        print(f"Data successfully exported to '{output_excel_file}'.")

    except Exception as e:
        print(f"Error occurred while exporting data: {e}")

    finally:
        cursor.close()
        connection.close()

# Call the function to export the SQLite database to an Excel file
export_sqlite_to_excel(db_file, output_excel_file)













import pandas as pd
from googletrans import Translator

def translate_column(input_column, source_language, target_language):
    translator = Translator()
    # Translate each value in the input column using Google Translate API.
    translated_column = [translator.translate(value, src=source_language, dest=target_language).text
                         for value in input_column]
    return translated_column

def main():
    # Read the CSV file containing country names in multiple languages.
    data = pd.read_csv("multilingual_countries.csv")

    source_languages = ["tr", "es", "de"]  # Turkish, Spanish, German
    target_languages = ["fr", "en", "ar"]  # French, English, Arabic

    # Translate the country names to each target language and create new columns for each translation.
    for source_lang, target_lang in zip(source_languages, target_languages):
        target_column_name = f"Country_{target_lang.upper()}"
        data[target_column_name] = translate_column(data["Country"], source_lang, target_lang)

    # Save the DataFrame to a new CSV file.
    data.to_csv("translated_countries.csv", index=False)

if __name__ == "__main__":
    main()













import pandas as pd

# Sample data
data = {
    'Client ID': [1001, 1002, 1003, 1004],
    'Transaction Amount': [50.35, 100.20, 75.10, 500.75],
    'Location': ['New York', 'Los Angeles', 'Miami', 'San Francisco'],
    'Transaction Date': ['2023-07-12', '2023-07-10', '2023-07-09', '2023-07-08']
}

# Create DataFrame
df = pd.DataFrame(data)

# Convert 'Transaction Date' to datetime
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

# Group data by client ID, date, and location and count the number of transactions
transaction_counts = df.groupby(['Client ID', 'Transaction Date', 'Location']).size().reset_index(name='Count')

# Create an empty similarity matrix with client IDs as index and columns
clients = df['Client ID'].unique()
similarity_matrix = pd.DataFrame(index=clients, columns=clients)
similarity_matrix = similarity_matrix.fillna(0)

# Fill similarity matrix with the count of transactions for each client pair
for _, row in transaction_counts.iterrows():
    client1, client2, count = row['Client ID'], row['Client ID'], row['Count']
    similarity_matrix.at[client1, client2] = count
    similarity_matrix.at[client2, client1] = count

print("Similarity Matrix:")
print(similarity_matrix)



import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder

# Sample data (replace this with your dataset)
data = {
    'ClientID': [1, 2, 3, 4],
    'Country': ['USA', 'Canada', 'USA', 'Mexico'],
    'Date': ['2023-07-28', '2023-07-29', '2023-07-28', '2023-07-30'],
    'Amount': [100, 200, 150, 300]
}

df = pd.DataFrame(data)

# Encode categorical data (Country and Date)
le_country = LabelEncoder()
le_date = LabelEncoder()

df['CountryEncoded'] = le_country.fit_transform(df['Country'])
df['DateEncoded'] = le_date.fit_transform(df['Date'])

# Pivot table to create the matrix
matrix = df.pivot(index='ClientID', columns=['CountryEncoded', 'DateEncoded'], values='Amount')
matrix = matrix.fillna(0)  # Replace NaN with 0 if needed

# Calculate the cosine similarity matrix
cosine_sim = cosine_similarity(matrix)

# Convert the similarity matrix to a DataFrame for better representation
similarity_matrix_df = pd.DataFrame(cosine_sim, index=df['ClientID'], columns=df['ClientID'])

print(similarity_matrix_df)




import pandas as pd
from itertools import combinations

# Sample data (replace this with your dataset)
data = {
    'TransactionID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'ClientID': [1, 2, 1, 3, 4, 2, 3, 5, 1, 5],
    'Location': ['New York', 'Chicago', 'New York', 'Los Angeles', 'Chicago', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York']
}

df = pd.DataFrame(data)

# Group transactions by Location and create a list of clients in each location
grouped_data = df.groupby('Location')['ClientID'].apply(list).reset_index()

# Generate all possible combinations of clients within each location
def generate_combinations(row):
    return list(combinations(row, 2))

grouped_data['Combinations'] = grouped_data['ClientID'].apply(generate_combinations)

# Flatten the combinations to get the pairs of clients
all_combinations = [pair for sublist in grouped_data['Combinations'] for pair in sublist]

# Create a dictionary to count the co-occurrence of each pair of clients in each location
co_occurrence_counts = {}
for pair in all_combinations:
    pair = tuple(sorted(pair))  # Sort the pair to ensure consistent counting
    co_occurrence_counts[pair] = co_occurrence_counts.get(pair, 0) + 1

# Create the co-occurrence matrix
unique_clients = df['ClientID'].unique()
co_occurrence_matrix = pd.DataFrame(0, index=unique_clients, columns=unique_clients)

for pair, count in co_occurrence_counts.items():
    client1, client2 = pair
    co_occurrence_matrix.loc[client1, client2] = count
    co_occurrence_matrix.loc[client2, client1] = count

print(co_occurrence_matrix)









import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler

# Assuming you have a DataFrame called 'data' with columns: 'client_id', 'transaction_date', 'amount'
# You can use 'pivot_table' to reshape your data into a matrix-like structure.
data_pivot = data.pivot_table(index='client_id', columns='transaction_date', values='amount', fill_value=0)

# Normalize the data to remove the influence of scale on the cosine similarity.
scaler = StandardScaler()
normalized_data = scaler.fit_transform(data_pivot)

# Calculate the cosine similarity matrix
cosine_sim_matrix = cosine_similarity(normalized_data)

# Convert the cosine similarity matrix into a DataFrame for better visualization
cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=data_pivot.index, columns=data_pivot.index)

# cosine_sim_df now contains the similarity scores between clients based on their transaction amounts.




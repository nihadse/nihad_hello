
import numpy as np
import pandas as pd
from collections import Counter

# Sample matrix of transactions (replace with your data)
data = [
    ["Client A", "USA"],
    ["Client B", "Canada"],
    ["Client A", "USA"],
    ["Client C", "Germany"],
    # Add more transactions here
]
columns = ["Client", "Country"]
df = pd.DataFrame(data, columns=columns)

# Calculate the most common countries
most_common_countries = Counter(df["Country"]).most_common()

# Display the results
print("Most used countries by clients:")
for country, count in most_common_countries:
    print(f"{country}: {count} transactions")













import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Sample matrix of transactions with dates (replace with your data)
data = [
    ["Client A", "2023-08-01", 100],
    ["Client B", "2023-08-01", 150],
    ["Client A", "2023-08-02", 50],
    ["Client C", "2023-08-02", 200],
    # Add more transactions here
]
columns = ["Client", "Date", "Amount"]
df = pd.DataFrame(data, columns=columns)
df["Date"] = pd.to_datetime(df["Date"])

# Group the data by date and calculate the total transaction amount
daily_usage = df.groupby("Date")["Amount"].sum()

# Create a line plot to visualize the tendency
plt.figure(figsize=(10, 6))
plt.plot(daily_usage.index, daily_usage.values, marker='o')
plt.xlabel("Date")
plt.ylabel("Total Transaction Amount")
plt.title("Visa Card Usage Tendency")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()










import pandas as pd

# Sample matrix of transactions with dates (replace with your data)
data = [
    ["Client A", "2023-08-01", 100],
    ["Client B", "2023-08-01", 150],
    ["Client A", "2023-08-02", 50],
    ["Client C", "2023-08-02", 200],
    # Add more transactions here
]
columns = ["Client", "Date", "Amount"]
df = pd.DataFrame(data, columns=columns)
df["Date"] = pd.to_datetime(df["Date"])

# Pivot the data to create a matrix of transactions
pivot_table = df.pivot_table(index="Client", columns="Date", values="Amount", aggfunc="sum", fill_value=0)

# Display the matrix
print(pivot_table)








import numpy as np
import matplotlib.pyplot as plt

# Create a random 100x100 matrix (replace this with your data)
matrix = np.random.randint(0, 10, size=(100, 100))

# Set up the plot
fig, ax = plt.subplots(figsize=(10, 10))

# Create a heatmap with color-coding
cax = ax.imshow(matrix, cmap='viridis', aspect='auto')

# Set column names as tick labels for columns
column_names = matrix[0, :]  # Use the first row for column names
ax.set_xticks(np.arange(matrix.shape[1]))
ax.set_xticklabels(column_names)

# Set row indices as tick labels for rows
row_indices = matrix[:, 0]  # Use the first column for row indices
ax.set_yticks(np.arange(matrix.shape[0]))
ax.set_yticklabels(row_indices)

# Add a colorbar
cbar = fig.colorbar(cax)

# Show the plot
plt.show()






import numpy as np
import matplotlib.pyplot as plt

# Create a random 100x100 matrix (replace this with your data)
matrix = np.random.randint(0, 10, size=(100, 100))

# Set up the plot
fig, ax = plt.subplots(figsize=(10, 10))

# Create a heatmap with color-coding
cax = ax.imshow(matrix, cmap='viridis', aspect='auto')

# Set tick labels for rows and columns
ax.set_xticks(np.arange(matrix.shape[1]))
ax.set_yticks(np.arange(matrix.shape[0]))
ax.set_xticklabels(matrix[0, :])  # Use the first row for column labels
ax.set_yticklabels(matrix[:, 0])  # Use the first column for row labels

# Add a colorbar
cbar = fig.colorbar(cax)

# Show the plot
plt.show()





import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# Create a random 100x100 matrix (replace this with your data)
matrix = np.random.randint(0, 10, size=(100, 100))

# Set threshold values for color-coding
threshold_low = 3
threshold_high = 7

# Create a colormap with colors for different thresholds
cmap = ListedColormap(['red', 'yellow', 'green'])

# Set up the plot
fig, ax = plt.subplots(figsize=(10, 10))

# Create a heatmap with color-coding
cax = ax.matshow(matrix, cmap=cmap, vmin=threshold_low, vmax=threshold_high)

# Add a colorbar
cbar = fig.colorbar(cax, ticks=[threshold_low, (threshold_low + threshold_high) / 2, threshold_high])
cbar.ax.set_yticklabels(['Low', 'Medium', 'High'])

# Show the plot
plt.show()






import numpy as np
from bokeh.plotting import figure, show
from bokeh.models import ColorBar
from bokeh.transform import linear_cmap
from bokeh.palettes import Viridis256

# Create a random 10000x10000 matrix
matrix = np.random.rand(10000, 10000)

# Define a color map
mapper = linear_cmap(field_name='matrix', palette=Viridis256, low=matrix.min(), high=matrix.max())

# Create a Bokeh figure
p = figure(width=800, height=800, toolbar_location='below')

# Add the heatmap to the figure
p.image(image=[matrix], x=0, y=0, dw=matrix.shape[1], dh=matrix.shape[0], color_mapper=mapper)

# Add a color bar to the plot
color_bar = ColorBar(color_mapper=mapper['transform'], width=8, location=(0,0))
p.add_layout(color_bar, 'right')

# Show the plot
show(p)







Bonjour [Nom du destinataire],

J'espère que vous allez bien. Je suis enthousiaste à l'idée de passer en revue les résultats finaux avec vous et de discuter des éléments supplémentaires que nous pourrions incorporer dans le fichier final avant que je ne vous l'envoie. Seriez-vous disponible pour une réunion afin d'en discuter ? Pourriez-vous s'il vous plaît me communiquer vos disponibilités pour la semaine à venir ?

Je vous remercie et j'attends avec impatience notre discussion.

Cordialement,
[Votre Nom]"



Cher [Nom du Collègue],

J'espère que vous allez bien. Je souhaite demander la création d'une carte Visa pour faciliter mes transactions financières. Pourriez-vous me guider sur les étapes nécessaires? Votre aide serait grandement appréciée.

Cordialement,
[Votre Nom]








import pickle
import nltk
from nltk.tokenize import PunktSentenceTokenizer

# Load the Punkt tokenizer model
with open('punkt_model.pickle', 'rb') as model_file:
    punkt_model = pickle.load(model_file)

# Load the custom stopwords from the file
with open('custom_stopwords.txt', 'r') as stopwords_file:
    custom_stop_words = set(stopword.strip() for stopword in stopwords_file)

# Load the text you want to tokenize
text = "This is an example sentence. And here's another one."

# Initialize the Punkt tokenizer with the loaded model
tokenizer = PunktSentenceTokenizer(punkt_model)

# Tokenize the text into sentences
sentences = tokenizer.tokenize(text)

# Tokenize each sentence and filter out custom stopwords
for sentence in sentences:
    words = nltk.word_tokenize(sentence)
    filtered_words = [word for word in words if word.lower() not in custom_stop_words]
    print(filtered_words)









# Load the custom punkt tokenizer model from a file
with open('path_to_downloaded_model', 'rb') as model_file:
    custom_punkt_model = model_file.read()

# Example text
text = "This is a sample sentence. And here is another one."

# Use the custom punkt tokenizer model to tokenize the text
from nltk.tokenize.punkt import PunktSentenceTokenizer
tokenizer = PunktSentenceTokenizer(custom_punkt_model)
sentences = tokenizer.tokenize(text)

# Print the sentences
for sentence in sentences:
    print(sentence)




git clone https://github.com/tesseract-ocr/tesseract.git

https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip
https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip


import fitz  # PyMuPDF
import pytesseract
from PIL import Image

pdf_path = "path_to_your_scanned_pdf_file.pdf"
doc = fitz.open(pdf_path)

target_word = "bank"

for page_number, page in enumerate(doc, start=1):
    image = page.get_pixmap()
    img = Image.frombytes("RGB", [image.width, image.height], image.samples)
    ocr_text = pytesseract.image_to_string(img)
    
    if target_word in ocr_text:
        sentences = ocr_text.split(".")
        for sentence in sentences:
            if target_word in sentence:
                print(f"Page {page_number}, Sentence: {sentence.strip()}")









mport fitz  # PyMuPDF

pdf_path = "path_to_your_pdf_file.pdf"
doc = fitz.open(pdf_path)

target_word = "bank"

for page_number, page in enumerate(doc, start=1):
    text = page.get_text()
    if target_word in text:
        sentences = text.split(".")
        for sentence in sentences:
            if target_word in sentence:
                print(f"Page {page_number}, Sentence: {sentence.strip()}")







import fitz  # PyMuPDF pdf_path = "path_to_your_pdf_file.pdf" doc = fitz.open(pdf_path) target_word = "bank" for page_number, page in enumerate(doc, start=1): text = page.get_text() if target_word in text: sentences = text.split(".") for sentence in sentences: if target_word in sentence: print(f"Page {page_number}, Sentence: {sentence.strip()}")





import fitz  # PyMuPDF

pdf_path = "path_to_your_pdf_file.pdf"
doc = fitz.open(pdf_path)

target_word = "bank"
word_found = False

for page in doc:
    text = page.get_text()
    if target_word in text:
        word_found = True
        break

if word_found:
    print(f"The word '{target_word}' was found in the PDF.")
else:
    print(f"The word '{target_word}' was not found in the PDF.")





import pandas as pd

# Sample data for left and right tables
left_data = {'ID': [1, 2, 3, 4],
             'Name': ['Alice', 'Bob', 'Charlie', 'David']}
right_data = {'ID': [2, 3, 5],
              'Name': ['Xavier', 'Yvonne', 'Zara']}

left_df = pd.DataFrame(left_data)
right_df = pd.DataFrame(right_data)

# Performing the left join
left_result = pd.merge(left_df, right_df, on=['ID', 'Name'], how='left')

# Performing the inner join
inner_result = pd.merge(left_df, right_df, on=['ID', 'Name'], how='inner')

# Removing common rows between inner and left join results
unique_left = left_result[~left_result.set_index(['ID', 'Name']).index.isin(inner_result.set_index(['ID', 'Name']).index)]

print(unique_left)




import pandas as pd

# Assuming you have loaded your datasets 'dr' and 'dl'

# Perform a left join
result = pd.merge(dr, dl, how='left', left_on='CODE SIEGE HMVT', right_on='CODE SIEGE -HMVT')

# Identify the intersection and remove it
intersection_columns = set(dr.columns) & set(dl.columns)
result.drop(columns=intersection_columns, inplace=True)

print(result)







import pandas as pd

# Define the first dataset
data1 = {'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
         'Value1': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}
df1 = pd.DataFrame(data1)

# Define the second dataset
data2 = {'ID': [3, 4, 5, 6, 7, 8, 11, 12, 13, 14],
         'Value2': [30, 40, 50, 60, 70, 80, 110, 120, 130, 140]}
df2 = pd.DataFrame(data2)

# Perform a left join
result = df1.merge(df2, how='left', on='ID', suffixes=('_df1', '_df2'))

# Remove the intersection
result = result.loc[result['Value2_df2'].isnull()].drop(columns=['Value2_df2'])

print(result)






import pandas as pd

# Assuming 'df' is your DataFrame
# Use apply with a lambda function to create the new 'class' column
df['class'] = df.apply(lambda row: 'COM TPE PAIEMENT' if pd.isnull(row['COM TPE'])
                            else 'COM TPE PAIEMENT' if pd.isnull(row['COM TPE']) and pd.isnull(row['COM RET GAB'])
                            else 'RET GAB BNPPARIBAS' if '*>' in row['LIB. COMPLEMENT. MVT CPTABLE -HMVT']
                            else 'RET GAB HORS BNPPARIBAS', axis=1)







78099i# detect the lontitude and latitude fron the place name 
from geopy.geocoders import Nominatim

def get_coordinates_from_place(place):
    geolocator = Nominatim(user_agent="coordinate_finder")
    location = geolocator.geocode(place)
    latitude = location.latitude
    longitude = location.longitude
    return latitude, longitude

place = "Statue of Liberty, New York"

latitude, longitude = get_coordinates_from_place(place)
print(f"The coordinates of {place} are Latitude: {latitude}, Longitude: {longitude}")

#detect country from lat and lon

from geopy.geocoders import Nominatim

def get_country_from_coordinates(latitude, longitude):
    geolocator = Nominatim(user_agent="country_detector")
    location = geolocator.reverse(f"{latitude}, {longitude}", exactly_one=True)
    address = location.raw['address']
    country = address.get('country', '')
    return country

latitude = 51.5074
longitude = -0.1278

country = get_country_from_coordinates(latitude, longitude)
print(f"The location is in {country}")



import pandas as pd

def remove_character_from_column(df, column, character):
    df[column] = df[column].str.replace(character, "")
    return df

# Assuming your DataFrame is named 'df' and the column is named 'text_column'
df = pd.DataFrame({'text_column': ['Hello*', 'How*are*you', 'Excited*']})

cleaned_df = remove_character_from_column(df, 'text_column', '*')
print(cleaned_df)






import geocoder

def get_coordinates_from_place(place):
    location = geocoder.osm(place)
    if location.ok:
        latitude = location.lat
        longitude = location.lng
        return latitude, longitude
    else:
        return None, None

place = "Statue of Liberty, New York"

latitude, longitude = get_coordinates_from_place(place)
print(f"The coordinates of {place} are Latitude: {latitude}, Longitude: {longitude}")













from sklearn.cluster import KMeans

# Assuming your DataFrame is named 'df' and contains a 'country' column


unique_countries = data_restaurant['wilaya_res'].unique()
k = len(unique_countries)


# Step 1: Create a numeric representation for each unique country
country_codes = data_restaurant['wilaya_res'].astype('category').cat.codes



kmeans = KMeans(n_clusters=k)
kmeans.fit(country_codes.values.reshape(-1, 1))

# Step 3: Assign cluster labels to DataFrame
data_restaurant['cluster_label'] = kmeans.labels_

# Step 4: Print the resulting DataFrame
data_restaurant['cluster_label'].unique()




CountryLatitudeLongitudeAllemagne51.165710.4515Russie61.5240105.3188Ukraine48.379431.1656Egypte26.820630.8025Portugal39.3999-8.2245Malaisie4.2105101.9758Pays-Bas52.36764.9041Chine35.8617104.1954Romanie45.943224.9668Suisse46.81828.2275Qatar25.354851.1839Grèce39.074221.8243Autriche47.516214.5501Côte d'Ivoire7.5399-5.5471Thailande15.8700100.9925Jordanie30.585236.2384Bulgarie42.733925.4858Indonesie-0.7893113.9213Japan36.2048138.2529Hongrie47.162519.5033Senegal14.4974-14.4524Benin9.30772.3158Pologne51.9194





Country, Latitude, Longitude
Allemagne (Germany), 51.1657, 10.4515
Russie (Russia), 61.5240, 105.3188
Ukraine, 48.3794, 31.1656
Egypte (Egypt), 26.8206, 30.8025
Portugal, 39.3999, -8.2245
Malaisie (Malaysia), 4.2105, 101.9758
Pays-Bas (Netherlands), 52.1326, 5.2913
Chine (China), 35.8617, 104.1954
Romanie (Romania), 45.9432, 24.9668
Suisse (Switzerland), 46.8182, 8.2275
Qatar, 25.3548, 51.1839
Grèce (Greece), 39.0742, 21.8243
Autriche (Austria), 47.5162, 14.5501
Côte d'Ivoire, 7.5400, -5.5471
Thailande (Thailand), 15.8700, 100.9925
Jordanie (Jordan), 30.5852, 36.2384
Bulgarie (Bulgaria), 42.7339, 25.4858
Indonesie (Indonesia), -0.7893, 113.9213
Japan, 36.2048, 138.2529
Hongrie (Hungary), 47.1625, 19.5033
Senegal, 14.4974, -14.4524
Benin, 9.3077, 2.3158
Pologne (Poland), 51.9194, 19.1451
Mauritanie (Mauritania), 21.0079, -10.9408
Norvège (Norway), 60.4720, 8.4689
Oman, 21.5126, 55.9233
Liban (Lebanon), 33.8547, 35.8623
Tchéquie (Czechia), 49.8175, 15.4730
Finlande (Finland), 61.9241, 25.7482
Mali, 17.5707, -3.9962
Honduras, 15.1994, -86.2419
Chypre (Cyprus), 35.1264, 33.4299
Vietnam, 14.0583, 108.2772
Costa Rica, 9.7489, -83.7534
Malte (Malta), 35.9375, 14.3754
Corée du Sud (South Korea), 35.9078, 127.7669
Bresil (Brazil), -14.2350, -51.9253
Guinée (Guinea), 9.9456, -9.6966
Congo, -0.2280, 15.8277
Philippines, 12.8797, 121.7740
Slovenia, 46.1512, 14.9955
Koweit (Kuwait), 29.3117, 47.4818
Irlande (Ireland), 53.1424, -7.6921
Denmark, 56.2639, 9.5018
Inde (India), 20.5937, 78.9629
Croatie (Croatia), 45.1000, 15.2000
Suède (Sweden), 60.1282, 18.6435
Biélorussie (Belarus), 53.7098, 27.9534
Slovakia, 48.6690, 19.6990
Serbie (Serbia), 44.0165, 21.0059
Moldavie (Moldova), 47.4116, 28.3699
Cameroun (Cameroon), 7.3697, 12.3547
Nigeria, 9.0820, 8.6753
Afrique du Sud (South Africa), -30.5595, 22.9375
Niger, 17.6078, 8.0817
Soudan (Sudan), 12.8628, 30.2176
Singapore, 1.3521, 103.8198
Népal (Nepal), 28.3949, 84.1240
Australie (Australia), -25.2744, 133.7751












[(36.8033059, 2.9216786), (36.7024722, 3.0801951), (36.3932061, 3.8246043), (36.4597538, 4.5289243), (36.8982165, 7.7549272), (35.856348, -0.313678), (36.72280205, 3.18388630275581), (36.6943783, 2.9718223), (47.0275167, 0.6200547), (36.7511783, 5.0643687), (47.6379599, 6.8628942), (42.0927922, -83.1125787), (36.759271, 3.0197169), (36.716075, 3.0497361), (34.784563500000004, 5.812435334419206), (36.4705658, 2.8274937), (36.8046778, 3.0426168), (36.5755523, 2.9128963), (36.2316481, 3.9082579191011253), (36.7358032, 3.6163045700585252), (36.790372, 3.0166197), (36.7669269, 2.9602571), (36.203419999999994, 1.2680696005416272), (36.3641642, 6.6084281), (36.7149768, 3.2094611), (36.7520316, 2.9815101), (36.7137875, 3.0016025479563098), (36.7341624, 2.9905236), (36.7688818, 3.030651), (36.7429388, 3.094027), (36.7474259, 3.0401832), (36.72921875, 5.960777606253644), (36.6373657, 2.769126), (23.0131338, -80.8328748), (45.7510888, -71.7670146), (35.397838500000006, 0.24301949927219488), (32.72105455, -117.17424172665355), (35.9282659, 0.118707), (34.4444796, -4.176394839428059), (35.7044415, -0.6502981), (35.707778149999996, -0.5792550049164042), (36.7364237, 3.2826417), (49.222747, 6.3331945), (36.1895852, 5.4024656), (36.19064, 5.4104537), (36.5906898, 4.6037292), (36.75451150000001, 6.885625492591183), (36.7687167, 3.0496817), (34.8947575, 1.594579173136212), (36.527157, 2.1672011802712086), (36.6816175, 4.237186047040007)]





import sqlite3

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'

def get_latitude_longitude(place_name):
    connection = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True)
    cursor = connection.cursor()

    try:
        # Search for the place name in the 'place' table
        query = f"SELECT lat, lon FROM place WHERE name='{place_name}' LIMIT 1;"
        cursor.execute(query)

        row = cursor.fetchone()
        if row:
            latitude, longitude = row
            return latitude, longitude
        else:
            print(f"Could not find coordinates for '{place_name}'.")
            return None, None

    except Exception as e:
        print(f"Error occurred while querying '{place_name}': {e}")
        return None, None

    finally:
        cursor.close()
        connection.close()

# Example usage:
place_name = "New York City"
latitude, longitude = get_latitude_longitude(place_name)
if latitude and longitude:
    print(f"Latitude: {latitude}, Longitude: {longitude}")


# tables

import sqlite3

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'

def explore_nominatim_database():
    connection = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True)
    cursor = connection.cursor()

    try:
        # Get a list of all tables in the database
        query = "SELECT name FROM sqlite_master WHERE type='table';"
        cursor.execute(query)

        table_names = [row[0] for row in cursor.fetchall()]

        # Print the table names
        print("Tables in the database:")
        for table_name in table_names:
            print(table_name)

        # Explore the contents of each table
        for table_name in table_names:
            print(f"\nContents of '{table_name}':")
            query = f"SELECT * FROM {table_name} LIMIT 5;"
            cursor.execute(query)
            rows = cursor.fetchall()

            # Print the column names
            column_names = [description[0] for description in cursor.description]
            print(column_names)

            # Print the data
            for row in rows:
                print(row)

    except Exception as e:
        print(f"Error occurred while exploring the database: {e}")

    finally:
        cursor.close()
        connection.close()

# Call the function to explore the Nominatim database
explore_nominatim_database()







import sqlite3
import gzip

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
gzipped_file = 'path/to/country_grid.sql.gz'

def explore_nominatim_database(db_file):
    with gzip.open(db_file, 'rb') as f:
        # Read the compressed data and decode it as UTF-8
        uncompressed_data = f.read().decode('utf-8')

    # Create an in-memory database to connect and query the data
    connection = sqlite3.connect(':memory:')
    cursor = connection.cursor()

    try:
        # Execute the uncompressed data as a SQL script to populate the in-memory database
        cursor.executescript(uncompressed_data)

        # Get a list of all tables in the database
        query = "SELECT name FROM sqlite_master WHERE type='table';"
        cursor.execute(query)

        table_names = [row[0] for row in cursor.fetchall()]

        # Print the table names
        print("Tables in the database:")
        for table_name in table_names:
            print(table_name)

            # Explore the contents of each table
            query = f"SELECT * FROM {table_name} LIMIT 5;"
            cursor.execute(query)
            rows = cursor.fetchall()

            # Print the column names
            column_names = [description[0] for description in cursor.description]
            print(column_names)

            # Print the data
            for row in rows:
                print(row)
            print("\n")

    except Exception as e:
        print(f"Error occurred while exploring the database: {e}")

    finally:
        cursor.close()
        connection.close()

# Call the function to explore the Nominatim database
explore_nominatim_database(gzipped_file)







from geopy.geocoders import Nominatim

geolocator = Nominatim(user_agent="geoapiExercises", database="path/to/country_grid.sql.gz")

countries = ["Algeria", "United States", "France"]

for country in countries:
    location = geolocator.geocode(country)
    print((location.latitude, location.longitude))


import sqlite3
from geopy.geocoders import Nominatim

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'

def get_country_coordinates(country_name):
    geolocator = Nominatim(user_agent="myGeocoder")

    # Establish a connection to the SQLite database
    connection = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True)
    cursor = connection.cursor()

    # Search for the country name in the 'country_info' table
    query = f"SELECT lat, lon FROM country_info WHERE country_name='{country_name}' LIMIT 1;"
    cursor.execute(query)

    row = cursor.fetchone()
    if row:
        latitude, longitude = row
        return latitude, longitude
    else:
        print(f"Could not find coordinates for '{country_name}'.")
        return None, None

    cursor.close()
    connection.close()

# Example usage:
country_name = "United States"
latitude, longitude = get_country_coordinates(country_name)
if latitude and longitude:
    print(f"Latitude: {latitude}, Longitude: {longitude}")


import sqlite3
import pandas as pd
import gzip

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'

def sqlite_to_excel(sqlite_file, output_excel):
    with gzip.open(sqlite_file, 'rb') as f:
        # Read the compressed data and decode it as UTF-8
        uncompressed_data = f.read().decode('utf-8')

    # Create an in-memory database to connect and query the data
    connection = sqlite3.connect(':memory:')
    cursor = connection.cursor()

    try:
        # Execute the uncompressed data as a SQL script to populate the in-memory database
        cursor.executescript(uncompressed_data)

        # Get a list of all tables in the database
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        table_names = [row[0] for row in cursor.fetchall()]

        # Create an Excel writer object to write data to Excel file
        writer = pd.ExcelWriter(output_excel, engine='xlsxwriter')

        # Loop through each table and convert it to an Excel sheet
        for table_name in table_names:
            # Read table data into a pandas DataFrame
            df = pd.read_sql_query(f"SELECT * FROM {table_name}", connection)

            # Write the DataFrame to the Excel file
            df.to_excel(writer, sheet_name=table_name, index=False)

        # Save the Excel file
        writer.save()

        print(f"Successfully exported database to '{output_excel}'.")

    except Exception as e:
        print(f"Error occurred while exporting the database: {e}")

    finally:
        cursor.close()
        connection.close()

# Example usage:
output_excel_file = 'output.xlsx'
sqlite_to_excel(db_file, output_excel_file)




import sqlite3
import pandas as pd
import gzip

# Replace 'path/to/country_grid.sql.gz' with the actual path to your downloaded file
db_file = 'path/to/country_grid.sql.gz'
output_excel_file = 'output_data.xlsx'  # Replace with the desired output Excel file name

def export_sqlite_to_excel(db_file, output_excel_file):
    with gzip.open(db_file, 'rb') as f:
        # Read the compressed data and decode it as UTF-8
        uncompressed_data = f.read().decode('utf-8')

    # Create an in-memory database to connect and query the data
    connection = sqlite3.connect(':memory:')
    cursor = connection.cursor()

    try:
        # Execute the uncompressed data as a SQL script to populate the in-memory database
        cursor.executescript(uncompressed_data)

        # Get the table names in the database
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        table_names = [row[0] for row in cursor.fetchall()]

        # Export each table to an Excel file
        with pd.ExcelWriter(output_excel_file) as writer:
            for table_name in table_names:
                # Get the column names of the table
                cursor.execute(f"PRAGMA table_info({table_name});")
                columns = [col[1] for col in cursor.fetchall()]

                # Generate the SELECT query with explicitly set column names
                select_query = f"SELECT {', '.join(columns)} FROM {table_name};"
                df = pd.read_sql_query(select_query, connection)
                df.to_excel(writer, sheet_name=table_name, index=False)

        print(f"Data successfully exported to '{output_excel_file}'.")

    except Exception as e:
        print(f"Error occurred while exporting data: {e}")

    finally:
        cursor.close()
        connection.close()

# Call the function to export the SQLite database to an Excel file
export_sqlite_to_excel(db_file, output_excel_file)













import pandas as pd
from googletrans import Translator

def translate_column(input_column, source_language, target_language):
    translator = Translator()
    # Translate each value in the input column using Google Translate API.
    translated_column = [translator.translate(value, src=source_language, dest=target_language).text
                         for value in input_column]
    return translated_column

def main():
    # Read the CSV file containing country names in multiple languages.
    data = pd.read_csv("multilingual_countries.csv")

    source_languages = ["tr", "es", "de"]  # Turkish, Spanish, German
    target_languages = ["fr", "en", "ar"]  # French, English, Arabic

    # Translate the country names to each target language and create new columns for each translation.
    for source_lang, target_lang in zip(source_languages, target_languages):
        target_column_name = f"Country_{target_lang.upper()}"
        data[target_column_name] = translate_column(data["Country"], source_lang, target_lang)

    # Save the DataFrame to a new CSV file.
    data.to_csv("translated_countries.csv", index=False)

if __name__ == "__main__":
    main()













import pandas as pd

# Sample data
data = {
    'Client ID': [1001, 1002, 1003, 1004],
    'Transaction Amount': [50.35, 100.20, 75.10, 500.75],
    'Location': ['New York', 'Los Angeles', 'Miami', 'San Francisco'],
    'Transaction Date': ['2023-07-12', '2023-07-10', '2023-07-09', '2023-07-08']
}

# Create DataFrame
df = pd.DataFrame(data)

# Convert 'Transaction Date' to datetime
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

# Group data by client ID, date, and location and count the number of transactions
transaction_counts = df.groupby(['Client ID', 'Transaction Date', 'Location']).size().reset_index(name='Count')

# Create an empty similarity matrix with client IDs as index and columns
clients = df['Client ID'].unique()
similarity_matrix = pd.DataFrame(index=clients, columns=clients)
similarity_matrix = similarity_matrix.fillna(0)

# Fill similarity matrix with the count of transactions for each client pair
for _, row in transaction_counts.iterrows():
    client1, client2, count = row['Client ID'], row['Client ID'], row['Count']
    similarity_matrix.at[client1, client2] = count
    similarity_matrix.at[client2, client1] = count

print("Similarity Matrix:")
print(similarity_matrix)



import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder

# Sample data (replace this with your dataset)
data = {
    'ClientID': [1, 2, 3, 4],
    'Country': ['USA', 'Canada', 'USA', 'Mexico'],
    'Date': ['2023-07-28', '2023-07-29', '2023-07-28', '2023-07-30'],
    'Amount': [100, 200, 150, 300]
}

df = pd.DataFrame(data)

# Encode categorical data (Country and Date)
le_country = LabelEncoder()
le_date = LabelEncoder()

df['CountryEncoded'] = le_country.fit_transform(df['Country'])
df['DateEncoded'] = le_date.fit_transform(df['Date'])

# Pivot table to create the matrix
matrix = df.pivot(index='ClientID', columns=['CountryEncoded', 'DateEncoded'], values='Amount')
matrix = matrix.fillna(0)  # Replace NaN with 0 if needed

# Calculate the cosine similarity matrix
cosine_sim = cosine_similarity(matrix)

# Convert the similarity matrix to a DataFrame for better representation
similarity_matrix_df = pd.DataFrame(cosine_sim, index=df['ClientID'], columns=df['ClientID'])

print(similarity_matrix_df)




import pandas as pd
from itertools import combinations

# Sample data (replace this with your dataset)
data = {
    'TransactionID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'ClientID': [1, 2, 1, 3, 4, 2, 3, 5, 1, 5],
    'Location': ['New York', 'Chicago', 'New York', 'Los Angeles', 'Chicago', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York']
}

df = pd.DataFrame(data)

# Group transactions by Location and create a list of clients in each location
grouped_data = df.groupby('Location')['ClientID'].apply(list).reset_index()

# Generate all possible combinations of clients within each location
def generate_combinations(row):
    return list(combinations(row, 2))

grouped_data['Combinations'] = grouped_data['ClientID'].apply(generate_combinations)

# Flatten the combinations to get the pairs of clients
all_combinations = [pair for sublist in grouped_data['Combinations'] for pair in sublist]

# Create a dictionary to count the co-occurrence of each pair of clients in each location
co_occurrence_counts = {}
for pair in all_combinations:
    pair = tuple(sorted(pair))  # Sort the pair to ensure consistent counting
    co_occurrence_counts[pair] = co_occurrence_counts.get(pair, 0) + 1

# Create the co-occurrence matrix
unique_clients = df['ClientID'].unique()
co_occurrence_matrix = pd.DataFrame(0, index=unique_clients, columns=unique_clients)

for pair, count in co_occurrence_counts.items():
    client1, client2 = pair
    co_occurrence_matrix.loc[client1, client2] = count
    co_occurrence_matrix.loc[client2, client1] = count

print(co_occurrence_matrix)









import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler

# Assuming you have a DataFrame called 'data' with columns: 'client_id', 'transaction_date', 'amount'
# You can use 'pivot_table' to reshape your data into a matrix-like structure.
data_pivot = data.pivot_table(index='client_id', columns='transaction_date', values='amount', fill_value=0)

# Normalize the data to remove the influence of scale on the cosine similarity.
scaler = StandardScaler()
normalized_data = scaler.fit_transform(data_pivot)

# Calculate the cosine similarity matrix
cosine_sim_matrix = cosine_similarity(normalized_data)

# Convert the cosine similarity matrix into a DataFrame for better visualization
cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=data_pivot.index, columns=data_pivot.index)

# cosine_sim_df now contains the similarity scores between clients based on their transaction amounts.






import pandas as pd
import numpy as np

# Sample transaction data as a pandas DataFrame
data = {
    'ClientID': [1, 2, 1, 3, 2, 1, 2, 3],
    'TransactionDate': ['2023-07-28', '2023-07-28', '2023-07-28', '2023-07-29',
                        '2023-07-29', '2023-07-30', '2023-07-30', '2023-07-30']
}

df = pd.DataFrame(data)

# Create a co-occurrence matrix using a dictionary of dictionaries
co_occurrence_matrix = {}
for client_id in df['ClientID'].unique():
    co_occurrence_matrix[client_id] = {}

# Iterate through the transactions and update the co-occurrence matrix
for i in range(len(df)):
    client_id_i = df.loc[i, 'ClientID']
    date_i = df.loc[i, 'TransactionDate']
    
    for j in range(i + 1, len(df)):
        client_id_j = df.loc[j, 'ClientID']
        date_j = df.loc[j, 'TransactionDate']
        
        if date_i == date_j:
            co_occurrence_matrix[client_id_i][client_id_j] = co_occurrence_matrix.get(client_id_i, {}).get(client_id_j, 0) + 1
            co_occurrence_matrix[client_id_j][client_id_i] = co_occurrence_matrix.get(client_id_j, {}).get(client_id_i, 0) + 1

# Convert the co-occurrence matrix to a 2D numpy array for easier manipulation
unique_clients = sorted(df['ClientID'].unique())
matrix_size = len(unique_clients)
matrix = np.zeros((matrix_size, matrix_size))

for i, client_id_i in enumerate(unique_clients):
    for j, client_id_j in enumerate(unique_clients):
        matrix[i][j] = co_occurrence_matrix.get(client_id_i, {}).get(client_id_j, 0)

print("Co-occurrence Matrix:")
print(matrix)









def transform_date_operation(date_str, complement_str):
    if date_str.month == 1 and complement_str[-4:-2] == "12":
        new_year = str(date_str.year - 1)
        new_month = complement_str[-2:]
        new_day = complement_str[:2]
        return new_year + "-" + new_month + "=" + new_day
    else:
        new_year = str(date_str.year)
        new_month = complement_str[-2:]
        new_day = complement_str[:2]
        return new_year + new_month + "-" + new_day

# Usage example:
# Assuming DATE_OPERATION and COMPLEMENT are datetime and string variables respectively
# and MVT, CPTABLE, and HMVT are extracted from those variables.
result = transform_date_operation(DATE_OPERATION, COMPLEMENT)
print(result)




import pandas as pd

# Assuming your data frame is named 'df'
def transform_date_operation(row):
    date_str = pd.to_datetime(row['DATE OPERATION MVT CPTABLE-HMVT'])
    complement_str = row['LIB. COMPLEMENT. MVT CPTABLE-HMVT']
    
    if date_str.month == 1 and complement_str[-4:-2] == "12":
        new_year = str(date_str.year - 1)
        new_month = complement_str[-2:]
        new_day = complement_str[:2]
        return new_year + "-" + new_month + "=" + new_day
    else:
        new_year = str(date_str.year)
        new_month = complement_str[-2:]
        new_day = complement_str[:2]
        return new_year + new_month + "-" + new_day

# Apply the transformation to the DataFrame
df['NEW_COLUMN'] = df.apply(lambda row: transform_date_operation(row), axis=1)

# Now the transformed values will be stored in the new column 'NEW_COLUMN'







import pandas as pd
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer

# Sample data: replace this with your own dataset
data = {
    'Location': ['Location 1', 'Location 2', 'Location 3', 'Location 4', 'Location 5'],
    'Country': ['Country A', 'Country B', None, 'Country C', None],
}

# Load data into a DataFrame
df = pd.DataFrame(data)

# Drop rows with missing countries
df = df.dropna(subset=['Country'])

# Feature engineering: Vectorize location names for k-means
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['Location'])

# Number of clusters (adjust this based on your dataset and needs)
num_clusters = 3

# Apply k-means clustering to locations with known countries
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
df['Cluster'] = kmeans.fit_predict(X)

# Function to predict missing countries using cluster information
def predict_missing_countries():
    missing_df = df[df['Country'].isnull()]
    for index, row in missing_df.iterrows():
        cluster = row['Cluster']
        predicted_country = df[df['Cluster'] == cluster]['Country'].mode().values[0]
        df.at[index, 'Country'] = predicted_country

# Predict missing countries
predict_missing_countries()

# Resulting DataFrame with predicted countries
print(df)


import pandas as pd
from difflib import SequenceMatcher

# Sample data: replace this with your own dataset
data = {
    'Location': ['Location 1', 'Location 2', 'Location 3', 'Location 4', 'Location 5'],
    'Country': ['Country A', 'Country B', None, 'Country C', None],
}

# Load data into a DataFrame
df = pd.DataFrame(data)

# Function to calculate string similarity between two locations
def location_similarity(loc1, loc2):
    return SequenceMatcher(None, loc1, loc2).ratio()

# Function to predict missing countries using string similarity
def predict_missing_countries():
    missing_df = df[df['Country'].isnull()]
    for index, row in missing_df.iterrows():
        location = row['Location']
        max_similarity = 0
        predicted_country = None

        # Find the most similar known location and use its country
        for idx, known_row in df[df['Country'].notnull()].iterrows():
            known_location = known_row['Location']
            similarity = location_similarity(location, known_location)
            if similarity > max_similarity:
                max_similarity = similarity
                predicted_country = known_row['Country']

        df.at[index, 'Country'] = predicted_country

# Predict missing countries
predict_missing_countries()

# Resulting DataFrame with predicted countries
print(df)



import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Create the dataset with some missing values in the "Country" column
data = {'Location': ['Paris', 'London', 'Paris 45 Avenue', 'Berlin'],
        'Country': ['France', 'UK', 'France', None]}
df = pd.DataFrame(data)

# Drop rows with missing values in the "Country" column
df.dropna(subset=['Country'], inplace=True)

# Convert location strings to numerical features
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['Location'])

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, df['Country'], test_size=0.2, random_state=42)

# Create and train the Decision Tree Classifier
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = model.predict(X_val)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_val, y_pred)
print("Model Accuracy:", accuracy)



import fitz  # PyMuPDF library for PDF processing
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

# Load the pre-trained BERT model and tokenizer for question-answering
model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page_num in range(doc.page_count):
        page = doc.load_page(page_num)
        text += page.get_text("text")
    doc.close()
    return text

# Function to perform question-answering on the PDF content
def perform_question_answering(pdf_content, question):
    inputs = tokenizer.encode_plus(question, pdf_content, add_special_tokens=True, return_tensors="pt")
    answer_start_scores, answer_end_scores = model(**inputs)
    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end]))
    return answer

# Example PDF file path (replace this with your PDF file)
pdf_file_path = "example.pdf"

# Example question to ask about the PDF content
question = "What does the PDF contain?"

# Extract text from the PDF
pdf_content = extract_text_from_pdf(pdf_file_path)

# Perform question-answering for the given question
answer = perform_question_answering(pdf_content, question)

# Print the answer
print(f"Question: {question}")
print(f"Answer: {answer}")

import pandas as pd

# Example DataFrame
data = pd.DataFrame({
    'lible': ['commis', 'open cart', 'RET GAB vis', 'something else', 'another RET GAB vis']
})

data['new_column'] = ''

def add_new_column(row):
    if row['lible'] == 'commis':
        return 'com tpe'
    elif 'RET' in row['lible'] and 'GAB' in row['lible'] and 'vis' in row['lible']:
        return 'complete gab'
    else:
        return ''

data['new_column'] = data.apply(add_new_column, axis=1)

print(data)
import pandas as pd

# Assuming 'df' is your DataFrame
# Use apply with a lambda function to create the new 'class' column
df['class'] = df.apply(lambda row: 'com_tpe_payment' if pd.notnull(row['com_tpe'])
                                              else 'com_tpe_null' if pd.isnull(row['com_ret_gab'])
                                              else 'com_tpe_and_com_ret_gab_null', axis=1)



import pandas as pd
import numpy as np

# Assuming 'df' is your DataFrame
# Use np.where to create the new 'class' column based on the conditions
df['class'] = np.where(pd.notnull(df['COM TPE']), 'COM TPE PAIEMENT',
                       np.where(pd.isnull(df['COM TPE']) & pd.isnull(df['COM RET GAB']), 'COM TPE PAIEMENT',
                                np.where(df['LIB. COMPLEMENT. MVT CPTABLE -HMVT'].str.contains("*>"), 'RET GAB BNPPARIBAS', 'RET GAB HORS BNPPARIBAS')))

